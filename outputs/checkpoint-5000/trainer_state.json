{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002,
      "grad_norm": 2.0221638679504395,
      "learning_rate": 0.0001,
      "loss": 3.5965,
      "step": 1
    },
    {
      "epoch": 0.004,
      "grad_norm": 1.935131311416626,
      "learning_rate": 0.0002,
      "loss": 3.4344,
      "step": 2
    },
    {
      "epoch": 0.006,
      "grad_norm": 2.245640516281128,
      "learning_rate": 0.00019995998399359745,
      "loss": 3.7084,
      "step": 3
    },
    {
      "epoch": 0.008,
      "grad_norm": 1.5754972696304321,
      "learning_rate": 0.00019991996798719488,
      "loss": 2.6866,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.7579797506332397,
      "learning_rate": 0.00019987995198079232,
      "loss": 2.4727,
      "step": 5
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.080029010772705,
      "learning_rate": 0.00019983993597438976,
      "loss": 2.4266,
      "step": 6
    },
    {
      "epoch": 0.014,
      "grad_norm": 1.5920244455337524,
      "learning_rate": 0.00019979991996798722,
      "loss": 1.9968,
      "step": 7
    },
    {
      "epoch": 0.016,
      "grad_norm": 1.1800373792648315,
      "learning_rate": 0.00019975990396158463,
      "loss": 1.6297,
      "step": 8
    },
    {
      "epoch": 0.018,
      "grad_norm": 1.5648276805877686,
      "learning_rate": 0.0001997198879551821,
      "loss": 2.1392,
      "step": 9
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.5616611242294312,
      "learning_rate": 0.00019967987194877953,
      "loss": 1.7658,
      "step": 10
    },
    {
      "epoch": 0.022,
      "grad_norm": 1.524091362953186,
      "learning_rate": 0.00019963985594237694,
      "loss": 1.7817,
      "step": 11
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.3885102272033691,
      "learning_rate": 0.0001995998399359744,
      "loss": 1.259,
      "step": 12
    },
    {
      "epoch": 0.026,
      "grad_norm": 1.2968969345092773,
      "learning_rate": 0.00019955982392957185,
      "loss": 1.2763,
      "step": 13
    },
    {
      "epoch": 0.028,
      "grad_norm": 1.2881919145584106,
      "learning_rate": 0.00019951980792316926,
      "loss": 1.4006,
      "step": 14
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.057982325553894,
      "learning_rate": 0.00019947979191676672,
      "loss": 1.1896,
      "step": 15
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.8950812816619873,
      "learning_rate": 0.00019943977591036416,
      "loss": 1.0305,
      "step": 16
    },
    {
      "epoch": 0.034,
      "grad_norm": 1.1149872541427612,
      "learning_rate": 0.0001993997599039616,
      "loss": 1.3174,
      "step": 17
    },
    {
      "epoch": 0.036,
      "grad_norm": 1.006791353225708,
      "learning_rate": 0.00019935974389755903,
      "loss": 1.5058,
      "step": 18
    },
    {
      "epoch": 0.038,
      "grad_norm": 1.1075940132141113,
      "learning_rate": 0.00019931972789115647,
      "loss": 1.0233,
      "step": 19
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.059651255607605,
      "learning_rate": 0.0001992797118847539,
      "loss": 1.173,
      "step": 20
    },
    {
      "epoch": 0.042,
      "grad_norm": 1.4350768327713013,
      "learning_rate": 0.00019923969587835134,
      "loss": 1.2629,
      "step": 21
    },
    {
      "epoch": 0.044,
      "grad_norm": 0.8532227873802185,
      "learning_rate": 0.00019919967987194878,
      "loss": 0.9531,
      "step": 22
    },
    {
      "epoch": 0.046,
      "grad_norm": 1.2793751955032349,
      "learning_rate": 0.00019915966386554624,
      "loss": 1.1919,
      "step": 23
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.018997073173523,
      "learning_rate": 0.00019911964785914368,
      "loss": 0.987,
      "step": 24
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.231921911239624,
      "learning_rate": 0.0001990796318527411,
      "loss": 1.2946,
      "step": 25
    },
    {
      "epoch": 0.052,
      "grad_norm": 1.245470643043518,
      "learning_rate": 0.00019903961584633856,
      "loss": 1.1705,
      "step": 26
    },
    {
      "epoch": 0.054,
      "grad_norm": 1.0812486410140991,
      "learning_rate": 0.000198999599839936,
      "loss": 1.2998,
      "step": 27
    },
    {
      "epoch": 0.056,
      "grad_norm": 1.3258495330810547,
      "learning_rate": 0.0001989595838335334,
      "loss": 1.0921,
      "step": 28
    },
    {
      "epoch": 0.058,
      "grad_norm": 1.1078262329101562,
      "learning_rate": 0.00019891956782713087,
      "loss": 0.8192,
      "step": 29
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.2225009202957153,
      "learning_rate": 0.0001988795518207283,
      "loss": 1.2245,
      "step": 30
    },
    {
      "epoch": 0.062,
      "grad_norm": 0.8624112606048584,
      "learning_rate": 0.00019883953581432574,
      "loss": 1.0761,
      "step": 31
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.926005482673645,
      "learning_rate": 0.00019879951980792318,
      "loss": 0.9193,
      "step": 32
    },
    {
      "epoch": 0.066,
      "grad_norm": 0.9844384789466858,
      "learning_rate": 0.00019875950380152061,
      "loss": 1.2699,
      "step": 33
    },
    {
      "epoch": 0.068,
      "grad_norm": 1.0531792640686035,
      "learning_rate": 0.00019871948779511805,
      "loss": 0.9214,
      "step": 34
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.1972600221633911,
      "learning_rate": 0.0001986794717887155,
      "loss": 0.9288,
      "step": 35
    },
    {
      "epoch": 0.072,
      "grad_norm": 1.1645710468292236,
      "learning_rate": 0.00019863945578231293,
      "loss": 1.2999,
      "step": 36
    },
    {
      "epoch": 0.074,
      "grad_norm": 1.1646463871002197,
      "learning_rate": 0.0001985994397759104,
      "loss": 1.0285,
      "step": 37
    },
    {
      "epoch": 0.076,
      "grad_norm": 1.1513386964797974,
      "learning_rate": 0.0001985594237695078,
      "loss": 1.0685,
      "step": 38
    },
    {
      "epoch": 0.078,
      "grad_norm": 1.4563333988189697,
      "learning_rate": 0.00019851940776310524,
      "loss": 0.9935,
      "step": 39
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.0809413194656372,
      "learning_rate": 0.0001984793917567027,
      "loss": 0.6796,
      "step": 40
    },
    {
      "epoch": 0.082,
      "grad_norm": 1.128322958946228,
      "learning_rate": 0.00019843937575030014,
      "loss": 1.1977,
      "step": 41
    },
    {
      "epoch": 0.084,
      "grad_norm": 1.1352465152740479,
      "learning_rate": 0.00019839935974389755,
      "loss": 0.9927,
      "step": 42
    },
    {
      "epoch": 0.086,
      "grad_norm": 1.0299326181411743,
      "learning_rate": 0.000198359343737495,
      "loss": 0.8979,
      "step": 43
    },
    {
      "epoch": 0.088,
      "grad_norm": 1.4210914373397827,
      "learning_rate": 0.00019831932773109245,
      "loss": 1.0458,
      "step": 44
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.074748158454895,
      "learning_rate": 0.0001982793117246899,
      "loss": 0.9939,
      "step": 45
    },
    {
      "epoch": 0.092,
      "grad_norm": 1.009193778038025,
      "learning_rate": 0.00019823929571828732,
      "loss": 0.7832,
      "step": 46
    },
    {
      "epoch": 0.094,
      "grad_norm": 1.1132973432540894,
      "learning_rate": 0.00019819927971188476,
      "loss": 0.9174,
      "step": 47
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.9246149659156799,
      "learning_rate": 0.0001981592637054822,
      "loss": 0.8898,
      "step": 48
    },
    {
      "epoch": 0.098,
      "grad_norm": 1.019877552986145,
      "learning_rate": 0.00019811924769907964,
      "loss": 0.922,
      "step": 49
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.0656808614730835,
      "learning_rate": 0.00019807923169267707,
      "loss": 1.1679,
      "step": 50
    },
    {
      "epoch": 0.102,
      "grad_norm": 0.9894453883171082,
      "learning_rate": 0.00019803921568627454,
      "loss": 0.9151,
      "step": 51
    },
    {
      "epoch": 0.104,
      "grad_norm": 1.2595229148864746,
      "learning_rate": 0.00019799919967987195,
      "loss": 1.1513,
      "step": 52
    },
    {
      "epoch": 0.106,
      "grad_norm": 0.9034907817840576,
      "learning_rate": 0.00019795918367346938,
      "loss": 0.8919,
      "step": 53
    },
    {
      "epoch": 0.108,
      "grad_norm": 0.9881527423858643,
      "learning_rate": 0.00019791916766706685,
      "loss": 0.747,
      "step": 54
    },
    {
      "epoch": 0.11,
      "grad_norm": 1.1398965120315552,
      "learning_rate": 0.00019787915166066429,
      "loss": 0.786,
      "step": 55
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.0054879188537598,
      "learning_rate": 0.0001978391356542617,
      "loss": 0.7814,
      "step": 56
    },
    {
      "epoch": 0.114,
      "grad_norm": 1.1986931562423706,
      "learning_rate": 0.00019779911964785916,
      "loss": 0.9899,
      "step": 57
    },
    {
      "epoch": 0.116,
      "grad_norm": 1.1443192958831787,
      "learning_rate": 0.0001977591036414566,
      "loss": 0.7526,
      "step": 58
    },
    {
      "epoch": 0.118,
      "grad_norm": 0.9014662504196167,
      "learning_rate": 0.00019771908763505403,
      "loss": 0.8464,
      "step": 59
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8869990110397339,
      "learning_rate": 0.00019767907162865147,
      "loss": 0.7122,
      "step": 60
    },
    {
      "epoch": 0.122,
      "grad_norm": 0.9273400902748108,
      "learning_rate": 0.0001976390556222489,
      "loss": 0.8474,
      "step": 61
    },
    {
      "epoch": 0.124,
      "grad_norm": 0.9780536890029907,
      "learning_rate": 0.00019759903961584635,
      "loss": 0.7221,
      "step": 62
    },
    {
      "epoch": 0.126,
      "grad_norm": 1.0905835628509521,
      "learning_rate": 0.00019755902360944378,
      "loss": 0.8538,
      "step": 63
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.8670175671577454,
      "learning_rate": 0.00019751900760304122,
      "loss": 0.7409,
      "step": 64
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.116119623184204,
      "learning_rate": 0.00019747899159663868,
      "loss": 0.8259,
      "step": 65
    },
    {
      "epoch": 0.132,
      "grad_norm": 1.4008511304855347,
      "learning_rate": 0.0001974389755902361,
      "loss": 1.1089,
      "step": 66
    },
    {
      "epoch": 0.134,
      "grad_norm": 1.0826139450073242,
      "learning_rate": 0.00019739895958383353,
      "loss": 0.6853,
      "step": 67
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.9369148015975952,
      "learning_rate": 0.000197358943577431,
      "loss": 0.7518,
      "step": 68
    },
    {
      "epoch": 0.138,
      "grad_norm": 0.9980671405792236,
      "learning_rate": 0.0001973189275710284,
      "loss": 0.5985,
      "step": 69
    },
    {
      "epoch": 0.14,
      "grad_norm": 1.4036153554916382,
      "learning_rate": 0.00019727891156462587,
      "loss": 0.8513,
      "step": 70
    },
    {
      "epoch": 0.142,
      "grad_norm": 0.8968185782432556,
      "learning_rate": 0.0001972388955582233,
      "loss": 0.7475,
      "step": 71
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.9388036131858826,
      "learning_rate": 0.00019719887955182074,
      "loss": 0.6781,
      "step": 72
    },
    {
      "epoch": 0.146,
      "grad_norm": 1.0758137702941895,
      "learning_rate": 0.00019715886354541818,
      "loss": 0.8005,
      "step": 73
    },
    {
      "epoch": 0.148,
      "grad_norm": 0.9833724498748779,
      "learning_rate": 0.00019711884753901562,
      "loss": 0.6681,
      "step": 74
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.3325488567352295,
      "learning_rate": 0.00019707883153261306,
      "loss": 0.7936,
      "step": 75
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.9277870655059814,
      "learning_rate": 0.0001970388155262105,
      "loss": 0.7407,
      "step": 76
    },
    {
      "epoch": 0.154,
      "grad_norm": 0.8063499927520752,
      "learning_rate": 0.00019699879951980793,
      "loss": 0.7669,
      "step": 77
    },
    {
      "epoch": 0.156,
      "grad_norm": 0.7643763422966003,
      "learning_rate": 0.00019695878351340537,
      "loss": 0.639,
      "step": 78
    },
    {
      "epoch": 0.158,
      "grad_norm": 0.9250590205192566,
      "learning_rate": 0.00019691876750700283,
      "loss": 0.6772,
      "step": 79
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.7812475562095642,
      "learning_rate": 0.00019687875150060024,
      "loss": 0.688,
      "step": 80
    },
    {
      "epoch": 0.162,
      "grad_norm": 0.8799681067466736,
      "learning_rate": 0.00019683873549419768,
      "loss": 0.6453,
      "step": 81
    },
    {
      "epoch": 0.164,
      "grad_norm": 0.8595180511474609,
      "learning_rate": 0.00019679871948779514,
      "loss": 0.7837,
      "step": 82
    },
    {
      "epoch": 0.166,
      "grad_norm": 1.0017406940460205,
      "learning_rate": 0.00019675870348139255,
      "loss": 0.8374,
      "step": 83
    },
    {
      "epoch": 0.168,
      "grad_norm": 1.0505634546279907,
      "learning_rate": 0.00019671868747499002,
      "loss": 1.0183,
      "step": 84
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.9358767867088318,
      "learning_rate": 0.00019667867146858745,
      "loss": 0.8065,
      "step": 85
    },
    {
      "epoch": 0.172,
      "grad_norm": 0.7226177453994751,
      "learning_rate": 0.00019663865546218486,
      "loss": 0.6753,
      "step": 86
    },
    {
      "epoch": 0.174,
      "grad_norm": 0.8806548118591309,
      "learning_rate": 0.00019659863945578233,
      "loss": 0.7591,
      "step": 87
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.8323225378990173,
      "learning_rate": 0.00019655862344937977,
      "loss": 0.6901,
      "step": 88
    },
    {
      "epoch": 0.178,
      "grad_norm": 1.1836646795272827,
      "learning_rate": 0.0001965186074429772,
      "loss": 0.8326,
      "step": 89
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.0662578344345093,
      "learning_rate": 0.00019647859143657464,
      "loss": 0.805,
      "step": 90
    },
    {
      "epoch": 0.182,
      "grad_norm": 1.081337809562683,
      "learning_rate": 0.00019643857543017208,
      "loss": 0.9725,
      "step": 91
    },
    {
      "epoch": 0.184,
      "grad_norm": 1.2090610265731812,
      "learning_rate": 0.00019639855942376951,
      "loss": 1.0046,
      "step": 92
    },
    {
      "epoch": 0.186,
      "grad_norm": 1.2256230115890503,
      "learning_rate": 0.00019635854341736695,
      "loss": 0.8588,
      "step": 93
    },
    {
      "epoch": 0.188,
      "grad_norm": 0.9794591665267944,
      "learning_rate": 0.0001963185274109644,
      "loss": 0.7572,
      "step": 94
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.9212394952774048,
      "learning_rate": 0.00019627851140456183,
      "loss": 0.8592,
      "step": 95
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.8725636601448059,
      "learning_rate": 0.0001962384953981593,
      "loss": 0.7059,
      "step": 96
    },
    {
      "epoch": 0.194,
      "grad_norm": 0.9404150247573853,
      "learning_rate": 0.0001961984793917567,
      "loss": 0.8007,
      "step": 97
    },
    {
      "epoch": 0.196,
      "grad_norm": 1.2731430530548096,
      "learning_rate": 0.00019615846338535416,
      "loss": 0.8171,
      "step": 98
    },
    {
      "epoch": 0.198,
      "grad_norm": 0.8923984169960022,
      "learning_rate": 0.0001961184473789516,
      "loss": 0.7609,
      "step": 99
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.1265723705291748,
      "learning_rate": 0.000196078431372549,
      "loss": 0.7949,
      "step": 100
    },
    {
      "epoch": 0.202,
      "grad_norm": 0.9581004977226257,
      "learning_rate": 0.00019603841536614648,
      "loss": 0.6722,
      "step": 101
    },
    {
      "epoch": 0.204,
      "grad_norm": 0.8791517019271851,
      "learning_rate": 0.0001959983993597439,
      "loss": 0.6436,
      "step": 102
    },
    {
      "epoch": 0.206,
      "grad_norm": 1.1084363460540771,
      "learning_rate": 0.00019595838335334132,
      "loss": 0.7104,
      "step": 103
    },
    {
      "epoch": 0.208,
      "grad_norm": 1.2205723524093628,
      "learning_rate": 0.0001959183673469388,
      "loss": 0.6948,
      "step": 104
    },
    {
      "epoch": 0.21,
      "grad_norm": 1.3686246871948242,
      "learning_rate": 0.00019587835134053622,
      "loss": 0.8867,
      "step": 105
    },
    {
      "epoch": 0.212,
      "grad_norm": 0.766969621181488,
      "learning_rate": 0.00019583833533413366,
      "loss": 0.5193,
      "step": 106
    },
    {
      "epoch": 0.214,
      "grad_norm": 1.1791776418685913,
      "learning_rate": 0.0001957983193277311,
      "loss": 0.6948,
      "step": 107
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.8143807649612427,
      "learning_rate": 0.00019575830332132854,
      "loss": 0.7775,
      "step": 108
    },
    {
      "epoch": 0.218,
      "grad_norm": 0.826600968837738,
      "learning_rate": 0.00019571828731492597,
      "loss": 0.8108,
      "step": 109
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.9637686610221863,
      "learning_rate": 0.0001956782713085234,
      "loss": 0.8025,
      "step": 110
    },
    {
      "epoch": 0.222,
      "grad_norm": 1.1313934326171875,
      "learning_rate": 0.00019563825530212085,
      "loss": 0.9817,
      "step": 111
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.7560111880302429,
      "learning_rate": 0.0001955982392957183,
      "loss": 0.6227,
      "step": 112
    },
    {
      "epoch": 0.226,
      "grad_norm": 0.8168442249298096,
      "learning_rate": 0.00019555822328931575,
      "loss": 0.532,
      "step": 113
    },
    {
      "epoch": 0.228,
      "grad_norm": 1.0279977321624756,
      "learning_rate": 0.00019551820728291316,
      "loss": 0.8802,
      "step": 114
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.9863279461860657,
      "learning_rate": 0.00019547819127651062,
      "loss": 0.9904,
      "step": 115
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.9254160523414612,
      "learning_rate": 0.00019543817527010806,
      "loss": 0.7037,
      "step": 116
    },
    {
      "epoch": 0.234,
      "grad_norm": 1.7595099210739136,
      "learning_rate": 0.00019539815926370547,
      "loss": 1.0124,
      "step": 117
    },
    {
      "epoch": 0.236,
      "grad_norm": 0.97328120470047,
      "learning_rate": 0.00019535814325730293,
      "loss": 0.7876,
      "step": 118
    },
    {
      "epoch": 0.238,
      "grad_norm": 1.26939857006073,
      "learning_rate": 0.00019531812725090037,
      "loss": 0.8707,
      "step": 119
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.9680797457695007,
      "learning_rate": 0.0001952781112444978,
      "loss": 0.8075,
      "step": 120
    },
    {
      "epoch": 0.242,
      "grad_norm": 1.0862386226654053,
      "learning_rate": 0.00019523809523809525,
      "loss": 0.8156,
      "step": 121
    },
    {
      "epoch": 0.244,
      "grad_norm": 1.0286129713058472,
      "learning_rate": 0.00019519807923169268,
      "loss": 0.7597,
      "step": 122
    },
    {
      "epoch": 0.246,
      "grad_norm": 0.9192622303962708,
      "learning_rate": 0.00019515806322529012,
      "loss": 0.6434,
      "step": 123
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.8805800080299377,
      "learning_rate": 0.00019511804721888756,
      "loss": 0.7513,
      "step": 124
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.8578165173530579,
      "learning_rate": 0.000195078031212485,
      "loss": 0.8119,
      "step": 125
    },
    {
      "epoch": 0.252,
      "grad_norm": 0.9991392493247986,
      "learning_rate": 0.00019503801520608246,
      "loss": 0.739,
      "step": 126
    },
    {
      "epoch": 0.254,
      "grad_norm": 0.8403825163841248,
      "learning_rate": 0.00019499799919967987,
      "loss": 0.7092,
      "step": 127
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.9621414542198181,
      "learning_rate": 0.0001949579831932773,
      "loss": 0.9035,
      "step": 128
    },
    {
      "epoch": 0.258,
      "grad_norm": 0.8193351626396179,
      "learning_rate": 0.00019491796718687477,
      "loss": 0.5586,
      "step": 129
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.9680688381195068,
      "learning_rate": 0.0001948779511804722,
      "loss": 0.8282,
      "step": 130
    },
    {
      "epoch": 0.262,
      "grad_norm": 0.9615050554275513,
      "learning_rate": 0.00019483793517406962,
      "loss": 0.8292,
      "step": 131
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.8664271831512451,
      "learning_rate": 0.00019479791916766708,
      "loss": 0.6155,
      "step": 132
    },
    {
      "epoch": 0.266,
      "grad_norm": 1.1489105224609375,
      "learning_rate": 0.00019475790316126452,
      "loss": 0.657,
      "step": 133
    },
    {
      "epoch": 0.268,
      "grad_norm": 0.9964358806610107,
      "learning_rate": 0.00019471788715486196,
      "loss": 0.826,
      "step": 134
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.7581567764282227,
      "learning_rate": 0.0001946778711484594,
      "loss": 0.6764,
      "step": 135
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.0081273317337036,
      "learning_rate": 0.00019463785514205683,
      "loss": 0.8386,
      "step": 136
    },
    {
      "epoch": 0.274,
      "grad_norm": 0.915859043598175,
      "learning_rate": 0.0001945978391356543,
      "loss": 0.7312,
      "step": 137
    },
    {
      "epoch": 0.276,
      "grad_norm": 1.0146435499191284,
      "learning_rate": 0.0001945578231292517,
      "loss": 0.7927,
      "step": 138
    },
    {
      "epoch": 0.278,
      "grad_norm": 0.8825095891952515,
      "learning_rate": 0.00019451780712284914,
      "loss": 0.9016,
      "step": 139
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.8653955459594727,
      "learning_rate": 0.0001944777911164466,
      "loss": 0.5934,
      "step": 140
    },
    {
      "epoch": 0.282,
      "grad_norm": 1.2110621929168701,
      "learning_rate": 0.00019443777511004402,
      "loss": 0.764,
      "step": 141
    },
    {
      "epoch": 0.284,
      "grad_norm": 1.1886318922042847,
      "learning_rate": 0.00019439775910364145,
      "loss": 0.7775,
      "step": 142
    },
    {
      "epoch": 0.286,
      "grad_norm": 0.932651937007904,
      "learning_rate": 0.00019435774309723892,
      "loss": 0.8723,
      "step": 143
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.8677171468734741,
      "learning_rate": 0.00019431772709083635,
      "loss": 0.6939,
      "step": 144
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.4496378898620605,
      "learning_rate": 0.0001942777110844338,
      "loss": 0.8201,
      "step": 145
    },
    {
      "epoch": 0.292,
      "grad_norm": 0.9382779598236084,
      "learning_rate": 0.00019423769507803123,
      "loss": 0.733,
      "step": 146
    },
    {
      "epoch": 0.294,
      "grad_norm": 0.7975431084632874,
      "learning_rate": 0.00019419767907162867,
      "loss": 0.6554,
      "step": 147
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.9558757543563843,
      "learning_rate": 0.0001941576630652261,
      "loss": 1.0364,
      "step": 148
    },
    {
      "epoch": 0.298,
      "grad_norm": 0.7880683541297913,
      "learning_rate": 0.00019411764705882354,
      "loss": 0.622,
      "step": 149
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.0168651342391968,
      "learning_rate": 0.00019407763105242098,
      "loss": 0.9386,
      "step": 150
    },
    {
      "epoch": 0.302,
      "grad_norm": 0.9403958916664124,
      "learning_rate": 0.00019403761504601841,
      "loss": 0.7922,
      "step": 151
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.8445661664009094,
      "learning_rate": 0.00019399759903961585,
      "loss": 0.8376,
      "step": 152
    },
    {
      "epoch": 0.306,
      "grad_norm": 0.944783091545105,
      "learning_rate": 0.0001939575830332133,
      "loss": 0.7117,
      "step": 153
    },
    {
      "epoch": 0.308,
      "grad_norm": 0.8569881319999695,
      "learning_rate": 0.00019391756702681075,
      "loss": 0.7961,
      "step": 154
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.8298698663711548,
      "learning_rate": 0.00019387755102040816,
      "loss": 0.5342,
      "step": 155
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.8237974047660828,
      "learning_rate": 0.0001938375350140056,
      "loss": 0.7617,
      "step": 156
    },
    {
      "epoch": 0.314,
      "grad_norm": 1.0334194898605347,
      "learning_rate": 0.00019379751900760306,
      "loss": 0.6781,
      "step": 157
    },
    {
      "epoch": 0.316,
      "grad_norm": 0.7655731439590454,
      "learning_rate": 0.00019375750300120047,
      "loss": 0.7227,
      "step": 158
    },
    {
      "epoch": 0.318,
      "grad_norm": 1.1844391822814941,
      "learning_rate": 0.00019371748699479794,
      "loss": 0.784,
      "step": 159
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.0338881015777588,
      "learning_rate": 0.00019367747098839538,
      "loss": 0.5929,
      "step": 160
    },
    {
      "epoch": 0.322,
      "grad_norm": 0.947904109954834,
      "learning_rate": 0.0001936374549819928,
      "loss": 0.6054,
      "step": 161
    },
    {
      "epoch": 0.324,
      "grad_norm": 1.164900302886963,
      "learning_rate": 0.00019359743897559025,
      "loss": 0.8743,
      "step": 162
    },
    {
      "epoch": 0.326,
      "grad_norm": 0.9322641491889954,
      "learning_rate": 0.0001935574229691877,
      "loss": 0.8476,
      "step": 163
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.9625057578086853,
      "learning_rate": 0.00019351740696278512,
      "loss": 0.6229,
      "step": 164
    },
    {
      "epoch": 0.33,
      "grad_norm": 1.0403448343276978,
      "learning_rate": 0.00019347739095638256,
      "loss": 0.6531,
      "step": 165
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.0857855081558228,
      "learning_rate": 0.00019343737494998,
      "loss": 0.6409,
      "step": 166
    },
    {
      "epoch": 0.334,
      "grad_norm": 0.9498406052589417,
      "learning_rate": 0.00019339735894357744,
      "loss": 0.9021,
      "step": 167
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.9137316346168518,
      "learning_rate": 0.0001933573429371749,
      "loss": 0.8722,
      "step": 168
    },
    {
      "epoch": 0.338,
      "grad_norm": 0.986888587474823,
      "learning_rate": 0.0001933173269307723,
      "loss": 0.5989,
      "step": 169
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.9513566493988037,
      "learning_rate": 0.00019327731092436975,
      "loss": 0.7261,
      "step": 170
    },
    {
      "epoch": 0.342,
      "grad_norm": 0.6738829612731934,
      "learning_rate": 0.0001932372949179672,
      "loss": 0.5583,
      "step": 171
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.7348714470863342,
      "learning_rate": 0.00019319727891156462,
      "loss": 0.6229,
      "step": 172
    },
    {
      "epoch": 0.346,
      "grad_norm": 0.8570204377174377,
      "learning_rate": 0.00019315726290516208,
      "loss": 0.6269,
      "step": 173
    },
    {
      "epoch": 0.348,
      "grad_norm": 0.8245512247085571,
      "learning_rate": 0.00019311724689875952,
      "loss": 0.8962,
      "step": 174
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.8643543124198914,
      "learning_rate": 0.00019307723089235693,
      "loss": 0.8539,
      "step": 175
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.9332842230796814,
      "learning_rate": 0.0001930372148859544,
      "loss": 0.987,
      "step": 176
    },
    {
      "epoch": 0.354,
      "grad_norm": 0.7413058280944824,
      "learning_rate": 0.00019299719887955183,
      "loss": 0.6349,
      "step": 177
    },
    {
      "epoch": 0.356,
      "grad_norm": 0.9016087055206299,
      "learning_rate": 0.00019295718287314927,
      "loss": 0.6434,
      "step": 178
    },
    {
      "epoch": 0.358,
      "grad_norm": 1.0092320442199707,
      "learning_rate": 0.0001929171668667467,
      "loss": 0.8689,
      "step": 179
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.8407918810844421,
      "learning_rate": 0.00019287715086034414,
      "loss": 0.8161,
      "step": 180
    },
    {
      "epoch": 0.362,
      "grad_norm": 0.9841901659965515,
      "learning_rate": 0.00019283713485394158,
      "loss": 0.6543,
      "step": 181
    },
    {
      "epoch": 0.364,
      "grad_norm": 0.9025986790657043,
      "learning_rate": 0.00019279711884753902,
      "loss": 0.9346,
      "step": 182
    },
    {
      "epoch": 0.366,
      "grad_norm": 0.8775114417076111,
      "learning_rate": 0.00019275710284113646,
      "loss": 0.5128,
      "step": 183
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.9827938079833984,
      "learning_rate": 0.0001927170868347339,
      "loss": 0.6929,
      "step": 184
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.1161257028579712,
      "learning_rate": 0.00019267707082833136,
      "loss": 0.96,
      "step": 185
    },
    {
      "epoch": 0.372,
      "grad_norm": 1.3675137758255005,
      "learning_rate": 0.00019263705482192877,
      "loss": 0.819,
      "step": 186
    },
    {
      "epoch": 0.374,
      "grad_norm": 0.7006962895393372,
      "learning_rate": 0.00019259703881552623,
      "loss": 0.5898,
      "step": 187
    },
    {
      "epoch": 0.376,
      "grad_norm": 1.3206003904342651,
      "learning_rate": 0.00019255702280912367,
      "loss": 0.8345,
      "step": 188
    },
    {
      "epoch": 0.378,
      "grad_norm": 1.0629329681396484,
      "learning_rate": 0.00019251700680272108,
      "loss": 0.8311,
      "step": 189
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.9563567042350769,
      "learning_rate": 0.00019247699079631854,
      "loss": 0.884,
      "step": 190
    },
    {
      "epoch": 0.382,
      "grad_norm": 1.1882236003875732,
      "learning_rate": 0.00019243697478991598,
      "loss": 0.9584,
      "step": 191
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.2402830123901367,
      "learning_rate": 0.0001923969587835134,
      "loss": 0.6655,
      "step": 192
    },
    {
      "epoch": 0.386,
      "grad_norm": 0.871196985244751,
      "learning_rate": 0.00019235694277711085,
      "loss": 0.4208,
      "step": 193
    },
    {
      "epoch": 0.388,
      "grad_norm": 0.9781157970428467,
      "learning_rate": 0.0001923169267707083,
      "loss": 0.4867,
      "step": 194
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.8023345470428467,
      "learning_rate": 0.00019227691076430573,
      "loss": 0.7569,
      "step": 195
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.74365234375,
      "learning_rate": 0.00019223689475790317,
      "loss": 0.5438,
      "step": 196
    },
    {
      "epoch": 0.394,
      "grad_norm": 1.0836737155914307,
      "learning_rate": 0.0001921968787515006,
      "loss": 0.7644,
      "step": 197
    },
    {
      "epoch": 0.396,
      "grad_norm": 0.897914707660675,
      "learning_rate": 0.00019215686274509807,
      "loss": 0.7045,
      "step": 198
    },
    {
      "epoch": 0.398,
      "grad_norm": 0.7675496339797974,
      "learning_rate": 0.00019211684673869548,
      "loss": 0.4743,
      "step": 199
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.8635857105255127,
      "learning_rate": 0.00019207683073229291,
      "loss": 0.8047,
      "step": 200
    },
    {
      "epoch": 0.402,
      "grad_norm": 0.7742391228675842,
      "learning_rate": 0.00019203681472589038,
      "loss": 0.6831,
      "step": 201
    },
    {
      "epoch": 0.404,
      "grad_norm": 0.8385729789733887,
      "learning_rate": 0.00019199679871948782,
      "loss": 0.6899,
      "step": 202
    },
    {
      "epoch": 0.406,
      "grad_norm": 0.89499431848526,
      "learning_rate": 0.00019195678271308523,
      "loss": 0.619,
      "step": 203
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.9151096343994141,
      "learning_rate": 0.0001919167667066827,
      "loss": 0.6553,
      "step": 204
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.971707284450531,
      "learning_rate": 0.00019187675070028013,
      "loss": 0.5724,
      "step": 205
    },
    {
      "epoch": 0.412,
      "grad_norm": 1.0600289106369019,
      "learning_rate": 0.00019183673469387756,
      "loss": 0.835,
      "step": 206
    },
    {
      "epoch": 0.414,
      "grad_norm": 1.0620962381362915,
      "learning_rate": 0.000191796718687475,
      "loss": 0.6313,
      "step": 207
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.9551624059677124,
      "learning_rate": 0.00019175670268107244,
      "loss": 0.9811,
      "step": 208
    },
    {
      "epoch": 0.418,
      "grad_norm": 0.8735371232032776,
      "learning_rate": 0.00019171668667466988,
      "loss": 0.7932,
      "step": 209
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.9499692320823669,
      "learning_rate": 0.0001916766706682673,
      "loss": 0.6981,
      "step": 210
    },
    {
      "epoch": 0.422,
      "grad_norm": 0.9740123152732849,
      "learning_rate": 0.00019163665466186475,
      "loss": 0.6268,
      "step": 211
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.2546263933181763,
      "learning_rate": 0.00019159663865546221,
      "loss": 0.8078,
      "step": 212
    },
    {
      "epoch": 0.426,
      "grad_norm": 0.8824973106384277,
      "learning_rate": 0.00019155662264905962,
      "loss": 0.5398,
      "step": 213
    },
    {
      "epoch": 0.428,
      "grad_norm": 1.2472034692764282,
      "learning_rate": 0.00019151660664265706,
      "loss": 0.7293,
      "step": 214
    },
    {
      "epoch": 0.43,
      "grad_norm": 1.0889045000076294,
      "learning_rate": 0.00019147659063625453,
      "loss": 0.6575,
      "step": 215
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.8485127091407776,
      "learning_rate": 0.00019143657462985194,
      "loss": 0.6561,
      "step": 216
    },
    {
      "epoch": 0.434,
      "grad_norm": 0.9893955588340759,
      "learning_rate": 0.00019139655862344937,
      "loss": 0.7243,
      "step": 217
    },
    {
      "epoch": 0.436,
      "grad_norm": 1.1651405096054077,
      "learning_rate": 0.00019135654261704684,
      "loss": 0.7286,
      "step": 218
    },
    {
      "epoch": 0.438,
      "grad_norm": 0.9675307869911194,
      "learning_rate": 0.00019131652661064427,
      "loss": 0.7336,
      "step": 219
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.8078059554100037,
      "learning_rate": 0.0001912765106042417,
      "loss": 0.5276,
      "step": 220
    },
    {
      "epoch": 0.442,
      "grad_norm": 0.8323243856430054,
      "learning_rate": 0.00019123649459783915,
      "loss": 0.5586,
      "step": 221
    },
    {
      "epoch": 0.444,
      "grad_norm": 1.0939277410507202,
      "learning_rate": 0.00019119647859143659,
      "loss": 0.7408,
      "step": 222
    },
    {
      "epoch": 0.446,
      "grad_norm": 0.9499965310096741,
      "learning_rate": 0.00019115646258503402,
      "loss": 0.8093,
      "step": 223
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.869689404964447,
      "learning_rate": 0.00019111644657863146,
      "loss": 0.6329,
      "step": 224
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.9221300482749939,
      "learning_rate": 0.0001910764305722289,
      "loss": 0.7643,
      "step": 225
    },
    {
      "epoch": 0.452,
      "grad_norm": 0.9110881686210632,
      "learning_rate": 0.00019103641456582636,
      "loss": 0.7007,
      "step": 226
    },
    {
      "epoch": 0.454,
      "grad_norm": 0.8030298352241516,
      "learning_rate": 0.00019099639855942377,
      "loss": 0.7109,
      "step": 227
    },
    {
      "epoch": 0.456,
      "grad_norm": 1.0036059617996216,
      "learning_rate": 0.0001909563825530212,
      "loss": 0.7176,
      "step": 228
    },
    {
      "epoch": 0.458,
      "grad_norm": 1.1204543113708496,
      "learning_rate": 0.00019091636654661867,
      "loss": 1.3515,
      "step": 229
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.7180495858192444,
      "learning_rate": 0.00019087635054021608,
      "loss": 0.9994,
      "step": 230
    },
    {
      "epoch": 0.462,
      "grad_norm": 0.8171234726905823,
      "learning_rate": 0.00019083633453381352,
      "loss": 0.842,
      "step": 231
    },
    {
      "epoch": 0.464,
      "grad_norm": 1.124143123626709,
      "learning_rate": 0.00019079631852741098,
      "loss": 0.7895,
      "step": 232
    },
    {
      "epoch": 0.466,
      "grad_norm": 0.8807092308998108,
      "learning_rate": 0.00019075630252100842,
      "loss": 0.7887,
      "step": 233
    },
    {
      "epoch": 0.468,
      "grad_norm": 0.9187426567077637,
      "learning_rate": 0.00019071628651460586,
      "loss": 0.6179,
      "step": 234
    },
    {
      "epoch": 0.47,
      "grad_norm": 1.0883722305297852,
      "learning_rate": 0.0001906762705082033,
      "loss": 0.746,
      "step": 235
    },
    {
      "epoch": 0.472,
      "grad_norm": 1.0418165922164917,
      "learning_rate": 0.00019063625450180073,
      "loss": 0.9935,
      "step": 236
    },
    {
      "epoch": 0.474,
      "grad_norm": 0.938402533531189,
      "learning_rate": 0.00019059623849539817,
      "loss": 0.8737,
      "step": 237
    },
    {
      "epoch": 0.476,
      "grad_norm": 0.8508722186088562,
      "learning_rate": 0.0001905562224889956,
      "loss": 0.9804,
      "step": 238
    },
    {
      "epoch": 0.478,
      "grad_norm": 1.0407742261886597,
      "learning_rate": 0.00019051620648259304,
      "loss": 0.4994,
      "step": 239
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.9173406362533569,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.6939,
      "step": 240
    },
    {
      "epoch": 0.482,
      "grad_norm": 1.0939979553222656,
      "learning_rate": 0.00019043617446978792,
      "loss": 0.7391,
      "step": 241
    },
    {
      "epoch": 0.484,
      "grad_norm": 0.8668310642242432,
      "learning_rate": 0.00019039615846338536,
      "loss": 0.6867,
      "step": 242
    },
    {
      "epoch": 0.486,
      "grad_norm": 1.353511095046997,
      "learning_rate": 0.00019035614245698282,
      "loss": 0.857,
      "step": 243
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.7905334234237671,
      "learning_rate": 0.00019031612645058023,
      "loss": 0.7283,
      "step": 244
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.7988908290863037,
      "learning_rate": 0.00019027611044417767,
      "loss": 0.6902,
      "step": 245
    },
    {
      "epoch": 0.492,
      "grad_norm": 0.8125269412994385,
      "learning_rate": 0.00019023609443777513,
      "loss": 0.7095,
      "step": 246
    },
    {
      "epoch": 0.494,
      "grad_norm": 0.8200152516365051,
      "learning_rate": 0.00019019607843137254,
      "loss": 0.6195,
      "step": 247
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.7407172918319702,
      "learning_rate": 0.00019015606242497,
      "loss": 0.6519,
      "step": 248
    },
    {
      "epoch": 0.498,
      "grad_norm": 0.8303446173667908,
      "learning_rate": 0.00019011604641856744,
      "loss": 0.791,
      "step": 249
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.8733075857162476,
      "learning_rate": 0.00019007603041216488,
      "loss": 0.5317,
      "step": 250
    },
    {
      "epoch": 0.502,
      "grad_norm": 0.748121440410614,
      "learning_rate": 0.00019003601440576232,
      "loss": 0.6573,
      "step": 251
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.9366533160209656,
      "learning_rate": 0.00018999599839935975,
      "loss": 0.6641,
      "step": 252
    },
    {
      "epoch": 0.506,
      "grad_norm": 0.8676928281784058,
      "learning_rate": 0.0001899559823929572,
      "loss": 0.6985,
      "step": 253
    },
    {
      "epoch": 0.508,
      "grad_norm": 0.929451584815979,
      "learning_rate": 0.00018991596638655463,
      "loss": 0.5128,
      "step": 254
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.064511775970459,
      "learning_rate": 0.00018987595038015207,
      "loss": 0.6783,
      "step": 255
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.8943939208984375,
      "learning_rate": 0.0001898359343737495,
      "loss": 0.8348,
      "step": 256
    },
    {
      "epoch": 0.514,
      "grad_norm": 0.9049853086471558,
      "learning_rate": 0.00018979591836734697,
      "loss": 0.9808,
      "step": 257
    },
    {
      "epoch": 0.516,
      "grad_norm": 0.8409368991851807,
      "learning_rate": 0.00018975590236094438,
      "loss": 0.7533,
      "step": 258
    },
    {
      "epoch": 0.518,
      "grad_norm": 1.2142423391342163,
      "learning_rate": 0.00018971588635454181,
      "loss": 0.5267,
      "step": 259
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.0286070108413696,
      "learning_rate": 0.00018967587034813928,
      "loss": 0.8909,
      "step": 260
    },
    {
      "epoch": 0.522,
      "grad_norm": 1.1104762554168701,
      "learning_rate": 0.0001896358543417367,
      "loss": 0.8818,
      "step": 261
    },
    {
      "epoch": 0.524,
      "grad_norm": 1.1811922788619995,
      "learning_rate": 0.00018959583833533415,
      "loss": 0.785,
      "step": 262
    },
    {
      "epoch": 0.526,
      "grad_norm": 0.9015246033668518,
      "learning_rate": 0.0001895558223289316,
      "loss": 0.6108,
      "step": 263
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.011355996131897,
      "learning_rate": 0.000189515806322529,
      "loss": 0.6972,
      "step": 264
    },
    {
      "epoch": 0.53,
      "grad_norm": 1.0175034999847412,
      "learning_rate": 0.00018947579031612646,
      "loss": 0.7111,
      "step": 265
    },
    {
      "epoch": 0.532,
      "grad_norm": 1.0717823505401611,
      "learning_rate": 0.0001894357743097239,
      "loss": 0.5875,
      "step": 266
    },
    {
      "epoch": 0.534,
      "grad_norm": 1.1122654676437378,
      "learning_rate": 0.00018939575830332134,
      "loss": 0.744,
      "step": 267
    },
    {
      "epoch": 0.536,
      "grad_norm": 1.0022928714752197,
      "learning_rate": 0.00018935574229691878,
      "loss": 0.7302,
      "step": 268
    },
    {
      "epoch": 0.538,
      "grad_norm": 0.768566370010376,
      "learning_rate": 0.0001893157262905162,
      "loss": 0.5555,
      "step": 269
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.8442056775093079,
      "learning_rate": 0.00018927571028411365,
      "loss": 0.624,
      "step": 270
    },
    {
      "epoch": 0.542,
      "grad_norm": 0.88161301612854,
      "learning_rate": 0.0001892356942777111,
      "loss": 0.7234,
      "step": 271
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.6535950303077698,
      "learning_rate": 0.00018919567827130852,
      "loss": 0.5381,
      "step": 272
    },
    {
      "epoch": 0.546,
      "grad_norm": 0.8054620623588562,
      "learning_rate": 0.000189155662264906,
      "loss": 0.7147,
      "step": 273
    },
    {
      "epoch": 0.548,
      "grad_norm": 0.8276938199996948,
      "learning_rate": 0.00018911564625850343,
      "loss": 0.6631,
      "step": 274
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.9745155572891235,
      "learning_rate": 0.00018907563025210084,
      "loss": 0.8186,
      "step": 275
    },
    {
      "epoch": 0.552,
      "grad_norm": 1.0556353330612183,
      "learning_rate": 0.0001890356142456983,
      "loss": 1.0628,
      "step": 276
    },
    {
      "epoch": 0.554,
      "grad_norm": 0.7573069930076599,
      "learning_rate": 0.00018899559823929574,
      "loss": 0.5985,
      "step": 277
    },
    {
      "epoch": 0.556,
      "grad_norm": 1.2133148908615112,
      "learning_rate": 0.00018895558223289315,
      "loss": 0.7225,
      "step": 278
    },
    {
      "epoch": 0.558,
      "grad_norm": 0.9006068110466003,
      "learning_rate": 0.0001889155662264906,
      "loss": 0.7919,
      "step": 279
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.8646310567855835,
      "learning_rate": 0.00018887555022008805,
      "loss": 0.8895,
      "step": 280
    },
    {
      "epoch": 0.562,
      "grad_norm": 0.9485259652137756,
      "learning_rate": 0.00018883553421368549,
      "loss": 0.7309,
      "step": 281
    },
    {
      "epoch": 0.564,
      "grad_norm": 0.7540104985237122,
      "learning_rate": 0.00018879551820728292,
      "loss": 0.8212,
      "step": 282
    },
    {
      "epoch": 0.566,
      "grad_norm": 0.9495885372161865,
      "learning_rate": 0.00018875550220088036,
      "loss": 0.4234,
      "step": 283
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.9708814024925232,
      "learning_rate": 0.0001887154861944778,
      "loss": 0.7034,
      "step": 284
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.7674104571342468,
      "learning_rate": 0.00018867547018807523,
      "loss": 0.7173,
      "step": 285
    },
    {
      "epoch": 0.572,
      "grad_norm": 1.0840071439743042,
      "learning_rate": 0.00018863545418167267,
      "loss": 0.917,
      "step": 286
    },
    {
      "epoch": 0.574,
      "grad_norm": 1.1254072189331055,
      "learning_rate": 0.00018859543817527014,
      "loss": 0.8686,
      "step": 287
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.9377637505531311,
      "learning_rate": 0.00018855542216886755,
      "loss": 0.6282,
      "step": 288
    },
    {
      "epoch": 0.578,
      "grad_norm": 0.9053687453269958,
      "learning_rate": 0.00018851540616246498,
      "loss": 0.5614,
      "step": 289
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.8224409222602844,
      "learning_rate": 0.00018847539015606245,
      "loss": 0.5641,
      "step": 290
    },
    {
      "epoch": 0.582,
      "grad_norm": 0.8863641619682312,
      "learning_rate": 0.00018843537414965988,
      "loss": 0.8834,
      "step": 291
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.811858057975769,
      "learning_rate": 0.0001883953581432573,
      "loss": 0.5101,
      "step": 292
    },
    {
      "epoch": 0.586,
      "grad_norm": 0.8696991205215454,
      "learning_rate": 0.00018835534213685476,
      "loss": 0.8483,
      "step": 293
    },
    {
      "epoch": 0.588,
      "grad_norm": 0.8013510704040527,
      "learning_rate": 0.0001883153261304522,
      "loss": 0.7982,
      "step": 294
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.8651753067970276,
      "learning_rate": 0.00018827531012404963,
      "loss": 0.5464,
      "step": 295
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.9208561182022095,
      "learning_rate": 0.00018823529411764707,
      "loss": 0.5132,
      "step": 296
    },
    {
      "epoch": 0.594,
      "grad_norm": 0.8552910089492798,
      "learning_rate": 0.0001881952781112445,
      "loss": 0.5554,
      "step": 297
    },
    {
      "epoch": 0.596,
      "grad_norm": 1.1005761623382568,
      "learning_rate": 0.00018815526210484194,
      "loss": 0.7261,
      "step": 298
    },
    {
      "epoch": 0.598,
      "grad_norm": 0.9559516310691833,
      "learning_rate": 0.00018811524609843938,
      "loss": 0.5836,
      "step": 299
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.0944688320159912,
      "learning_rate": 0.00018807523009203682,
      "loss": 0.6062,
      "step": 300
    },
    {
      "epoch": 0.602,
      "grad_norm": 1.1390025615692139,
      "learning_rate": 0.00018803521408563428,
      "loss": 0.8518,
      "step": 301
    },
    {
      "epoch": 0.604,
      "grad_norm": 1.120448350906372,
      "learning_rate": 0.0001879951980792317,
      "loss": 0.8167,
      "step": 302
    },
    {
      "epoch": 0.606,
      "grad_norm": 1.044143557548523,
      "learning_rate": 0.00018795518207282913,
      "loss": 0.4803,
      "step": 303
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.855675995349884,
      "learning_rate": 0.0001879151660664266,
      "loss": 0.8183,
      "step": 304
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.8493300080299377,
      "learning_rate": 0.000187875150060024,
      "loss": 0.8243,
      "step": 305
    },
    {
      "epoch": 0.612,
      "grad_norm": 1.1071442365646362,
      "learning_rate": 0.00018783513405362144,
      "loss": 0.7041,
      "step": 306
    },
    {
      "epoch": 0.614,
      "grad_norm": 0.77329421043396,
      "learning_rate": 0.0001877951180472189,
      "loss": 0.6854,
      "step": 307
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.8277734518051147,
      "learning_rate": 0.00018775510204081634,
      "loss": 0.5851,
      "step": 308
    },
    {
      "epoch": 0.618,
      "grad_norm": 0.9216166138648987,
      "learning_rate": 0.00018771508603441378,
      "loss": 0.8012,
      "step": 309
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.8070897459983826,
      "learning_rate": 0.00018767507002801122,
      "loss": 0.8028,
      "step": 310
    },
    {
      "epoch": 0.622,
      "grad_norm": 0.7778437733650208,
      "learning_rate": 0.00018763505402160865,
      "loss": 0.5599,
      "step": 311
    },
    {
      "epoch": 0.624,
      "grad_norm": 1.0612412691116333,
      "learning_rate": 0.0001875950380152061,
      "loss": 0.8047,
      "step": 312
    },
    {
      "epoch": 0.626,
      "grad_norm": 0.8359618782997131,
      "learning_rate": 0.00018755502200880353,
      "loss": 0.7195,
      "step": 313
    },
    {
      "epoch": 0.628,
      "grad_norm": 0.7961705923080444,
      "learning_rate": 0.00018751500600240096,
      "loss": 0.7445,
      "step": 314
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.7420427203178406,
      "learning_rate": 0.00018747498999599843,
      "loss": 0.5837,
      "step": 315
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.7466630935668945,
      "learning_rate": 0.00018743497398959584,
      "loss": 0.5541,
      "step": 316
    },
    {
      "epoch": 0.634,
      "grad_norm": 0.9842317700386047,
      "learning_rate": 0.00018739495798319328,
      "loss": 0.7813,
      "step": 317
    },
    {
      "epoch": 0.636,
      "grad_norm": 0.8436076641082764,
      "learning_rate": 0.00018735494197679074,
      "loss": 0.7562,
      "step": 318
    },
    {
      "epoch": 0.638,
      "grad_norm": 0.8749016523361206,
      "learning_rate": 0.00018731492597038815,
      "loss": 0.5791,
      "step": 319
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.037420392036438,
      "learning_rate": 0.0001872749099639856,
      "loss": 0.6972,
      "step": 320
    },
    {
      "epoch": 0.642,
      "grad_norm": 0.799246072769165,
      "learning_rate": 0.00018723489395758305,
      "loss": 0.6554,
      "step": 321
    },
    {
      "epoch": 0.644,
      "grad_norm": 0.8352530002593994,
      "learning_rate": 0.0001871948779511805,
      "loss": 0.5888,
      "step": 322
    },
    {
      "epoch": 0.646,
      "grad_norm": 1.0801995992660522,
      "learning_rate": 0.00018715486194477793,
      "loss": 0.8264,
      "step": 323
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.7470101714134216,
      "learning_rate": 0.00018711484593837536,
      "loss": 0.6596,
      "step": 324
    },
    {
      "epoch": 0.65,
      "grad_norm": 1.3598309755325317,
      "learning_rate": 0.0001870748299319728,
      "loss": 0.7509,
      "step": 325
    },
    {
      "epoch": 0.652,
      "grad_norm": 0.9030330181121826,
      "learning_rate": 0.00018703481392557024,
      "loss": 0.6828,
      "step": 326
    },
    {
      "epoch": 0.654,
      "grad_norm": 0.9848232865333557,
      "learning_rate": 0.00018699479791916767,
      "loss": 0.7194,
      "step": 327
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.0303828716278076,
      "learning_rate": 0.0001869547819127651,
      "loss": 0.7664,
      "step": 328
    },
    {
      "epoch": 0.658,
      "grad_norm": 0.9804043173789978,
      "learning_rate": 0.00018691476590636255,
      "loss": 0.8391,
      "step": 329
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.015519380569458,
      "learning_rate": 0.00018687474989995999,
      "loss": 0.8662,
      "step": 330
    },
    {
      "epoch": 0.662,
      "grad_norm": 1.0367897748947144,
      "learning_rate": 0.00018683473389355742,
      "loss": 0.7011,
      "step": 331
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.0993320941925049,
      "learning_rate": 0.0001867947178871549,
      "loss": 0.7193,
      "step": 332
    },
    {
      "epoch": 0.666,
      "grad_norm": 1.0486024618148804,
      "learning_rate": 0.0001867547018807523,
      "loss": 0.5719,
      "step": 333
    },
    {
      "epoch": 0.668,
      "grad_norm": 0.7523289322853088,
      "learning_rate": 0.00018671468587434976,
      "loss": 0.5325,
      "step": 334
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.9366663098335266,
      "learning_rate": 0.0001866746698679472,
      "loss": 0.6238,
      "step": 335
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.234481692314148,
      "learning_rate": 0.0001866346538615446,
      "loss": 0.778,
      "step": 336
    },
    {
      "epoch": 0.674,
      "grad_norm": 0.8928302526473999,
      "learning_rate": 0.00018659463785514207,
      "loss": 0.6993,
      "step": 337
    },
    {
      "epoch": 0.676,
      "grad_norm": 0.902603268623352,
      "learning_rate": 0.0001865546218487395,
      "loss": 0.738,
      "step": 338
    },
    {
      "epoch": 0.678,
      "grad_norm": 1.637108325958252,
      "learning_rate": 0.00018651460584233695,
      "loss": 0.9587,
      "step": 339
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.9415454268455505,
      "learning_rate": 0.00018647458983593438,
      "loss": 0.5415,
      "step": 340
    },
    {
      "epoch": 0.682,
      "grad_norm": 1.1012903451919556,
      "learning_rate": 0.00018643457382953182,
      "loss": 0.6568,
      "step": 341
    },
    {
      "epoch": 0.684,
      "grad_norm": 0.8825573921203613,
      "learning_rate": 0.00018639455782312926,
      "loss": 0.7039,
      "step": 342
    },
    {
      "epoch": 0.686,
      "grad_norm": 0.9596903920173645,
      "learning_rate": 0.0001863545418167267,
      "loss": 0.7745,
      "step": 343
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.9016962647438049,
      "learning_rate": 0.00018631452581032413,
      "loss": 0.6046,
      "step": 344
    },
    {
      "epoch": 0.69,
      "grad_norm": 1.0625050067901611,
      "learning_rate": 0.00018627450980392157,
      "loss": 0.6844,
      "step": 345
    },
    {
      "epoch": 0.692,
      "grad_norm": 1.0519928932189941,
      "learning_rate": 0.00018623449379751903,
      "loss": 0.7502,
      "step": 346
    },
    {
      "epoch": 0.694,
      "grad_norm": 0.8824842572212219,
      "learning_rate": 0.00018619447779111644,
      "loss": 0.8186,
      "step": 347
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.7887817621231079,
      "learning_rate": 0.0001861544617847139,
      "loss": 0.5639,
      "step": 348
    },
    {
      "epoch": 0.698,
      "grad_norm": 0.8834928274154663,
      "learning_rate": 0.00018611444577831135,
      "loss": 0.6848,
      "step": 349
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.789182186126709,
      "learning_rate": 0.00018607442977190876,
      "loss": 0.6998,
      "step": 350
    },
    {
      "epoch": 0.702,
      "grad_norm": 0.751751184463501,
      "learning_rate": 0.00018603441376550622,
      "loss": 0.5994,
      "step": 351
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.7511959671974182,
      "learning_rate": 0.00018599439775910366,
      "loss": 0.8047,
      "step": 352
    },
    {
      "epoch": 0.706,
      "grad_norm": 0.820637047290802,
      "learning_rate": 0.00018595438175270107,
      "loss": 0.7893,
      "step": 353
    },
    {
      "epoch": 0.708,
      "grad_norm": 1.2446588277816772,
      "learning_rate": 0.00018591436574629853,
      "loss": 0.9603,
      "step": 354
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.8621218204498291,
      "learning_rate": 0.00018587434973989597,
      "loss": 0.7607,
      "step": 355
    },
    {
      "epoch": 0.712,
      "grad_norm": 1.174698829650879,
      "learning_rate": 0.0001858343337334934,
      "loss": 0.679,
      "step": 356
    },
    {
      "epoch": 0.714,
      "grad_norm": 1.0390554666519165,
      "learning_rate": 0.00018579431772709084,
      "loss": 0.7214,
      "step": 357
    },
    {
      "epoch": 0.716,
      "grad_norm": 1.0062975883483887,
      "learning_rate": 0.00018575430172068828,
      "loss": 0.6349,
      "step": 358
    },
    {
      "epoch": 0.718,
      "grad_norm": 0.9543635845184326,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.733,
      "step": 359
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.0552722215652466,
      "learning_rate": 0.00018567426970788315,
      "loss": 0.6618,
      "step": 360
    },
    {
      "epoch": 0.722,
      "grad_norm": 1.0609252452850342,
      "learning_rate": 0.0001856342537014806,
      "loss": 0.7116,
      "step": 361
    },
    {
      "epoch": 0.724,
      "grad_norm": 0.8309359550476074,
      "learning_rate": 0.00018559423769507806,
      "loss": 0.6664,
      "step": 362
    },
    {
      "epoch": 0.726,
      "grad_norm": 0.7618370056152344,
      "learning_rate": 0.0001855542216886755,
      "loss": 0.6193,
      "step": 363
    },
    {
      "epoch": 0.728,
      "grad_norm": 1.1074597835540771,
      "learning_rate": 0.0001855142056822729,
      "loss": 0.9184,
      "step": 364
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.8421787023544312,
      "learning_rate": 0.00018547418967587037,
      "loss": 0.5382,
      "step": 365
    },
    {
      "epoch": 0.732,
      "grad_norm": 1.3068801164627075,
      "learning_rate": 0.0001854341736694678,
      "loss": 0.4967,
      "step": 366
    },
    {
      "epoch": 0.734,
      "grad_norm": 0.9907718300819397,
      "learning_rate": 0.00018539415766306521,
      "loss": 0.7862,
      "step": 367
    },
    {
      "epoch": 0.736,
      "grad_norm": 1.0510388612747192,
      "learning_rate": 0.00018535414165666268,
      "loss": 0.5618,
      "step": 368
    },
    {
      "epoch": 0.738,
      "grad_norm": 0.8970263004302979,
      "learning_rate": 0.00018531412565026012,
      "loss": 0.5614,
      "step": 369
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.7629092335700989,
      "learning_rate": 0.00018527410964385755,
      "loss": 0.6376,
      "step": 370
    },
    {
      "epoch": 0.742,
      "grad_norm": 0.7300921082496643,
      "learning_rate": 0.000185234093637455,
      "loss": 0.5514,
      "step": 371
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.8715873956680298,
      "learning_rate": 0.00018519407763105243,
      "loss": 0.6744,
      "step": 372
    },
    {
      "epoch": 0.746,
      "grad_norm": 0.869067907333374,
      "learning_rate": 0.00018515406162464986,
      "loss": 0.5629,
      "step": 373
    },
    {
      "epoch": 0.748,
      "grad_norm": 0.7842347025871277,
      "learning_rate": 0.0001851140456182473,
      "loss": 0.6687,
      "step": 374
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.9609864950180054,
      "learning_rate": 0.00018507402961184474,
      "loss": 0.6091,
      "step": 375
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.8101732134819031,
      "learning_rate": 0.0001850340136054422,
      "loss": 0.6613,
      "step": 376
    },
    {
      "epoch": 0.754,
      "grad_norm": 1.0051997900009155,
      "learning_rate": 0.0001849939975990396,
      "loss": 0.8243,
      "step": 377
    },
    {
      "epoch": 0.756,
      "grad_norm": 0.9121248126029968,
      "learning_rate": 0.00018495398159263705,
      "loss": 0.6595,
      "step": 378
    },
    {
      "epoch": 0.758,
      "grad_norm": 0.9110776782035828,
      "learning_rate": 0.00018491396558623451,
      "loss": 0.5867,
      "step": 379
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.0047239065170288,
      "learning_rate": 0.00018487394957983195,
      "loss": 0.6105,
      "step": 380
    },
    {
      "epoch": 0.762,
      "grad_norm": 1.1640279293060303,
      "learning_rate": 0.00018483393357342936,
      "loss": 0.6267,
      "step": 381
    },
    {
      "epoch": 0.764,
      "grad_norm": 1.070616364479065,
      "learning_rate": 0.00018479391756702683,
      "loss": 0.6428,
      "step": 382
    },
    {
      "epoch": 0.766,
      "grad_norm": 0.9737921953201294,
      "learning_rate": 0.00018475390156062426,
      "loss": 0.8242,
      "step": 383
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.9567099809646606,
      "learning_rate": 0.0001847138855542217,
      "loss": 0.7994,
      "step": 384
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.7734240293502808,
      "learning_rate": 0.00018467386954781914,
      "loss": 0.6064,
      "step": 385
    },
    {
      "epoch": 0.772,
      "grad_norm": 0.7692877054214478,
      "learning_rate": 0.00018463385354141657,
      "loss": 0.72,
      "step": 386
    },
    {
      "epoch": 0.774,
      "grad_norm": 0.9558541178703308,
      "learning_rate": 0.000184593837535014,
      "loss": 0.9856,
      "step": 387
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.8081263899803162,
      "learning_rate": 0.00018455382152861145,
      "loss": 0.6223,
      "step": 388
    },
    {
      "epoch": 0.778,
      "grad_norm": 1.1859095096588135,
      "learning_rate": 0.00018451380552220889,
      "loss": 0.7203,
      "step": 389
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.8365432620048523,
      "learning_rate": 0.00018447378951580635,
      "loss": 0.8971,
      "step": 390
    },
    {
      "epoch": 0.782,
      "grad_norm": 0.7338436841964722,
      "learning_rate": 0.00018443377350940376,
      "loss": 0.68,
      "step": 391
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.8480226397514343,
      "learning_rate": 0.0001843937575030012,
      "loss": 0.7798,
      "step": 392
    },
    {
      "epoch": 0.786,
      "grad_norm": 1.1124935150146484,
      "learning_rate": 0.00018435374149659866,
      "loss": 0.9119,
      "step": 393
    },
    {
      "epoch": 0.788,
      "grad_norm": 0.9587172865867615,
      "learning_rate": 0.00018431372549019607,
      "loss": 0.7407,
      "step": 394
    },
    {
      "epoch": 0.79,
      "grad_norm": 1.119470477104187,
      "learning_rate": 0.0001842737094837935,
      "loss": 0.766,
      "step": 395
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.9113985300064087,
      "learning_rate": 0.00018423369347739097,
      "loss": 0.5558,
      "step": 396
    },
    {
      "epoch": 0.794,
      "grad_norm": 0.7920353412628174,
      "learning_rate": 0.0001841936774709884,
      "loss": 0.5755,
      "step": 397
    },
    {
      "epoch": 0.796,
      "grad_norm": 1.096689224243164,
      "learning_rate": 0.00018415366146458585,
      "loss": 0.7713,
      "step": 398
    },
    {
      "epoch": 0.798,
      "grad_norm": 0.8764595985412598,
      "learning_rate": 0.00018411364545818328,
      "loss": 0.8211,
      "step": 399
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.9486632943153381,
      "learning_rate": 0.00018407362945178072,
      "loss": 0.6828,
      "step": 400
    },
    {
      "epoch": 0.802,
      "grad_norm": 0.849711000919342,
      "learning_rate": 0.00018403361344537816,
      "loss": 0.5359,
      "step": 401
    },
    {
      "epoch": 0.804,
      "grad_norm": 0.7921581864356995,
      "learning_rate": 0.0001839935974389756,
      "loss": 0.8659,
      "step": 402
    },
    {
      "epoch": 0.806,
      "grad_norm": 0.9328872561454773,
      "learning_rate": 0.00018395358143257303,
      "loss": 0.5321,
      "step": 403
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.853043794631958,
      "learning_rate": 0.0001839135654261705,
      "loss": 1.0232,
      "step": 404
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.8085250854492188,
      "learning_rate": 0.0001838735494197679,
      "loss": 0.9074,
      "step": 405
    },
    {
      "epoch": 0.812,
      "grad_norm": 0.9248250126838684,
      "learning_rate": 0.00018383353341336534,
      "loss": 0.6774,
      "step": 406
    },
    {
      "epoch": 0.814,
      "grad_norm": 0.8795982003211975,
      "learning_rate": 0.0001837935174069628,
      "loss": 0.6901,
      "step": 407
    },
    {
      "epoch": 0.816,
      "grad_norm": 1.103008508682251,
      "learning_rate": 0.00018375350140056022,
      "loss": 0.589,
      "step": 408
    },
    {
      "epoch": 0.818,
      "grad_norm": 1.3590466976165771,
      "learning_rate": 0.00018371348539415768,
      "loss": 1.066,
      "step": 409
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.7483341693878174,
      "learning_rate": 0.00018367346938775512,
      "loss": 0.8435,
      "step": 410
    },
    {
      "epoch": 0.822,
      "grad_norm": 1.1286609172821045,
      "learning_rate": 0.00018363345338135256,
      "loss": 0.7496,
      "step": 411
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.9116888046264648,
      "learning_rate": 0.00018359343737495,
      "loss": 0.5922,
      "step": 412
    },
    {
      "epoch": 0.826,
      "grad_norm": 1.210038661956787,
      "learning_rate": 0.00018355342136854743,
      "loss": 0.7977,
      "step": 413
    },
    {
      "epoch": 0.828,
      "grad_norm": 0.8360371589660645,
      "learning_rate": 0.00018351340536214487,
      "loss": 0.7155,
      "step": 414
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.8609020709991455,
      "learning_rate": 0.0001834733893557423,
      "loss": 0.6868,
      "step": 415
    },
    {
      "epoch": 0.832,
      "grad_norm": 1.0062627792358398,
      "learning_rate": 0.00018343337334933974,
      "loss": 0.6912,
      "step": 416
    },
    {
      "epoch": 0.834,
      "grad_norm": 1.6466220617294312,
      "learning_rate": 0.00018339335734293718,
      "loss": 0.6133,
      "step": 417
    },
    {
      "epoch": 0.836,
      "grad_norm": 1.1368483304977417,
      "learning_rate": 0.00018335334133653462,
      "loss": 0.6563,
      "step": 418
    },
    {
      "epoch": 0.838,
      "grad_norm": 0.9042803645133972,
      "learning_rate": 0.00018331332533013205,
      "loss": 0.5787,
      "step": 419
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.355832815170288,
      "learning_rate": 0.0001832733093237295,
      "loss": 0.6881,
      "step": 420
    },
    {
      "epoch": 0.842,
      "grad_norm": 0.8892641663551331,
      "learning_rate": 0.00018323329331732696,
      "loss": 0.5736,
      "step": 421
    },
    {
      "epoch": 0.844,
      "grad_norm": 0.792163610458374,
      "learning_rate": 0.00018319327731092437,
      "loss": 0.5014,
      "step": 422
    },
    {
      "epoch": 0.846,
      "grad_norm": 0.9523109197616577,
      "learning_rate": 0.00018315326130452183,
      "loss": 0.6124,
      "step": 423
    },
    {
      "epoch": 0.848,
      "grad_norm": 1.0477432012557983,
      "learning_rate": 0.00018311324529811927,
      "loss": 0.5674,
      "step": 424
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.0349332094192505,
      "learning_rate": 0.00018307322929171668,
      "loss": 0.8534,
      "step": 425
    },
    {
      "epoch": 0.852,
      "grad_norm": 0.9261445999145508,
      "learning_rate": 0.00018303321328531414,
      "loss": 0.5545,
      "step": 426
    },
    {
      "epoch": 0.854,
      "grad_norm": 0.8053192496299744,
      "learning_rate": 0.00018299319727891158,
      "loss": 0.6896,
      "step": 427
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.7930742502212524,
      "learning_rate": 0.00018295318127250902,
      "loss": 0.6507,
      "step": 428
    },
    {
      "epoch": 0.858,
      "grad_norm": 0.9406780004501343,
      "learning_rate": 0.00018291316526610645,
      "loss": 1.0385,
      "step": 429
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.9963036775588989,
      "learning_rate": 0.0001828731492597039,
      "loss": 0.476,
      "step": 430
    },
    {
      "epoch": 0.862,
      "grad_norm": 0.7984018325805664,
      "learning_rate": 0.00018283313325330133,
      "loss": 0.532,
      "step": 431
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.9909313917160034,
      "learning_rate": 0.00018279311724689876,
      "loss": 0.7872,
      "step": 432
    },
    {
      "epoch": 0.866,
      "grad_norm": 0.866631805896759,
      "learning_rate": 0.0001827531012404962,
      "loss": 0.6377,
      "step": 433
    },
    {
      "epoch": 0.868,
      "grad_norm": 0.9061357378959656,
      "learning_rate": 0.00018271308523409364,
      "loss": 0.6587,
      "step": 434
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.8908079862594604,
      "learning_rate": 0.0001826730692276911,
      "loss": 0.8367,
      "step": 435
    },
    {
      "epoch": 0.872,
      "grad_norm": 1.0192705392837524,
      "learning_rate": 0.0001826330532212885,
      "loss": 0.5395,
      "step": 436
    },
    {
      "epoch": 0.874,
      "grad_norm": 0.9936214089393616,
      "learning_rate": 0.00018259303721488598,
      "loss": 0.5547,
      "step": 437
    },
    {
      "epoch": 0.876,
      "grad_norm": 1.1518105268478394,
      "learning_rate": 0.00018255302120848341,
      "loss": 0.7423,
      "step": 438
    },
    {
      "epoch": 0.878,
      "grad_norm": 0.8654768466949463,
      "learning_rate": 0.00018251300520208082,
      "loss": 0.5738,
      "step": 439
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.8984208106994629,
      "learning_rate": 0.0001824729891956783,
      "loss": 0.6264,
      "step": 440
    },
    {
      "epoch": 0.882,
      "grad_norm": 0.916939914226532,
      "learning_rate": 0.00018243297318927573,
      "loss": 0.9127,
      "step": 441
    },
    {
      "epoch": 0.884,
      "grad_norm": 1.26111900806427,
      "learning_rate": 0.00018239295718287314,
      "loss": 0.7426,
      "step": 442
    },
    {
      "epoch": 0.886,
      "grad_norm": 0.6051502227783203,
      "learning_rate": 0.0001823529411764706,
      "loss": 0.6428,
      "step": 443
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.8050684332847595,
      "learning_rate": 0.00018231292517006804,
      "loss": 0.9308,
      "step": 444
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.872821569442749,
      "learning_rate": 0.00018227290916366547,
      "loss": 0.6937,
      "step": 445
    },
    {
      "epoch": 0.892,
      "grad_norm": 0.7318564057350159,
      "learning_rate": 0.0001822328931572629,
      "loss": 0.7563,
      "step": 446
    },
    {
      "epoch": 0.894,
      "grad_norm": 1.976357340812683,
      "learning_rate": 0.00018219287715086035,
      "loss": 0.7414,
      "step": 447
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.069830060005188,
      "learning_rate": 0.00018215286114445779,
      "loss": 0.732,
      "step": 448
    },
    {
      "epoch": 0.898,
      "grad_norm": 1.1543080806732178,
      "learning_rate": 0.00018211284513805522,
      "loss": 0.9533,
      "step": 449
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.0916662216186523,
      "learning_rate": 0.00018207282913165266,
      "loss": 0.8984,
      "step": 450
    },
    {
      "epoch": 0.902,
      "grad_norm": 0.966383159160614,
      "learning_rate": 0.00018203281312525012,
      "loss": 0.7282,
      "step": 451
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.8599497675895691,
      "learning_rate": 0.00018199279711884756,
      "loss": 0.6341,
      "step": 452
    },
    {
      "epoch": 0.906,
      "grad_norm": 0.9115334749221802,
      "learning_rate": 0.00018195278111244497,
      "loss": 0.5882,
      "step": 453
    },
    {
      "epoch": 0.908,
      "grad_norm": 0.8789594769477844,
      "learning_rate": 0.00018191276510604243,
      "loss": 0.484,
      "step": 454
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.932789146900177,
      "learning_rate": 0.00018187274909963987,
      "loss": 0.8813,
      "step": 455
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.0407319068908691,
      "learning_rate": 0.00018183273309323728,
      "loss": 0.5751,
      "step": 456
    },
    {
      "epoch": 0.914,
      "grad_norm": 0.8717420697212219,
      "learning_rate": 0.00018179271708683475,
      "loss": 0.506,
      "step": 457
    },
    {
      "epoch": 0.916,
      "grad_norm": 0.8526805639266968,
      "learning_rate": 0.00018175270108043218,
      "loss": 0.6339,
      "step": 458
    },
    {
      "epoch": 0.918,
      "grad_norm": 1.2701759338378906,
      "learning_rate": 0.00018171268507402962,
      "loss": 1.0003,
      "step": 459
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.8175613284111023,
      "learning_rate": 0.00018167266906762706,
      "loss": 0.6501,
      "step": 460
    },
    {
      "epoch": 0.922,
      "grad_norm": 0.8554035425186157,
      "learning_rate": 0.0001816326530612245,
      "loss": 0.5529,
      "step": 461
    },
    {
      "epoch": 0.924,
      "grad_norm": 0.8336797952651978,
      "learning_rate": 0.00018159263705482196,
      "loss": 0.5988,
      "step": 462
    },
    {
      "epoch": 0.926,
      "grad_norm": 0.7925403118133545,
      "learning_rate": 0.00018155262104841937,
      "loss": 0.8187,
      "step": 463
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.8607579469680786,
      "learning_rate": 0.0001815126050420168,
      "loss": 0.619,
      "step": 464
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.7729213833808899,
      "learning_rate": 0.00018147258903561427,
      "loss": 0.5948,
      "step": 465
    },
    {
      "epoch": 0.932,
      "grad_norm": 1.191397786140442,
      "learning_rate": 0.00018143257302921168,
      "loss": 0.8059,
      "step": 466
    },
    {
      "epoch": 0.934,
      "grad_norm": 0.9451543688774109,
      "learning_rate": 0.00018139255702280912,
      "loss": 0.7525,
      "step": 467
    },
    {
      "epoch": 0.936,
      "grad_norm": 1.4376124143600464,
      "learning_rate": 0.00018135254101640658,
      "loss": 0.916,
      "step": 468
    },
    {
      "epoch": 0.938,
      "grad_norm": 0.8954967260360718,
      "learning_rate": 0.00018131252501000402,
      "loss": 0.6165,
      "step": 469
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.0677090883255005,
      "learning_rate": 0.00018127250900360146,
      "loss": 0.5274,
      "step": 470
    },
    {
      "epoch": 0.942,
      "grad_norm": 0.9213704466819763,
      "learning_rate": 0.0001812324929971989,
      "loss": 0.6008,
      "step": 471
    },
    {
      "epoch": 0.944,
      "grad_norm": 1.161563515663147,
      "learning_rate": 0.00018119247699079633,
      "loss": 0.6977,
      "step": 472
    },
    {
      "epoch": 0.946,
      "grad_norm": 0.864751398563385,
      "learning_rate": 0.00018115246098439377,
      "loss": 0.5296,
      "step": 473
    },
    {
      "epoch": 0.948,
      "grad_norm": 1.1780481338500977,
      "learning_rate": 0.0001811124449779912,
      "loss": 0.6497,
      "step": 474
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.1349239349365234,
      "learning_rate": 0.00018107242897158864,
      "loss": 0.7379,
      "step": 475
    },
    {
      "epoch": 0.952,
      "grad_norm": 1.0223993062973022,
      "learning_rate": 0.0001810324129651861,
      "loss": 0.7772,
      "step": 476
    },
    {
      "epoch": 0.954,
      "grad_norm": 1.0189412832260132,
      "learning_rate": 0.00018099239695878352,
      "loss": 0.5654,
      "step": 477
    },
    {
      "epoch": 0.956,
      "grad_norm": 0.8913608193397522,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.6693,
      "step": 478
    },
    {
      "epoch": 0.958,
      "grad_norm": 0.9032777547836304,
      "learning_rate": 0.00018091236494597842,
      "loss": 0.7845,
      "step": 479
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.8735088109970093,
      "learning_rate": 0.00018087234893957583,
      "loss": 0.6855,
      "step": 480
    },
    {
      "epoch": 0.962,
      "grad_norm": 1.1237525939941406,
      "learning_rate": 0.00018083233293317326,
      "loss": 0.6053,
      "step": 481
    },
    {
      "epoch": 0.964,
      "grad_norm": 0.8302886486053467,
      "learning_rate": 0.00018079231692677073,
      "loss": 0.7223,
      "step": 482
    },
    {
      "epoch": 0.966,
      "grad_norm": 1.036895513534546,
      "learning_rate": 0.00018075230092036814,
      "loss": 0.5626,
      "step": 483
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.8636153340339661,
      "learning_rate": 0.0001807122849139656,
      "loss": 0.5369,
      "step": 484
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.0498114824295044,
      "learning_rate": 0.00018067226890756304,
      "loss": 0.8161,
      "step": 485
    },
    {
      "epoch": 0.972,
      "grad_norm": 0.8117936253547668,
      "learning_rate": 0.00018063225290116048,
      "loss": 0.6734,
      "step": 486
    },
    {
      "epoch": 0.974,
      "grad_norm": 1.0182421207427979,
      "learning_rate": 0.00018059223689475791,
      "loss": 0.7379,
      "step": 487
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.9270890355110168,
      "learning_rate": 0.00018055222088835535,
      "loss": 0.4754,
      "step": 488
    },
    {
      "epoch": 0.978,
      "grad_norm": 0.8910108208656311,
      "learning_rate": 0.0001805122048819528,
      "loss": 0.6561,
      "step": 489
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.9070796966552734,
      "learning_rate": 0.00018047218887555023,
      "loss": 0.6472,
      "step": 490
    },
    {
      "epoch": 0.982,
      "grad_norm": 0.9575451612472534,
      "learning_rate": 0.00018043217286914766,
      "loss": 0.7282,
      "step": 491
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.8542510271072388,
      "learning_rate": 0.0001803921568627451,
      "loss": 0.9102,
      "step": 492
    },
    {
      "epoch": 0.986,
      "grad_norm": 0.8508243560791016,
      "learning_rate": 0.00018035214085634256,
      "loss": 0.8068,
      "step": 493
    },
    {
      "epoch": 0.988,
      "grad_norm": 0.8436374068260193,
      "learning_rate": 0.00018031212484993997,
      "loss": 0.8098,
      "step": 494
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.7050228118896484,
      "learning_rate": 0.0001802721088435374,
      "loss": 0.6426,
      "step": 495
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.9386613965034485,
      "learning_rate": 0.00018023209283713488,
      "loss": 0.8719,
      "step": 496
    },
    {
      "epoch": 0.994,
      "grad_norm": 0.9588035345077515,
      "learning_rate": 0.00018019207683073229,
      "loss": 0.6189,
      "step": 497
    },
    {
      "epoch": 0.996,
      "grad_norm": 1.3055716753005981,
      "learning_rate": 0.00018015206082432975,
      "loss": 0.6654,
      "step": 498
    },
    {
      "epoch": 0.998,
      "grad_norm": 1.09621000289917,
      "learning_rate": 0.0001801120448179272,
      "loss": 0.5323,
      "step": 499
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.9623786807060242,
      "learning_rate": 0.00018007202881152462,
      "loss": 0.6129,
      "step": 500
    },
    {
      "epoch": 1.002,
      "grad_norm": 0.8214089274406433,
      "learning_rate": 0.00018003201280512206,
      "loss": 0.621,
      "step": 501
    },
    {
      "epoch": 1.004,
      "grad_norm": 0.9097223281860352,
      "learning_rate": 0.0001799919967987195,
      "loss": 0.6636,
      "step": 502
    },
    {
      "epoch": 1.006,
      "grad_norm": 0.8161148428916931,
      "learning_rate": 0.00017995198079231694,
      "loss": 0.5548,
      "step": 503
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.7074782252311707,
      "learning_rate": 0.00017991196478591437,
      "loss": 0.5191,
      "step": 504
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.8085071444511414,
      "learning_rate": 0.0001798719487795118,
      "loss": 0.5774,
      "step": 505
    },
    {
      "epoch": 1.012,
      "grad_norm": 0.7885637283325195,
      "learning_rate": 0.00017983193277310925,
      "loss": 0.5167,
      "step": 506
    },
    {
      "epoch": 1.014,
      "grad_norm": 0.7614253163337708,
      "learning_rate": 0.00017979191676670668,
      "loss": 0.6634,
      "step": 507
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.8025088310241699,
      "learning_rate": 0.00017975190076030412,
      "loss": 0.6466,
      "step": 508
    },
    {
      "epoch": 1.018,
      "grad_norm": 0.9734378457069397,
      "learning_rate": 0.00017971188475390156,
      "loss": 0.601,
      "step": 509
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.7968488335609436,
      "learning_rate": 0.00017967186874749902,
      "loss": 0.6405,
      "step": 510
    },
    {
      "epoch": 1.022,
      "grad_norm": 0.7985473275184631,
      "learning_rate": 0.00017963185274109643,
      "loss": 0.5834,
      "step": 511
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.9036059975624084,
      "learning_rate": 0.0001795918367346939,
      "loss": 0.7312,
      "step": 512
    },
    {
      "epoch": 1.026,
      "grad_norm": 0.7501940727233887,
      "learning_rate": 0.00017955182072829133,
      "loss": 0.5719,
      "step": 513
    },
    {
      "epoch": 1.028,
      "grad_norm": 0.8171011209487915,
      "learning_rate": 0.00017951180472188874,
      "loss": 0.6559,
      "step": 514
    },
    {
      "epoch": 1.03,
      "grad_norm": 1.0623703002929688,
      "learning_rate": 0.0001794717887154862,
      "loss": 0.7417,
      "step": 515
    },
    {
      "epoch": 1.032,
      "grad_norm": 1.275913119316101,
      "learning_rate": 0.00017943177270908365,
      "loss": 0.7873,
      "step": 516
    },
    {
      "epoch": 1.034,
      "grad_norm": 1.0738383531570435,
      "learning_rate": 0.00017939175670268108,
      "loss": 0.6019,
      "step": 517
    },
    {
      "epoch": 1.036,
      "grad_norm": 0.7150505185127258,
      "learning_rate": 0.00017935174069627852,
      "loss": 0.3701,
      "step": 518
    },
    {
      "epoch": 1.038,
      "grad_norm": 0.8696085214614868,
      "learning_rate": 0.00017931172468987596,
      "loss": 0.4668,
      "step": 519
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.2122328281402588,
      "learning_rate": 0.0001792717086834734,
      "loss": 0.7077,
      "step": 520
    },
    {
      "epoch": 1.042,
      "grad_norm": 0.9142858386039734,
      "learning_rate": 0.00017923169267707083,
      "loss": 0.7015,
      "step": 521
    },
    {
      "epoch": 1.044,
      "grad_norm": 0.8766498565673828,
      "learning_rate": 0.00017919167667066827,
      "loss": 0.5963,
      "step": 522
    },
    {
      "epoch": 1.046,
      "grad_norm": 0.7383868098258972,
      "learning_rate": 0.0001791516606642657,
      "loss": 0.3843,
      "step": 523
    },
    {
      "epoch": 1.048,
      "grad_norm": 1.010263442993164,
      "learning_rate": 0.00017911164465786317,
      "loss": 0.6593,
      "step": 524
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.6465957164764404,
      "learning_rate": 0.00017907162865146058,
      "loss": 0.6535,
      "step": 525
    },
    {
      "epoch": 1.052,
      "grad_norm": 0.6987144947052002,
      "learning_rate": 0.00017903161264505804,
      "loss": 0.532,
      "step": 526
    },
    {
      "epoch": 1.054,
      "grad_norm": 0.8780688643455505,
      "learning_rate": 0.00017899159663865548,
      "loss": 0.8025,
      "step": 527
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.8347105979919434,
      "learning_rate": 0.0001789515806322529,
      "loss": 0.4605,
      "step": 528
    },
    {
      "epoch": 1.058,
      "grad_norm": 0.9306262135505676,
      "learning_rate": 0.00017891156462585036,
      "loss": 0.5078,
      "step": 529
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.9706998467445374,
      "learning_rate": 0.0001788715486194478,
      "loss": 0.5117,
      "step": 530
    },
    {
      "epoch": 1.062,
      "grad_norm": 1.3965179920196533,
      "learning_rate": 0.0001788315326130452,
      "loss": 0.7714,
      "step": 531
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.8055754899978638,
      "learning_rate": 0.00017879151660664267,
      "loss": 0.7413,
      "step": 532
    },
    {
      "epoch": 1.066,
      "grad_norm": 0.788706362247467,
      "learning_rate": 0.0001787515006002401,
      "loss": 0.6315,
      "step": 533
    },
    {
      "epoch": 1.068,
      "grad_norm": 0.7813794612884521,
      "learning_rate": 0.00017871148459383754,
      "loss": 0.4345,
      "step": 534
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.8102042078971863,
      "learning_rate": 0.00017867146858743498,
      "loss": 0.5922,
      "step": 535
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.8078272938728333,
      "learning_rate": 0.00017863145258103242,
      "loss": 0.4978,
      "step": 536
    },
    {
      "epoch": 1.074,
      "grad_norm": 0.7873803377151489,
      "learning_rate": 0.00017859143657462988,
      "loss": 0.6822,
      "step": 537
    },
    {
      "epoch": 1.076,
      "grad_norm": 1.3133906126022339,
      "learning_rate": 0.0001785514205682273,
      "loss": 0.5093,
      "step": 538
    },
    {
      "epoch": 1.078,
      "grad_norm": 0.8085002303123474,
      "learning_rate": 0.00017851140456182473,
      "loss": 0.4661,
      "step": 539
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.9853790998458862,
      "learning_rate": 0.0001784713885554222,
      "loss": 0.4984,
      "step": 540
    },
    {
      "epoch": 1.082,
      "grad_norm": 0.9045859575271606,
      "learning_rate": 0.00017843137254901963,
      "loss": 0.413,
      "step": 541
    },
    {
      "epoch": 1.084,
      "grad_norm": 1.489405870437622,
      "learning_rate": 0.00017839135654261704,
      "loss": 0.506,
      "step": 542
    },
    {
      "epoch": 1.086,
      "grad_norm": 1.1933625936508179,
      "learning_rate": 0.0001783513405362145,
      "loss": 0.5,
      "step": 543
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.8299921154975891,
      "learning_rate": 0.00017831132452981194,
      "loss": 0.6177,
      "step": 544
    },
    {
      "epoch": 1.09,
      "grad_norm": 1.0230159759521484,
      "learning_rate": 0.00017827130852340938,
      "loss": 0.5114,
      "step": 545
    },
    {
      "epoch": 1.092,
      "grad_norm": 0.7568209767341614,
      "learning_rate": 0.00017823129251700681,
      "loss": 0.6842,
      "step": 546
    },
    {
      "epoch": 1.094,
      "grad_norm": 1.0300066471099854,
      "learning_rate": 0.00017819127651060425,
      "loss": 0.4889,
      "step": 547
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.9418200850486755,
      "learning_rate": 0.0001781512605042017,
      "loss": 0.6973,
      "step": 548
    },
    {
      "epoch": 1.098,
      "grad_norm": 1.4367755651474,
      "learning_rate": 0.00017811124449779913,
      "loss": 0.5833,
      "step": 549
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.2185381650924683,
      "learning_rate": 0.00017807122849139656,
      "loss": 0.5914,
      "step": 550
    },
    {
      "epoch": 1.102,
      "grad_norm": 1.00178062915802,
      "learning_rate": 0.00017803121248499403,
      "loss": 0.7193,
      "step": 551
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.8982461094856262,
      "learning_rate": 0.00017799119647859144,
      "loss": 0.5051,
      "step": 552
    },
    {
      "epoch": 1.106,
      "grad_norm": 0.8707364797592163,
      "learning_rate": 0.00017795118047218887,
      "loss": 0.4343,
      "step": 553
    },
    {
      "epoch": 1.108,
      "grad_norm": 0.9622125029563904,
      "learning_rate": 0.00017791116446578634,
      "loss": 0.6768,
      "step": 554
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.8949439525604248,
      "learning_rate": 0.00017787114845938375,
      "loss": 0.7031,
      "step": 555
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.7327916026115417,
      "learning_rate": 0.00017783113245298119,
      "loss": 0.5206,
      "step": 556
    },
    {
      "epoch": 1.114,
      "grad_norm": 0.8273465037345886,
      "learning_rate": 0.00017779111644657865,
      "loss": 0.6827,
      "step": 557
    },
    {
      "epoch": 1.116,
      "grad_norm": 0.7956949472427368,
      "learning_rate": 0.0001777511004401761,
      "loss": 0.5648,
      "step": 558
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 0.7888972759246826,
      "learning_rate": 0.00017771108443377352,
      "loss": 0.6435,
      "step": 559
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.9115406274795532,
      "learning_rate": 0.00017767106842737096,
      "loss": 0.514,
      "step": 560
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 0.9089226126670837,
      "learning_rate": 0.0001776310524209684,
      "loss": 0.54,
      "step": 561
    },
    {
      "epoch": 1.124,
      "grad_norm": 0.8261673450469971,
      "learning_rate": 0.00017759103641456584,
      "loss": 0.6915,
      "step": 562
    },
    {
      "epoch": 1.126,
      "grad_norm": 0.9115936160087585,
      "learning_rate": 0.00017755102040816327,
      "loss": 0.5092,
      "step": 563
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.7251215577125549,
      "learning_rate": 0.0001775110044017607,
      "loss": 0.4423,
      "step": 564
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.9204118847846985,
      "learning_rate": 0.00017747098839535817,
      "loss": 0.6499,
      "step": 565
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 0.8930321931838989,
      "learning_rate": 0.00017743097238895558,
      "loss": 0.3946,
      "step": 566
    },
    {
      "epoch": 1.134,
      "grad_norm": 0.9443559050559998,
      "learning_rate": 0.00017739095638255302,
      "loss": 0.7368,
      "step": 567
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.3171428442001343,
      "learning_rate": 0.00017735094037615049,
      "loss": 0.7673,
      "step": 568
    },
    {
      "epoch": 1.138,
      "grad_norm": 0.8699580430984497,
      "learning_rate": 0.0001773109243697479,
      "loss": 0.6504,
      "step": 569
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 0.9485766887664795,
      "learning_rate": 0.00017727090836334533,
      "loss": 0.6899,
      "step": 570
    },
    {
      "epoch": 1.142,
      "grad_norm": 0.9017691016197205,
      "learning_rate": 0.0001772308923569428,
      "loss": 0.4985,
      "step": 571
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.7360055446624756,
      "learning_rate": 0.0001771908763505402,
      "loss": 0.4003,
      "step": 572
    },
    {
      "epoch": 1.146,
      "grad_norm": 1.0187568664550781,
      "learning_rate": 0.00017715086034413767,
      "loss": 0.6648,
      "step": 573
    },
    {
      "epoch": 1.148,
      "grad_norm": 1.0167449712753296,
      "learning_rate": 0.0001771108443377351,
      "loss": 0.443,
      "step": 574
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.7763217687606812,
      "learning_rate": 0.00017707082833133255,
      "loss": 0.6443,
      "step": 575
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.7056629061698914,
      "learning_rate": 0.00017703081232492998,
      "loss": 0.5953,
      "step": 576
    },
    {
      "epoch": 1.154,
      "grad_norm": 0.9469751715660095,
      "learning_rate": 0.00017699079631852742,
      "loss": 0.5291,
      "step": 577
    },
    {
      "epoch": 1.156,
      "grad_norm": 0.7219312191009521,
      "learning_rate": 0.00017695078031212486,
      "loss": 0.458,
      "step": 578
    },
    {
      "epoch": 1.158,
      "grad_norm": 1.043749213218689,
      "learning_rate": 0.0001769107643057223,
      "loss": 0.6996,
      "step": 579
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.9039927124977112,
      "learning_rate": 0.00017687074829931973,
      "loss": 0.6006,
      "step": 580
    },
    {
      "epoch": 1.162,
      "grad_norm": 0.9976288080215454,
      "learning_rate": 0.00017683073229291717,
      "loss": 0.5404,
      "step": 581
    },
    {
      "epoch": 1.164,
      "grad_norm": 1.0201270580291748,
      "learning_rate": 0.00017679071628651463,
      "loss": 0.7022,
      "step": 582
    },
    {
      "epoch": 1.166,
      "grad_norm": 0.964789628982544,
      "learning_rate": 0.00017675070028011204,
      "loss": 0.5964,
      "step": 583
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.8642668724060059,
      "learning_rate": 0.00017671068427370948,
      "loss": 0.4945,
      "step": 584
    },
    {
      "epoch": 1.17,
      "grad_norm": 1.01886785030365,
      "learning_rate": 0.00017667066826730694,
      "loss": 0.5917,
      "step": 585
    },
    {
      "epoch": 1.172,
      "grad_norm": 0.7986592650413513,
      "learning_rate": 0.00017663065226090435,
      "loss": 0.4369,
      "step": 586
    },
    {
      "epoch": 1.174,
      "grad_norm": 0.8796225190162659,
      "learning_rate": 0.00017659063625450182,
      "loss": 0.484,
      "step": 587
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.8429057002067566,
      "learning_rate": 0.00017655062024809926,
      "loss": 0.5611,
      "step": 588
    },
    {
      "epoch": 1.178,
      "grad_norm": 0.8166570067405701,
      "learning_rate": 0.0001765106042416967,
      "loss": 0.6321,
      "step": 589
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.8425673842430115,
      "learning_rate": 0.00017647058823529413,
      "loss": 0.562,
      "step": 590
    },
    {
      "epoch": 1.182,
      "grad_norm": 0.9507769346237183,
      "learning_rate": 0.00017643057222889157,
      "loss": 0.674,
      "step": 591
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.0370097160339355,
      "learning_rate": 0.000176390556222489,
      "loss": 0.56,
      "step": 592
    },
    {
      "epoch": 1.186,
      "grad_norm": 0.994991660118103,
      "learning_rate": 0.00017635054021608644,
      "loss": 0.6301,
      "step": 593
    },
    {
      "epoch": 1.188,
      "grad_norm": 0.8790720701217651,
      "learning_rate": 0.00017631052420968388,
      "loss": 0.4488,
      "step": 594
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.8190792202949524,
      "learning_rate": 0.00017627050820328131,
      "loss": 0.6605,
      "step": 595
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.8618162870407104,
      "learning_rate": 0.00017623049219687875,
      "loss": 0.5687,
      "step": 596
    },
    {
      "epoch": 1.194,
      "grad_norm": 0.892555296421051,
      "learning_rate": 0.0001761904761904762,
      "loss": 0.7519,
      "step": 597
    },
    {
      "epoch": 1.196,
      "grad_norm": 1.1111938953399658,
      "learning_rate": 0.00017615046018407365,
      "loss": 0.8713,
      "step": 598
    },
    {
      "epoch": 1.198,
      "grad_norm": 0.7878332138061523,
      "learning_rate": 0.0001761104441776711,
      "loss": 0.5576,
      "step": 599
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.8802340030670166,
      "learning_rate": 0.0001760704281712685,
      "loss": 0.6031,
      "step": 600
    },
    {
      "epoch": 1.202,
      "grad_norm": 0.9320932030677795,
      "learning_rate": 0.00017603041216486596,
      "loss": 0.5612,
      "step": 601
    },
    {
      "epoch": 1.204,
      "grad_norm": 1.1037577390670776,
      "learning_rate": 0.0001759903961584634,
      "loss": 0.5893,
      "step": 602
    },
    {
      "epoch": 1.206,
      "grad_norm": 1.387222170829773,
      "learning_rate": 0.0001759503801520608,
      "loss": 0.8414,
      "step": 603
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.9415119290351868,
      "learning_rate": 0.00017591036414565828,
      "loss": 0.6051,
      "step": 604
    },
    {
      "epoch": 1.21,
      "grad_norm": 1.010910987854004,
      "learning_rate": 0.0001758703481392557,
      "loss": 0.6246,
      "step": 605
    },
    {
      "epoch": 1.212,
      "grad_norm": 0.8769938945770264,
      "learning_rate": 0.00017583033213285315,
      "loss": 0.6173,
      "step": 606
    },
    {
      "epoch": 1.214,
      "grad_norm": 0.9381066560745239,
      "learning_rate": 0.0001757903161264506,
      "loss": 0.42,
      "step": 607
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.7955170273780823,
      "learning_rate": 0.00017575030012004802,
      "loss": 0.605,
      "step": 608
    },
    {
      "epoch": 1.218,
      "grad_norm": 0.7836009860038757,
      "learning_rate": 0.00017571028411364546,
      "loss": 0.4027,
      "step": 609
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.0419002771377563,
      "learning_rate": 0.0001756702681072429,
      "loss": 0.7273,
      "step": 610
    },
    {
      "epoch": 1.222,
      "grad_norm": 0.97085040807724,
      "learning_rate": 0.00017563025210084034,
      "loss": 0.584,
      "step": 611
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.9569370746612549,
      "learning_rate": 0.0001755902360944378,
      "loss": 0.6037,
      "step": 612
    },
    {
      "epoch": 1.226,
      "grad_norm": 0.9269262552261353,
      "learning_rate": 0.00017555022008803524,
      "loss": 0.4895,
      "step": 613
    },
    {
      "epoch": 1.228,
      "grad_norm": 0.8341909646987915,
      "learning_rate": 0.00017551020408163265,
      "loss": 0.4711,
      "step": 614
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.9149338006973267,
      "learning_rate": 0.0001754701880752301,
      "loss": 0.6082,
      "step": 615
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.8339246511459351,
      "learning_rate": 0.00017543017206882755,
      "loss": 0.8071,
      "step": 616
    },
    {
      "epoch": 1.234,
      "grad_norm": 1.4049110412597656,
      "learning_rate": 0.00017539015606242496,
      "loss": 0.7633,
      "step": 617
    },
    {
      "epoch": 1.236,
      "grad_norm": 1.1161853075027466,
      "learning_rate": 0.00017535014005602242,
      "loss": 1.0144,
      "step": 618
    },
    {
      "epoch": 1.238,
      "grad_norm": 0.822979748249054,
      "learning_rate": 0.00017531012404961986,
      "loss": 0.4552,
      "step": 619
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.81640625,
      "learning_rate": 0.0001752701080432173,
      "loss": 0.5104,
      "step": 620
    },
    {
      "epoch": 1.242,
      "grad_norm": 0.8929588198661804,
      "learning_rate": 0.00017523009203681473,
      "loss": 0.5726,
      "step": 621
    },
    {
      "epoch": 1.244,
      "grad_norm": 0.9330250024795532,
      "learning_rate": 0.00017519007603041217,
      "loss": 0.3567,
      "step": 622
    },
    {
      "epoch": 1.246,
      "grad_norm": 0.78773033618927,
      "learning_rate": 0.0001751500600240096,
      "loss": 0.4512,
      "step": 623
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.1009492874145508,
      "learning_rate": 0.00017511004401760705,
      "loss": 0.626,
      "step": 624
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.8571855425834656,
      "learning_rate": 0.00017507002801120448,
      "loss": 0.6459,
      "step": 625
    },
    {
      "epoch": 1.252,
      "grad_norm": 0.8126084804534912,
      "learning_rate": 0.00017503001200480195,
      "loss": 0.6066,
      "step": 626
    },
    {
      "epoch": 1.254,
      "grad_norm": 0.8052722811698914,
      "learning_rate": 0.00017498999599839936,
      "loss": 0.4254,
      "step": 627
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.7402371168136597,
      "learning_rate": 0.0001749499799919968,
      "loss": 0.4796,
      "step": 628
    },
    {
      "epoch": 1.258,
      "grad_norm": 1.0720347166061401,
      "learning_rate": 0.00017490996398559426,
      "loss": 0.7083,
      "step": 629
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.0231410264968872,
      "learning_rate": 0.0001748699479791917,
      "loss": 0.7794,
      "step": 630
    },
    {
      "epoch": 1.262,
      "grad_norm": 0.8315404653549194,
      "learning_rate": 0.0001748299319727891,
      "loss": 0.4721,
      "step": 631
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.8410038948059082,
      "learning_rate": 0.00017478991596638657,
      "loss": 0.5767,
      "step": 632
    },
    {
      "epoch": 1.266,
      "grad_norm": 1.0305166244506836,
      "learning_rate": 0.000174749899959984,
      "loss": 0.5191,
      "step": 633
    },
    {
      "epoch": 1.268,
      "grad_norm": 0.9320019483566284,
      "learning_rate": 0.00017470988395358144,
      "loss": 0.4962,
      "step": 634
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.90835040807724,
      "learning_rate": 0.00017466986794717888,
      "loss": 0.678,
      "step": 635
    },
    {
      "epoch": 1.272,
      "grad_norm": 1.11126708984375,
      "learning_rate": 0.00017462985194077632,
      "loss": 0.5921,
      "step": 636
    },
    {
      "epoch": 1.274,
      "grad_norm": 1.0690784454345703,
      "learning_rate": 0.00017458983593437376,
      "loss": 0.5824,
      "step": 637
    },
    {
      "epoch": 1.276,
      "grad_norm": 1.172568917274475,
      "learning_rate": 0.0001745498199279712,
      "loss": 0.6192,
      "step": 638
    },
    {
      "epoch": 1.278,
      "grad_norm": 1.0967128276824951,
      "learning_rate": 0.00017450980392156863,
      "loss": 0.5827,
      "step": 639
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.1609292030334473,
      "learning_rate": 0.0001744697879151661,
      "loss": 0.8248,
      "step": 640
    },
    {
      "epoch": 1.282,
      "grad_norm": 1.1156907081604004,
      "learning_rate": 0.0001744297719087635,
      "loss": 0.7031,
      "step": 641
    },
    {
      "epoch": 1.284,
      "grad_norm": 0.961581289768219,
      "learning_rate": 0.00017438975590236094,
      "loss": 0.5118,
      "step": 642
    },
    {
      "epoch": 1.286,
      "grad_norm": 0.9718291759490967,
      "learning_rate": 0.0001743497398959584,
      "loss": 0.7564,
      "step": 643
    },
    {
      "epoch": 1.288,
      "grad_norm": 1.0367878675460815,
      "learning_rate": 0.00017430972388955582,
      "loss": 0.632,
      "step": 644
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.9947198629379272,
      "learning_rate": 0.00017426970788315325,
      "loss": 0.5875,
      "step": 645
    },
    {
      "epoch": 1.292,
      "grad_norm": 0.7783114910125732,
      "learning_rate": 0.00017422969187675072,
      "loss": 0.3552,
      "step": 646
    },
    {
      "epoch": 1.294,
      "grad_norm": 1.1645818948745728,
      "learning_rate": 0.00017418967587034815,
      "loss": 0.4893,
      "step": 647
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.8492037057876587,
      "learning_rate": 0.0001741496598639456,
      "loss": 0.6189,
      "step": 648
    },
    {
      "epoch": 1.298,
      "grad_norm": 0.7862721085548401,
      "learning_rate": 0.00017410964385754303,
      "loss": 0.4464,
      "step": 649
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.7967877984046936,
      "learning_rate": 0.00017406962785114047,
      "loss": 0.4086,
      "step": 650
    },
    {
      "epoch": 1.302,
      "grad_norm": 0.9620580077171326,
      "learning_rate": 0.0001740296118447379,
      "loss": 0.5445,
      "step": 651
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.9668026566505432,
      "learning_rate": 0.00017398959583833534,
      "loss": 0.4992,
      "step": 652
    },
    {
      "epoch": 1.306,
      "grad_norm": 0.8129428029060364,
      "learning_rate": 0.00017394957983193278,
      "loss": 0.5318,
      "step": 653
    },
    {
      "epoch": 1.308,
      "grad_norm": 0.8930736184120178,
      "learning_rate": 0.00017390956382553024,
      "loss": 0.6171,
      "step": 654
    },
    {
      "epoch": 1.31,
      "grad_norm": 1.0192950963974,
      "learning_rate": 0.00017386954781912765,
      "loss": 0.7218,
      "step": 655
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.1515235900878906,
      "learning_rate": 0.0001738295318127251,
      "loss": 0.4828,
      "step": 656
    },
    {
      "epoch": 1.314,
      "grad_norm": 0.9468941688537598,
      "learning_rate": 0.00017378951580632255,
      "loss": 0.5379,
      "step": 657
    },
    {
      "epoch": 1.316,
      "grad_norm": 1.0648995637893677,
      "learning_rate": 0.00017374949979991996,
      "loss": 0.6114,
      "step": 658
    },
    {
      "epoch": 1.318,
      "grad_norm": 0.9308446049690247,
      "learning_rate": 0.0001737094837935174,
      "loss": 0.5429,
      "step": 659
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.1849421262741089,
      "learning_rate": 0.00017366946778711486,
      "loss": 0.7408,
      "step": 660
    },
    {
      "epoch": 1.322,
      "grad_norm": 1.01514732837677,
      "learning_rate": 0.00017362945178071227,
      "loss": 0.7736,
      "step": 661
    },
    {
      "epoch": 1.324,
      "grad_norm": 1.203231930732727,
      "learning_rate": 0.00017358943577430974,
      "loss": 0.5937,
      "step": 662
    },
    {
      "epoch": 1.326,
      "grad_norm": 1.013895034790039,
      "learning_rate": 0.00017354941976790718,
      "loss": 0.4935,
      "step": 663
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.9344672560691833,
      "learning_rate": 0.0001735094037615046,
      "loss": 0.4333,
      "step": 664
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.9790710806846619,
      "learning_rate": 0.00017346938775510205,
      "loss": 0.8895,
      "step": 665
    },
    {
      "epoch": 1.332,
      "grad_norm": 0.975405216217041,
      "learning_rate": 0.0001734293717486995,
      "loss": 0.5357,
      "step": 666
    },
    {
      "epoch": 1.334,
      "grad_norm": 0.7841880917549133,
      "learning_rate": 0.00017338935574229692,
      "loss": 0.4291,
      "step": 667
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.9604461789131165,
      "learning_rate": 0.00017334933973589436,
      "loss": 0.5744,
      "step": 668
    },
    {
      "epoch": 1.338,
      "grad_norm": 1.052727222442627,
      "learning_rate": 0.0001733093237294918,
      "loss": 0.5732,
      "step": 669
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.012575626373291,
      "learning_rate": 0.00017326930772308924,
      "loss": 0.6665,
      "step": 670
    },
    {
      "epoch": 1.342,
      "grad_norm": 0.939420223236084,
      "learning_rate": 0.0001732292917166867,
      "loss": 0.4232,
      "step": 671
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.7950401306152344,
      "learning_rate": 0.0001731892757102841,
      "loss": 0.4801,
      "step": 672
    },
    {
      "epoch": 1.346,
      "grad_norm": 0.7899267673492432,
      "learning_rate": 0.00017314925970388157,
      "loss": 0.5119,
      "step": 673
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 0.8789637684822083,
      "learning_rate": 0.000173109243697479,
      "loss": 0.5576,
      "step": 674
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.0767825841903687,
      "learning_rate": 0.00017306922769107642,
      "loss": 0.6698,
      "step": 675
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.9097235798835754,
      "learning_rate": 0.00017302921168467389,
      "loss": 0.728,
      "step": 676
    },
    {
      "epoch": 1.354,
      "grad_norm": 0.818047285079956,
      "learning_rate": 0.00017298919567827132,
      "loss": 0.6034,
      "step": 677
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 0.8606382012367249,
      "learning_rate": 0.00017294917967186876,
      "loss": 0.5351,
      "step": 678
    },
    {
      "epoch": 1.358,
      "grad_norm": 1.098699688911438,
      "learning_rate": 0.0001729091636654662,
      "loss": 0.5223,
      "step": 679
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.8741934895515442,
      "learning_rate": 0.00017286914765906363,
      "loss": 0.5877,
      "step": 680
    },
    {
      "epoch": 1.362,
      "grad_norm": 1.0313067436218262,
      "learning_rate": 0.00017282913165266107,
      "loss": 0.6163,
      "step": 681
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 0.9448000192642212,
      "learning_rate": 0.0001727891156462585,
      "loss": 0.4908,
      "step": 682
    },
    {
      "epoch": 1.366,
      "grad_norm": 0.912571907043457,
      "learning_rate": 0.00017274909963985595,
      "loss": 0.4336,
      "step": 683
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 1.066299557685852,
      "learning_rate": 0.00017270908363345338,
      "loss": 0.6327,
      "step": 684
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.9169372320175171,
      "learning_rate": 0.00017266906762705082,
      "loss": 0.7118,
      "step": 685
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 1.2845978736877441,
      "learning_rate": 0.00017262905162064826,
      "loss": 0.8913,
      "step": 686
    },
    {
      "epoch": 1.374,
      "grad_norm": 1.1892740726470947,
      "learning_rate": 0.00017258903561424572,
      "loss": 0.5793,
      "step": 687
    },
    {
      "epoch": 1.376,
      "grad_norm": 1.0534002780914307,
      "learning_rate": 0.00017254901960784316,
      "loss": 0.6056,
      "step": 688
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 1.1333892345428467,
      "learning_rate": 0.00017250900360144057,
      "loss": 0.6342,
      "step": 689
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.9473435282707214,
      "learning_rate": 0.00017246898759503803,
      "loss": 0.5987,
      "step": 690
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 1.1002849340438843,
      "learning_rate": 0.00017242897158863547,
      "loss": 0.8034,
      "step": 691
    },
    {
      "epoch": 1.384,
      "grad_norm": 1.0090365409851074,
      "learning_rate": 0.00017238895558223288,
      "loss": 0.6243,
      "step": 692
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 1.1537292003631592,
      "learning_rate": 0.00017234893957583034,
      "loss": 0.584,
      "step": 693
    },
    {
      "epoch": 1.388,
      "grad_norm": 1.1367181539535522,
      "learning_rate": 0.00017230892356942778,
      "loss": 0.7459,
      "step": 694
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 1.031333565711975,
      "learning_rate": 0.00017226890756302522,
      "loss": 0.6461,
      "step": 695
    },
    {
      "epoch": 1.392,
      "grad_norm": 1.2773493528366089,
      "learning_rate": 0.00017222889155662266,
      "loss": 0.7189,
      "step": 696
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 1.0892518758773804,
      "learning_rate": 0.0001721888755502201,
      "loss": 0.6645,
      "step": 697
    },
    {
      "epoch": 1.396,
      "grad_norm": 0.9252284169197083,
      "learning_rate": 0.00017214885954381753,
      "loss": 0.4434,
      "step": 698
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 0.8895227313041687,
      "learning_rate": 0.00017210884353741497,
      "loss": 0.4127,
      "step": 699
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.8482716083526611,
      "learning_rate": 0.0001720688275310124,
      "loss": 0.5035,
      "step": 700
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 0.964252233505249,
      "learning_rate": 0.00017202881152460987,
      "loss": 0.6649,
      "step": 701
    },
    {
      "epoch": 1.404,
      "grad_norm": 0.9234007596969604,
      "learning_rate": 0.0001719887955182073,
      "loss": 0.5859,
      "step": 702
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 1.0770117044448853,
      "learning_rate": 0.00017194877951180472,
      "loss": 0.7469,
      "step": 703
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.8556125164031982,
      "learning_rate": 0.00017190876350540218,
      "loss": 0.4881,
      "step": 704
    },
    {
      "epoch": 1.41,
      "grad_norm": 1.0026272535324097,
      "learning_rate": 0.00017186874749899962,
      "loss": 0.6378,
      "step": 705
    },
    {
      "epoch": 1.412,
      "grad_norm": 1.0607497692108154,
      "learning_rate": 0.00017182873149259703,
      "loss": 0.5909,
      "step": 706
    },
    {
      "epoch": 1.414,
      "grad_norm": 0.8867909908294678,
      "learning_rate": 0.0001717887154861945,
      "loss": 0.5988,
      "step": 707
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.864119291305542,
      "learning_rate": 0.00017174869947979193,
      "loss": 0.5439,
      "step": 708
    },
    {
      "epoch": 1.418,
      "grad_norm": 1.1083439588546753,
      "learning_rate": 0.00017170868347338937,
      "loss": 0.6524,
      "step": 709
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.8649158477783203,
      "learning_rate": 0.0001716686674669868,
      "loss": 0.4316,
      "step": 710
    },
    {
      "epoch": 1.422,
      "grad_norm": 1.2481869459152222,
      "learning_rate": 0.00017162865146058424,
      "loss": 0.6567,
      "step": 711
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.1257922649383545,
      "learning_rate": 0.00017158863545418168,
      "loss": 0.459,
      "step": 712
    },
    {
      "epoch": 1.426,
      "grad_norm": 1.0287104845046997,
      "learning_rate": 0.00017154861944777911,
      "loss": 0.9514,
      "step": 713
    },
    {
      "epoch": 1.428,
      "grad_norm": 1.243639349937439,
      "learning_rate": 0.00017150860344137655,
      "loss": 0.5485,
      "step": 714
    },
    {
      "epoch": 1.43,
      "grad_norm": 1.0024007558822632,
      "learning_rate": 0.00017146858743497402,
      "loss": 0.7507,
      "step": 715
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.9301564693450928,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.5174,
      "step": 716
    },
    {
      "epoch": 1.434,
      "grad_norm": 1.0753121376037598,
      "learning_rate": 0.00017138855542216886,
      "loss": 0.6672,
      "step": 717
    },
    {
      "epoch": 1.436,
      "grad_norm": 0.9878348112106323,
      "learning_rate": 0.00017134853941576633,
      "loss": 0.5071,
      "step": 718
    },
    {
      "epoch": 1.438,
      "grad_norm": 1.5172585248947144,
      "learning_rate": 0.00017130852340936376,
      "loss": 1.2483,
      "step": 719
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.2049373388290405,
      "learning_rate": 0.00017126850740296117,
      "loss": 0.5104,
      "step": 720
    },
    {
      "epoch": 1.442,
      "grad_norm": 1.0199648141860962,
      "learning_rate": 0.00017122849139655864,
      "loss": 0.4888,
      "step": 721
    },
    {
      "epoch": 1.444,
      "grad_norm": 0.952781617641449,
      "learning_rate": 0.00017118847539015608,
      "loss": 0.4951,
      "step": 722
    },
    {
      "epoch": 1.446,
      "grad_norm": 0.8831384778022766,
      "learning_rate": 0.0001711484593837535,
      "loss": 0.4412,
      "step": 723
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.8745229840278625,
      "learning_rate": 0.00017110844337735095,
      "loss": 0.6549,
      "step": 724
    },
    {
      "epoch": 1.45,
      "grad_norm": 1.1048731803894043,
      "learning_rate": 0.0001710684273709484,
      "loss": 0.6765,
      "step": 725
    },
    {
      "epoch": 1.452,
      "grad_norm": 0.8865031003952026,
      "learning_rate": 0.00017102841136454585,
      "loss": 0.6619,
      "step": 726
    },
    {
      "epoch": 1.454,
      "grad_norm": 1.094138741493225,
      "learning_rate": 0.00017098839535814326,
      "loss": 0.6596,
      "step": 727
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.9874744415283203,
      "learning_rate": 0.0001709483793517407,
      "loss": 0.5515,
      "step": 728
    },
    {
      "epoch": 1.458,
      "grad_norm": 0.9475225806236267,
      "learning_rate": 0.00017090836334533816,
      "loss": 0.5117,
      "step": 729
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.8016526103019714,
      "learning_rate": 0.00017086834733893557,
      "loss": 0.6683,
      "step": 730
    },
    {
      "epoch": 1.462,
      "grad_norm": 0.8900169730186462,
      "learning_rate": 0.000170828331332533,
      "loss": 0.6805,
      "step": 731
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.9949558973312378,
      "learning_rate": 0.00017078831532613047,
      "loss": 0.6172,
      "step": 732
    },
    {
      "epoch": 1.466,
      "grad_norm": 0.7819185256958008,
      "learning_rate": 0.00017074829931972788,
      "loss": 0.6688,
      "step": 733
    },
    {
      "epoch": 1.468,
      "grad_norm": 0.9040623307228088,
      "learning_rate": 0.00017070828331332535,
      "loss": 0.622,
      "step": 734
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.9458033442497253,
      "learning_rate": 0.00017066826730692278,
      "loss": 0.5308,
      "step": 735
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.1083749532699585,
      "learning_rate": 0.00017062825130052022,
      "loss": 0.8128,
      "step": 736
    },
    {
      "epoch": 1.474,
      "grad_norm": 1.194437861442566,
      "learning_rate": 0.00017058823529411766,
      "loss": 0.6546,
      "step": 737
    },
    {
      "epoch": 1.476,
      "grad_norm": 1.2156788110733032,
      "learning_rate": 0.0001705482192877151,
      "loss": 0.5593,
      "step": 738
    },
    {
      "epoch": 1.478,
      "grad_norm": 0.8321724534034729,
      "learning_rate": 0.00017050820328131253,
      "loss": 0.4824,
      "step": 739
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.8708323836326599,
      "learning_rate": 0.00017046818727490997,
      "loss": 0.4673,
      "step": 740
    },
    {
      "epoch": 1.482,
      "grad_norm": 0.9518771767616272,
      "learning_rate": 0.0001704281712685074,
      "loss": 0.5722,
      "step": 741
    },
    {
      "epoch": 1.484,
      "grad_norm": 1.0618067979812622,
      "learning_rate": 0.00017038815526210484,
      "loss": 0.6348,
      "step": 742
    },
    {
      "epoch": 1.486,
      "grad_norm": 1.1785826683044434,
      "learning_rate": 0.0001703481392557023,
      "loss": 0.7547,
      "step": 743
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.1366275548934937,
      "learning_rate": 0.00017030812324929972,
      "loss": 0.5531,
      "step": 744
    },
    {
      "epoch": 1.49,
      "grad_norm": 1.121962547302246,
      "learning_rate": 0.00017026810724289716,
      "loss": 0.4704,
      "step": 745
    },
    {
      "epoch": 1.492,
      "grad_norm": 1.2830772399902344,
      "learning_rate": 0.00017022809123649462,
      "loss": 0.8078,
      "step": 746
    },
    {
      "epoch": 1.494,
      "grad_norm": 1.0392471551895142,
      "learning_rate": 0.00017018807523009203,
      "loss": 0.7757,
      "step": 747
    },
    {
      "epoch": 1.496,
      "grad_norm": 1.079883337020874,
      "learning_rate": 0.0001701480592236895,
      "loss": 0.5347,
      "step": 748
    },
    {
      "epoch": 1.498,
      "grad_norm": 0.9714980125427246,
      "learning_rate": 0.00017010804321728693,
      "loss": 0.3666,
      "step": 749
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.04716956615448,
      "learning_rate": 0.00017006802721088434,
      "loss": 0.486,
      "step": 750
    },
    {
      "epoch": 1.502,
      "grad_norm": 0.8447955250740051,
      "learning_rate": 0.0001700280112044818,
      "loss": 0.5949,
      "step": 751
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.802815318107605,
      "learning_rate": 0.00016998799519807924,
      "loss": 0.5714,
      "step": 752
    },
    {
      "epoch": 1.506,
      "grad_norm": 0.9239459037780762,
      "learning_rate": 0.00016994797919167668,
      "loss": 0.5587,
      "step": 753
    },
    {
      "epoch": 1.508,
      "grad_norm": 0.9766963124275208,
      "learning_rate": 0.00016990796318527412,
      "loss": 0.7562,
      "step": 754
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.9949856400489807,
      "learning_rate": 0.00016986794717887155,
      "loss": 0.7731,
      "step": 755
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.2131258249282837,
      "learning_rate": 0.000169827931172469,
      "loss": 0.56,
      "step": 756
    },
    {
      "epoch": 1.514,
      "grad_norm": 0.9644022583961487,
      "learning_rate": 0.00016978791516606643,
      "loss": 0.7263,
      "step": 757
    },
    {
      "epoch": 1.516,
      "grad_norm": 1.20486581325531,
      "learning_rate": 0.00016974789915966387,
      "loss": 0.7265,
      "step": 758
    },
    {
      "epoch": 1.518,
      "grad_norm": 0.832122802734375,
      "learning_rate": 0.0001697078831532613,
      "loss": 0.4216,
      "step": 759
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.0056893825531006,
      "learning_rate": 0.00016966786714685877,
      "loss": 0.6248,
      "step": 760
    },
    {
      "epoch": 1.522,
      "grad_norm": 0.7547525763511658,
      "learning_rate": 0.00016962785114045618,
      "loss": 0.5273,
      "step": 761
    },
    {
      "epoch": 1.524,
      "grad_norm": 0.9533004760742188,
      "learning_rate": 0.00016958783513405364,
      "loss": 0.6252,
      "step": 762
    },
    {
      "epoch": 1.526,
      "grad_norm": 0.9379053115844727,
      "learning_rate": 0.00016954781912765108,
      "loss": 0.6156,
      "step": 763
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.8372834920883179,
      "learning_rate": 0.0001695078031212485,
      "loss": 0.479,
      "step": 764
    },
    {
      "epoch": 1.53,
      "grad_norm": 1.046715259552002,
      "learning_rate": 0.00016946778711484595,
      "loss": 0.6155,
      "step": 765
    },
    {
      "epoch": 1.532,
      "grad_norm": 0.9240854978561401,
      "learning_rate": 0.0001694277711084434,
      "loss": 0.4994,
      "step": 766
    },
    {
      "epoch": 1.534,
      "grad_norm": 1.1138590574264526,
      "learning_rate": 0.00016938775510204083,
      "loss": 0.5661,
      "step": 767
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.9195464253425598,
      "learning_rate": 0.00016934773909563826,
      "loss": 0.7321,
      "step": 768
    },
    {
      "epoch": 1.538,
      "grad_norm": 1.063637614250183,
      "learning_rate": 0.0001693077230892357,
      "loss": 0.801,
      "step": 769
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.8537842631340027,
      "learning_rate": 0.00016926770708283314,
      "loss": 0.5213,
      "step": 770
    },
    {
      "epoch": 1.542,
      "grad_norm": 0.9764654636383057,
      "learning_rate": 0.00016922769107643058,
      "loss": 0.5117,
      "step": 771
    },
    {
      "epoch": 1.544,
      "grad_norm": 1.742078185081482,
      "learning_rate": 0.000169187675070028,
      "loss": 0.7841,
      "step": 772
    },
    {
      "epoch": 1.546,
      "grad_norm": 0.8714882135391235,
      "learning_rate": 0.00016914765906362545,
      "loss": 0.5023,
      "step": 773
    },
    {
      "epoch": 1.548,
      "grad_norm": 1.4232264757156372,
      "learning_rate": 0.0001691076430572229,
      "loss": 0.5823,
      "step": 774
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.0767849683761597,
      "learning_rate": 0.00016906762705082032,
      "loss": 0.5579,
      "step": 775
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.9927548170089722,
      "learning_rate": 0.0001690276110444178,
      "loss": 0.6583,
      "step": 776
    },
    {
      "epoch": 1.554,
      "grad_norm": 1.0678993463516235,
      "learning_rate": 0.00016898759503801523,
      "loss": 0.5829,
      "step": 777
    },
    {
      "epoch": 1.556,
      "grad_norm": 0.8055137395858765,
      "learning_rate": 0.00016894757903161264,
      "loss": 0.5588,
      "step": 778
    },
    {
      "epoch": 1.558,
      "grad_norm": 1.2230702638626099,
      "learning_rate": 0.0001689075630252101,
      "loss": 0.7553,
      "step": 779
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.8429524898529053,
      "learning_rate": 0.00016886754701880754,
      "loss": 0.6407,
      "step": 780
    },
    {
      "epoch": 1.562,
      "grad_norm": 0.8419714570045471,
      "learning_rate": 0.00016882753101240495,
      "loss": 0.5766,
      "step": 781
    },
    {
      "epoch": 1.564,
      "grad_norm": 1.253245234489441,
      "learning_rate": 0.0001687875150060024,
      "loss": 0.6653,
      "step": 782
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 0.8841511011123657,
      "learning_rate": 0.00016874749899959985,
      "loss": 0.5025,
      "step": 783
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.927223801612854,
      "learning_rate": 0.00016870748299319729,
      "loss": 0.5684,
      "step": 784
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 0.8433817625045776,
      "learning_rate": 0.00016866746698679472,
      "loss": 0.5249,
      "step": 785
    },
    {
      "epoch": 1.572,
      "grad_norm": 1.3443412780761719,
      "learning_rate": 0.00016862745098039216,
      "loss": 0.6044,
      "step": 786
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 0.7818428874015808,
      "learning_rate": 0.0001685874349739896,
      "loss": 0.5952,
      "step": 787
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.1141996383666992,
      "learning_rate": 0.00016854741896758703,
      "loss": 0.6378,
      "step": 788
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 1.0062204599380493,
      "learning_rate": 0.00016850740296118447,
      "loss": 0.7134,
      "step": 789
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.8658462762832642,
      "learning_rate": 0.00016846738695478194,
      "loss": 0.4667,
      "step": 790
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 0.9645835161209106,
      "learning_rate": 0.00016842737094837937,
      "loss": 0.6646,
      "step": 791
    },
    {
      "epoch": 1.584,
      "grad_norm": 1.1966413259506226,
      "learning_rate": 0.00016838735494197678,
      "loss": 0.4824,
      "step": 792
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 0.9480036497116089,
      "learning_rate": 0.00016834733893557425,
      "loss": 0.4968,
      "step": 793
    },
    {
      "epoch": 1.588,
      "grad_norm": 0.9499500393867493,
      "learning_rate": 0.00016830732292917168,
      "loss": 0.6219,
      "step": 794
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 0.817382276058197,
      "learning_rate": 0.0001682673069227691,
      "loss": 0.4991,
      "step": 795
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.8350610733032227,
      "learning_rate": 0.00016822729091636656,
      "loss": 0.5031,
      "step": 796
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 1.3546720743179321,
      "learning_rate": 0.000168187274909964,
      "loss": 0.5529,
      "step": 797
    },
    {
      "epoch": 1.596,
      "grad_norm": 1.0127795934677124,
      "learning_rate": 0.00016814725890356143,
      "loss": 0.5679,
      "step": 798
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 0.9606553316116333,
      "learning_rate": 0.00016810724289715887,
      "loss": 0.4723,
      "step": 799
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.2879102230072021,
      "learning_rate": 0.0001680672268907563,
      "loss": 0.8782,
      "step": 800
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 0.953312873840332,
      "learning_rate": 0.00016802721088435377,
      "loss": 0.6993,
      "step": 801
    },
    {
      "epoch": 1.604,
      "grad_norm": 0.8325719833374023,
      "learning_rate": 0.00016798719487795118,
      "loss": 0.544,
      "step": 802
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 1.044710397720337,
      "learning_rate": 0.00016794717887154862,
      "loss": 0.44,
      "step": 803
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.3410003185272217,
      "learning_rate": 0.00016790716286514608,
      "loss": 0.6431,
      "step": 804
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 0.8255342841148376,
      "learning_rate": 0.0001678671468587435,
      "loss": 0.5003,
      "step": 805
    },
    {
      "epoch": 1.612,
      "grad_norm": 0.91832035779953,
      "learning_rate": 0.00016782713085234093,
      "loss": 0.5178,
      "step": 806
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 0.7916572690010071,
      "learning_rate": 0.0001677871148459384,
      "loss": 0.5312,
      "step": 807
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.9585390090942383,
      "learning_rate": 0.00016774709883953583,
      "loss": 0.7851,
      "step": 808
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 0.9998632669448853,
      "learning_rate": 0.00016770708283313327,
      "loss": 0.5112,
      "step": 809
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.0752184391021729,
      "learning_rate": 0.0001676670668267307,
      "loss": 0.6541,
      "step": 810
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 1.0508180856704712,
      "learning_rate": 0.00016762705082032814,
      "loss": 0.6857,
      "step": 811
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.06639564037323,
      "learning_rate": 0.00016758703481392558,
      "loss": 0.4638,
      "step": 812
    },
    {
      "epoch": 1.626,
      "grad_norm": 0.8185691833496094,
      "learning_rate": 0.00016754701880752302,
      "loss": 0.5211,
      "step": 813
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 1.0776699781417847,
      "learning_rate": 0.00016750700280112045,
      "loss": 0.4824,
      "step": 814
    },
    {
      "epoch": 1.63,
      "grad_norm": 1.0461986064910889,
      "learning_rate": 0.00016746698679471792,
      "loss": 0.555,
      "step": 815
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.8275187015533447,
      "learning_rate": 0.00016742697078831533,
      "loss": 0.4326,
      "step": 816
    },
    {
      "epoch": 1.634,
      "grad_norm": 0.8513644337654114,
      "learning_rate": 0.00016738695478191277,
      "loss": 0.4922,
      "step": 817
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 0.9129961729049683,
      "learning_rate": 0.00016734693877551023,
      "loss": 0.5013,
      "step": 818
    },
    {
      "epoch": 1.638,
      "grad_norm": 0.8391352295875549,
      "learning_rate": 0.00016730692276910764,
      "loss": 0.531,
      "step": 819
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.7599107623100281,
      "learning_rate": 0.00016726690676270508,
      "loss": 0.463,
      "step": 820
    },
    {
      "epoch": 1.642,
      "grad_norm": 0.8259713053703308,
      "learning_rate": 0.00016722689075630254,
      "loss": 0.5171,
      "step": 821
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 0.8449570536613464,
      "learning_rate": 0.00016718687474989995,
      "loss": 0.7491,
      "step": 822
    },
    {
      "epoch": 1.646,
      "grad_norm": 0.9813569188117981,
      "learning_rate": 0.00016714685874349742,
      "loss": 0.6257,
      "step": 823
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1.1608649492263794,
      "learning_rate": 0.00016710684273709485,
      "loss": 0.6613,
      "step": 824
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.8957746624946594,
      "learning_rate": 0.0001670668267306923,
      "loss": 0.5695,
      "step": 825
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 1.7580598592758179,
      "learning_rate": 0.00016702681072428973,
      "loss": 0.5255,
      "step": 826
    },
    {
      "epoch": 1.654,
      "grad_norm": 1.054764747619629,
      "learning_rate": 0.00016698679471788716,
      "loss": 0.699,
      "step": 827
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 1.218562126159668,
      "learning_rate": 0.0001669467787114846,
      "loss": 0.7255,
      "step": 828
    },
    {
      "epoch": 1.658,
      "grad_norm": 1.1451988220214844,
      "learning_rate": 0.00016690676270508204,
      "loss": 0.4975,
      "step": 829
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.8028342127799988,
      "learning_rate": 0.00016686674669867948,
      "loss": 0.4977,
      "step": 830
    },
    {
      "epoch": 1.662,
      "grad_norm": 0.8464062213897705,
      "learning_rate": 0.0001668267306922769,
      "loss": 0.617,
      "step": 831
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 1.2107888460159302,
      "learning_rate": 0.00016678671468587438,
      "loss": 0.8693,
      "step": 832
    },
    {
      "epoch": 1.666,
      "grad_norm": 0.9205864071846008,
      "learning_rate": 0.0001667466986794718,
      "loss": 0.496,
      "step": 833
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 1.089193344116211,
      "learning_rate": 0.00016670668267306922,
      "loss": 0.6106,
      "step": 834
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.8671337962150574,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.4298,
      "step": 835
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.8851087093353271,
      "learning_rate": 0.0001666266506602641,
      "loss": 0.5017,
      "step": 836
    },
    {
      "epoch": 1.674,
      "grad_norm": 0.9373218417167664,
      "learning_rate": 0.00016658663465386156,
      "loss": 0.746,
      "step": 837
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 1.0108126401901245,
      "learning_rate": 0.000166546618647459,
      "loss": 0.4704,
      "step": 838
    },
    {
      "epoch": 1.678,
      "grad_norm": 1.1136822700500488,
      "learning_rate": 0.0001665066026410564,
      "loss": 0.6258,
      "step": 839
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.9297665357589722,
      "learning_rate": 0.00016646658663465387,
      "loss": 0.5188,
      "step": 840
    },
    {
      "epoch": 1.682,
      "grad_norm": 1.048720121383667,
      "learning_rate": 0.0001664265706282513,
      "loss": 0.6178,
      "step": 841
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 1.1828432083129883,
      "learning_rate": 0.00016638655462184875,
      "loss": 0.5673,
      "step": 842
    },
    {
      "epoch": 1.686,
      "grad_norm": 0.948161244392395,
      "learning_rate": 0.00016634653861544619,
      "loss": 0.5251,
      "step": 843
    },
    {
      "epoch": 1.688,
      "grad_norm": 1.0712306499481201,
      "learning_rate": 0.00016630652260904362,
      "loss": 0.7541,
      "step": 844
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.8264464735984802,
      "learning_rate": 0.00016626650660264106,
      "loss": 0.5101,
      "step": 845
    },
    {
      "epoch": 1.692,
      "grad_norm": 1.0269306898117065,
      "learning_rate": 0.0001662264905962385,
      "loss": 0.518,
      "step": 846
    },
    {
      "epoch": 1.694,
      "grad_norm": 1.043114423751831,
      "learning_rate": 0.00016618647458983593,
      "loss": 0.6872,
      "step": 847
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.0633100271224976,
      "learning_rate": 0.00016614645858343337,
      "loss": 0.5803,
      "step": 848
    },
    {
      "epoch": 1.698,
      "grad_norm": 1.2895865440368652,
      "learning_rate": 0.00016610644257703084,
      "loss": 0.5394,
      "step": 849
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.181101679801941,
      "learning_rate": 0.00016606642657062825,
      "loss": 0.5429,
      "step": 850
    },
    {
      "epoch": 1.702,
      "grad_norm": 0.9739941954612732,
      "learning_rate": 0.0001660264105642257,
      "loss": 0.5202,
      "step": 851
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.8713230490684509,
      "learning_rate": 0.00016598639455782315,
      "loss": 0.7101,
      "step": 852
    },
    {
      "epoch": 1.706,
      "grad_norm": 1.1222187280654907,
      "learning_rate": 0.00016594637855142056,
      "loss": 0.5729,
      "step": 853
    },
    {
      "epoch": 1.708,
      "grad_norm": 1.1009290218353271,
      "learning_rate": 0.00016590636254501802,
      "loss": 0.6313,
      "step": 854
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.8840258717536926,
      "learning_rate": 0.00016586634653861546,
      "loss": 0.5667,
      "step": 855
    },
    {
      "epoch": 1.712,
      "grad_norm": 1.3443195819854736,
      "learning_rate": 0.0001658263305322129,
      "loss": 0.5805,
      "step": 856
    },
    {
      "epoch": 1.714,
      "grad_norm": 0.9016503095626831,
      "learning_rate": 0.00016578631452581033,
      "loss": 0.5102,
      "step": 857
    },
    {
      "epoch": 1.716,
      "grad_norm": 1.0419456958770752,
      "learning_rate": 0.00016574629851940777,
      "loss": 0.6905,
      "step": 858
    },
    {
      "epoch": 1.718,
      "grad_norm": 0.7870181798934937,
      "learning_rate": 0.0001657062825130052,
      "loss": 0.4023,
      "step": 859
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.3794949054718018,
      "learning_rate": 0.00016566626650660264,
      "loss": 0.6141,
      "step": 860
    },
    {
      "epoch": 1.722,
      "grad_norm": 1.102526068687439,
      "learning_rate": 0.00016562625050020008,
      "loss": 0.5999,
      "step": 861
    },
    {
      "epoch": 1.724,
      "grad_norm": 0.9104664921760559,
      "learning_rate": 0.00016558623449379755,
      "loss": 0.5261,
      "step": 862
    },
    {
      "epoch": 1.726,
      "grad_norm": 0.7952398657798767,
      "learning_rate": 0.00016554621848739496,
      "loss": 0.4939,
      "step": 863
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.8430768251419067,
      "learning_rate": 0.0001655062024809924,
      "loss": 0.5585,
      "step": 864
    },
    {
      "epoch": 1.73,
      "grad_norm": 1.2330178022384644,
      "learning_rate": 0.00016546618647458986,
      "loss": 0.7566,
      "step": 865
    },
    {
      "epoch": 1.732,
      "grad_norm": 1.1982470750808716,
      "learning_rate": 0.0001654261704681873,
      "loss": 0.572,
      "step": 866
    },
    {
      "epoch": 1.734,
      "grad_norm": 0.9084339737892151,
      "learning_rate": 0.0001653861544617847,
      "loss": 0.6219,
      "step": 867
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.0415462255477905,
      "learning_rate": 0.00016534613845538217,
      "loss": 0.6515,
      "step": 868
    },
    {
      "epoch": 1.738,
      "grad_norm": 0.9985767602920532,
      "learning_rate": 0.0001653061224489796,
      "loss": 0.8079,
      "step": 869
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.8394185304641724,
      "learning_rate": 0.00016526610644257704,
      "loss": 0.4555,
      "step": 870
    },
    {
      "epoch": 1.742,
      "grad_norm": 0.8568782806396484,
      "learning_rate": 0.00016522609043617448,
      "loss": 0.3874,
      "step": 871
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.9686130881309509,
      "learning_rate": 0.00016518607442977192,
      "loss": 0.5844,
      "step": 872
    },
    {
      "epoch": 1.746,
      "grad_norm": 0.9807780385017395,
      "learning_rate": 0.00016514605842336935,
      "loss": 0.4754,
      "step": 873
    },
    {
      "epoch": 1.748,
      "grad_norm": 1.0071227550506592,
      "learning_rate": 0.0001651060424169668,
      "loss": 0.6328,
      "step": 874
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.432931900024414,
      "learning_rate": 0.00016506602641056423,
      "loss": 0.6986,
      "step": 875
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.9963555335998535,
      "learning_rate": 0.0001650260104041617,
      "loss": 0.5965,
      "step": 876
    },
    {
      "epoch": 1.754,
      "grad_norm": 0.8445236086845398,
      "learning_rate": 0.0001649859943977591,
      "loss": 0.4694,
      "step": 877
    },
    {
      "epoch": 1.756,
      "grad_norm": 0.8357356190681458,
      "learning_rate": 0.00016494597839135654,
      "loss": 0.5566,
      "step": 878
    },
    {
      "epoch": 1.758,
      "grad_norm": 1.229999303817749,
      "learning_rate": 0.000164905962384954,
      "loss": 0.6486,
      "step": 879
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.3772428035736084,
      "learning_rate": 0.00016486594637855144,
      "loss": 0.5491,
      "step": 880
    },
    {
      "epoch": 1.762,
      "grad_norm": 1.0235356092453003,
      "learning_rate": 0.00016482593037214885,
      "loss": 0.4928,
      "step": 881
    },
    {
      "epoch": 1.764,
      "grad_norm": 0.8714808225631714,
      "learning_rate": 0.00016478591436574631,
      "loss": 0.5549,
      "step": 882
    },
    {
      "epoch": 1.766,
      "grad_norm": 0.9936070442199707,
      "learning_rate": 0.00016474589835934375,
      "loss": 0.6644,
      "step": 883
    },
    {
      "epoch": 1.768,
      "grad_norm": 1.1924384832382202,
      "learning_rate": 0.0001647058823529412,
      "loss": 0.7462,
      "step": 884
    },
    {
      "epoch": 1.77,
      "grad_norm": 1.0181946754455566,
      "learning_rate": 0.00016466586634653863,
      "loss": 0.5125,
      "step": 885
    },
    {
      "epoch": 1.772,
      "grad_norm": 1.2749956846237183,
      "learning_rate": 0.00016462585034013606,
      "loss": 0.6952,
      "step": 886
    },
    {
      "epoch": 1.774,
      "grad_norm": 1.0935745239257812,
      "learning_rate": 0.0001645858343337335,
      "loss": 1.0121,
      "step": 887
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.9324166178703308,
      "learning_rate": 0.00016454581832733094,
      "loss": 0.5914,
      "step": 888
    },
    {
      "epoch": 1.778,
      "grad_norm": 0.976314127445221,
      "learning_rate": 0.00016450580232092837,
      "loss": 0.5773,
      "step": 889
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.8418412804603577,
      "learning_rate": 0.00016446578631452584,
      "loss": 0.4104,
      "step": 890
    },
    {
      "epoch": 1.782,
      "grad_norm": 1.2344383001327515,
      "learning_rate": 0.00016442577030812325,
      "loss": 0.4498,
      "step": 891
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.0658375024795532,
      "learning_rate": 0.00016438575430172069,
      "loss": 0.6236,
      "step": 892
    },
    {
      "epoch": 1.786,
      "grad_norm": 0.9540947675704956,
      "learning_rate": 0.00016434573829531815,
      "loss": 0.5746,
      "step": 893
    },
    {
      "epoch": 1.788,
      "grad_norm": 1.2821284532546997,
      "learning_rate": 0.00016430572228891556,
      "loss": 0.9512,
      "step": 894
    },
    {
      "epoch": 1.79,
      "grad_norm": 1.1900142431259155,
      "learning_rate": 0.000164265706282513,
      "loss": 0.7853,
      "step": 895
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.1140549182891846,
      "learning_rate": 0.00016422569027611046,
      "loss": 0.605,
      "step": 896
    },
    {
      "epoch": 1.794,
      "grad_norm": 0.8187505602836609,
      "learning_rate": 0.0001641856742697079,
      "loss": 0.5835,
      "step": 897
    },
    {
      "epoch": 1.796,
      "grad_norm": 1.0024667978286743,
      "learning_rate": 0.00016414565826330534,
      "loss": 0.5746,
      "step": 898
    },
    {
      "epoch": 1.798,
      "grad_norm": 0.9348322749137878,
      "learning_rate": 0.00016410564225690277,
      "loss": 0.601,
      "step": 899
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.9487556219100952,
      "learning_rate": 0.0001640656262505002,
      "loss": 0.5729,
      "step": 900
    },
    {
      "epoch": 1.802,
      "grad_norm": 0.8373274207115173,
      "learning_rate": 0.00016402561024409765,
      "loss": 0.5701,
      "step": 901
    },
    {
      "epoch": 1.804,
      "grad_norm": 1.3745079040527344,
      "learning_rate": 0.00016398559423769508,
      "loss": 0.8296,
      "step": 902
    },
    {
      "epoch": 1.806,
      "grad_norm": 1.007744550704956,
      "learning_rate": 0.00016394557823129252,
      "loss": 0.5873,
      "step": 903
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.0360445976257324,
      "learning_rate": 0.00016390556222488999,
      "loss": 0.6123,
      "step": 904
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.9358206391334534,
      "learning_rate": 0.0001638655462184874,
      "loss": 0.5788,
      "step": 905
    },
    {
      "epoch": 1.812,
      "grad_norm": 1.073337435722351,
      "learning_rate": 0.00016382553021208483,
      "loss": 0.459,
      "step": 906
    },
    {
      "epoch": 1.814,
      "grad_norm": 1.3556761741638184,
      "learning_rate": 0.0001637855142056823,
      "loss": 0.5784,
      "step": 907
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 1.157926082611084,
      "learning_rate": 0.0001637454981992797,
      "loss": 0.7044,
      "step": 908
    },
    {
      "epoch": 1.818,
      "grad_norm": 0.9559356570243835,
      "learning_rate": 0.00016370548219287714,
      "loss": 0.5004,
      "step": 909
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.0695222616195679,
      "learning_rate": 0.0001636654661864746,
      "loss": 0.5713,
      "step": 910
    },
    {
      "epoch": 1.822,
      "grad_norm": 1.2750566005706787,
      "learning_rate": 0.00016362545018007202,
      "loss": 0.5905,
      "step": 911
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.9911559820175171,
      "learning_rate": 0.00016358543417366948,
      "loss": 0.5639,
      "step": 912
    },
    {
      "epoch": 1.826,
      "grad_norm": 1.2338730096817017,
      "learning_rate": 0.00016354541816726692,
      "loss": 0.6792,
      "step": 913
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 1.1564114093780518,
      "learning_rate": 0.00016350540216086436,
      "loss": 0.5675,
      "step": 914
    },
    {
      "epoch": 1.83,
      "grad_norm": 1.0155162811279297,
      "learning_rate": 0.0001634653861544618,
      "loss": 0.5917,
      "step": 915
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1.0309141874313354,
      "learning_rate": 0.00016342537014805923,
      "loss": 0.7497,
      "step": 916
    },
    {
      "epoch": 1.834,
      "grad_norm": 1.1829429864883423,
      "learning_rate": 0.00016338535414165667,
      "loss": 0.5678,
      "step": 917
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 0.9395422339439392,
      "learning_rate": 0.0001633453381352541,
      "loss": 0.5568,
      "step": 918
    },
    {
      "epoch": 1.838,
      "grad_norm": 1.0950796604156494,
      "learning_rate": 0.00016330532212885154,
      "loss": 0.6543,
      "step": 919
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.159612774848938,
      "learning_rate": 0.00016326530612244898,
      "loss": 0.6591,
      "step": 920
    },
    {
      "epoch": 1.842,
      "grad_norm": 0.9216699600219727,
      "learning_rate": 0.00016322529011604644,
      "loss": 0.6553,
      "step": 921
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 1.0543787479400635,
      "learning_rate": 0.00016318527410964385,
      "loss": 0.5238,
      "step": 922
    },
    {
      "epoch": 1.846,
      "grad_norm": 1.0768520832061768,
      "learning_rate": 0.00016314525810324132,
      "loss": 0.5488,
      "step": 923
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.9299106597900391,
      "learning_rate": 0.00016310524209683876,
      "loss": 0.7343,
      "step": 924
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.9031289219856262,
      "learning_rate": 0.00016306522609043617,
      "loss": 0.5389,
      "step": 925
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 0.952978789806366,
      "learning_rate": 0.00016302521008403363,
      "loss": 0.8036,
      "step": 926
    },
    {
      "epoch": 1.854,
      "grad_norm": 0.9215312600135803,
      "learning_rate": 0.00016298519407763107,
      "loss": 0.7162,
      "step": 927
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 2.2331502437591553,
      "learning_rate": 0.00016294517807122848,
      "loss": 0.9263,
      "step": 928
    },
    {
      "epoch": 1.858,
      "grad_norm": 1.6011086702346802,
      "learning_rate": 0.00016290516206482594,
      "loss": 0.9118,
      "step": 929
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.0348604917526245,
      "learning_rate": 0.00016286514605842338,
      "loss": 0.7025,
      "step": 930
    },
    {
      "epoch": 1.862,
      "grad_norm": 0.8512579798698425,
      "learning_rate": 0.00016282513005202082,
      "loss": 0.603,
      "step": 931
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.9976438879966736,
      "learning_rate": 0.00016278511404561825,
      "loss": 0.4796,
      "step": 932
    },
    {
      "epoch": 1.866,
      "grad_norm": 1.034428596496582,
      "learning_rate": 0.0001627450980392157,
      "loss": 0.4797,
      "step": 933
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 0.8507282137870789,
      "learning_rate": 0.00016270508203281313,
      "loss": 0.6527,
      "step": 934
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.8698385953903198,
      "learning_rate": 0.00016266506602641056,
      "loss": 0.522,
      "step": 935
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.886466383934021,
      "learning_rate": 0.000162625050020008,
      "loss": 0.6235,
      "step": 936
    },
    {
      "epoch": 1.874,
      "grad_norm": 0.8935819864273071,
      "learning_rate": 0.00016258503401360547,
      "loss": 0.6029,
      "step": 937
    },
    {
      "epoch": 1.876,
      "grad_norm": 0.8944278359413147,
      "learning_rate": 0.0001625450180072029,
      "loss": 0.4978,
      "step": 938
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 1.2690156698226929,
      "learning_rate": 0.0001625050020008003,
      "loss": 0.9181,
      "step": 939
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.1172881126403809,
      "learning_rate": 0.00016246498599439778,
      "loss": 0.5361,
      "step": 940
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 0.894995927810669,
      "learning_rate": 0.00016242496998799521,
      "loss": 0.5328,
      "step": 941
    },
    {
      "epoch": 1.884,
      "grad_norm": 0.8135939240455627,
      "learning_rate": 0.00016238495398159262,
      "loss": 0.5058,
      "step": 942
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 1.0151329040527344,
      "learning_rate": 0.0001623449379751901,
      "loss": 0.5157,
      "step": 943
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.0054584741592407,
      "learning_rate": 0.00016230492196878753,
      "loss": 0.791,
      "step": 944
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 0.8624423742294312,
      "learning_rate": 0.00016226490596238496,
      "loss": 0.4978,
      "step": 945
    },
    {
      "epoch": 1.892,
      "grad_norm": 1.021506905555725,
      "learning_rate": 0.0001622248899559824,
      "loss": 0.4836,
      "step": 946
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 0.9172539710998535,
      "learning_rate": 0.00016218487394957984,
      "loss": 0.472,
      "step": 947
    },
    {
      "epoch": 1.896,
      "grad_norm": 1.0386995077133179,
      "learning_rate": 0.00016214485794317727,
      "loss": 0.6657,
      "step": 948
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 1.1159751415252686,
      "learning_rate": 0.0001621048419367747,
      "loss": 0.5844,
      "step": 949
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.2272006273269653,
      "learning_rate": 0.00016206482593037215,
      "loss": 0.6986,
      "step": 950
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 1.0275382995605469,
      "learning_rate": 0.0001620248099239696,
      "loss": 0.6101,
      "step": 951
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.0607030391693115,
      "learning_rate": 0.00016198479391756702,
      "loss": 0.5208,
      "step": 952
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 0.984518826007843,
      "learning_rate": 0.00016194477791116446,
      "loss": 0.6768,
      "step": 953
    },
    {
      "epoch": 1.908,
      "grad_norm": 1.0944212675094604,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.7198,
      "step": 954
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 1.0273381471633911,
      "learning_rate": 0.00016186474589835936,
      "loss": 0.5918,
      "step": 955
    },
    {
      "epoch": 1.912,
      "grad_norm": 1.4341003894805908,
      "learning_rate": 0.00016182472989195677,
      "loss": 0.7427,
      "step": 956
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 0.9386453628540039,
      "learning_rate": 0.00016178471388555424,
      "loss": 0.5438,
      "step": 957
    },
    {
      "epoch": 1.916,
      "grad_norm": 0.9496095180511475,
      "learning_rate": 0.00016174469787915167,
      "loss": 0.4542,
      "step": 958
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 1.156126856803894,
      "learning_rate": 0.0001617046818727491,
      "loss": 0.6354,
      "step": 959
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.9812803268432617,
      "learning_rate": 0.00016166466586634655,
      "loss": 0.537,
      "step": 960
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 1.1788851022720337,
      "learning_rate": 0.00016162464985994398,
      "loss": 0.6529,
      "step": 961
    },
    {
      "epoch": 1.924,
      "grad_norm": 1.2360090017318726,
      "learning_rate": 0.00016158463385354142,
      "loss": 0.5636,
      "step": 962
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 1.0821267366409302,
      "learning_rate": 0.00016154461784713886,
      "loss": 0.6678,
      "step": 963
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.9030392169952393,
      "learning_rate": 0.0001615046018407363,
      "loss": 0.5876,
      "step": 964
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 0.9503375291824341,
      "learning_rate": 0.00016146458583433376,
      "loss": 0.7068,
      "step": 965
    },
    {
      "epoch": 1.932,
      "grad_norm": 0.927760124206543,
      "learning_rate": 0.00016142456982793117,
      "loss": 0.7042,
      "step": 966
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 0.857209324836731,
      "learning_rate": 0.0001613845538215286,
      "loss": 0.3696,
      "step": 967
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.1320163011550903,
      "learning_rate": 0.00016134453781512607,
      "loss": 0.5696,
      "step": 968
    },
    {
      "epoch": 1.938,
      "grad_norm": 0.9064521789550781,
      "learning_rate": 0.0001613045218087235,
      "loss": 0.6791,
      "step": 969
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.3373754024505615,
      "learning_rate": 0.00016126450580232092,
      "loss": 0.5452,
      "step": 970
    },
    {
      "epoch": 1.942,
      "grad_norm": 1.0549331903457642,
      "learning_rate": 0.00016122448979591838,
      "loss": 0.6512,
      "step": 971
    },
    {
      "epoch": 1.944,
      "grad_norm": 1.1543335914611816,
      "learning_rate": 0.00016118447378951582,
      "loss": 0.6888,
      "step": 972
    },
    {
      "epoch": 1.946,
      "grad_norm": 0.9781131148338318,
      "learning_rate": 0.00016114445778311326,
      "loss": 0.559,
      "step": 973
    },
    {
      "epoch": 1.948,
      "grad_norm": 1.137228012084961,
      "learning_rate": 0.0001611044417767107,
      "loss": 0.6703,
      "step": 974
    },
    {
      "epoch": 1.95,
      "grad_norm": 1.028766393661499,
      "learning_rate": 0.00016106442577030813,
      "loss": 0.7022,
      "step": 975
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.1971640586853027,
      "learning_rate": 0.00016102440976390557,
      "loss": 0.4476,
      "step": 976
    },
    {
      "epoch": 1.954,
      "grad_norm": 1.098822832107544,
      "learning_rate": 0.000160984393757503,
      "loss": 0.9149,
      "step": 977
    },
    {
      "epoch": 1.956,
      "grad_norm": 0.8120781183242798,
      "learning_rate": 0.00016094437775110044,
      "loss": 0.5742,
      "step": 978
    },
    {
      "epoch": 1.958,
      "grad_norm": 0.9488286375999451,
      "learning_rate": 0.0001609043617446979,
      "loss": 0.7154,
      "step": 979
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.2296152114868164,
      "learning_rate": 0.00016086434573829532,
      "loss": 0.5903,
      "step": 980
    },
    {
      "epoch": 1.962,
      "grad_norm": 1.1598303318023682,
      "learning_rate": 0.00016082432973189275,
      "loss": 0.6434,
      "step": 981
    },
    {
      "epoch": 1.964,
      "grad_norm": 0.9668370485305786,
      "learning_rate": 0.00016078431372549022,
      "loss": 0.6694,
      "step": 982
    },
    {
      "epoch": 1.966,
      "grad_norm": 1.304489016532898,
      "learning_rate": 0.00016074429771908763,
      "loss": 0.5674,
      "step": 983
    },
    {
      "epoch": 1.968,
      "grad_norm": 1.0641545057296753,
      "learning_rate": 0.00016070428171268507,
      "loss": 0.7928,
      "step": 984
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.9118685722351074,
      "learning_rate": 0.00016066426570628253,
      "loss": 0.4956,
      "step": 985
    },
    {
      "epoch": 1.972,
      "grad_norm": 1.1823787689208984,
      "learning_rate": 0.00016062424969987997,
      "loss": 0.5825,
      "step": 986
    },
    {
      "epoch": 1.974,
      "grad_norm": 1.2535570859909058,
      "learning_rate": 0.0001605842336934774,
      "loss": 0.5355,
      "step": 987
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.122708797454834,
      "learning_rate": 0.00016054421768707484,
      "loss": 0.6407,
      "step": 988
    },
    {
      "epoch": 1.978,
      "grad_norm": 1.2920386791229248,
      "learning_rate": 0.00016050420168067228,
      "loss": 0.6929,
      "step": 989
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.925889253616333,
      "learning_rate": 0.00016046418567426972,
      "loss": 0.5326,
      "step": 990
    },
    {
      "epoch": 1.982,
      "grad_norm": 1.0635844469070435,
      "learning_rate": 0.00016042416966786715,
      "loss": 0.5101,
      "step": 991
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.0844465494155884,
      "learning_rate": 0.0001603841536614646,
      "loss": 0.5959,
      "step": 992
    },
    {
      "epoch": 1.986,
      "grad_norm": 0.9555956721305847,
      "learning_rate": 0.00016034413765506205,
      "loss": 0.5033,
      "step": 993
    },
    {
      "epoch": 1.988,
      "grad_norm": 1.1911630630493164,
      "learning_rate": 0.00016030412164865946,
      "loss": 0.6173,
      "step": 994
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.974707841873169,
      "learning_rate": 0.0001602641056422569,
      "loss": 0.5576,
      "step": 995
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.9079520106315613,
      "learning_rate": 0.00016022408963585437,
      "loss": 0.4797,
      "step": 996
    },
    {
      "epoch": 1.994,
      "grad_norm": 0.9906744956970215,
      "learning_rate": 0.00016018407362945178,
      "loss": 0.4808,
      "step": 997
    },
    {
      "epoch": 1.996,
      "grad_norm": 0.9409071803092957,
      "learning_rate": 0.00016014405762304924,
      "loss": 0.5765,
      "step": 998
    },
    {
      "epoch": 1.998,
      "grad_norm": 1.0272035598754883,
      "learning_rate": 0.00016010404161664668,
      "loss": 0.6591,
      "step": 999
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.0895428657531738,
      "learning_rate": 0.0001600640256102441,
      "loss": 0.7769,
      "step": 1000
    },
    {
      "epoch": 2.002,
      "grad_norm": 0.8604057431221008,
      "learning_rate": 0.00016002400960384155,
      "loss": 0.4381,
      "step": 1001
    },
    {
      "epoch": 2.004,
      "grad_norm": 0.8128531575202942,
      "learning_rate": 0.000159983993597439,
      "loss": 0.5722,
      "step": 1002
    },
    {
      "epoch": 2.006,
      "grad_norm": 0.8074403405189514,
      "learning_rate": 0.00015994397759103643,
      "loss": 0.4791,
      "step": 1003
    },
    {
      "epoch": 2.008,
      "grad_norm": 1.0126177072525024,
      "learning_rate": 0.00015990396158463386,
      "loss": 0.4275,
      "step": 1004
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.7256768941879272,
      "learning_rate": 0.0001598639455782313,
      "loss": 0.4504,
      "step": 1005
    },
    {
      "epoch": 2.012,
      "grad_norm": 1.0362398624420166,
      "learning_rate": 0.00015982392957182874,
      "loss": 0.578,
      "step": 1006
    },
    {
      "epoch": 2.014,
      "grad_norm": 0.8870787620544434,
      "learning_rate": 0.00015978391356542617,
      "loss": 0.5015,
      "step": 1007
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.9029262065887451,
      "learning_rate": 0.0001597438975590236,
      "loss": 0.5708,
      "step": 1008
    },
    {
      "epoch": 2.018,
      "grad_norm": 0.7464474439620972,
      "learning_rate": 0.00015970388155262105,
      "loss": 0.3838,
      "step": 1009
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.0647715330123901,
      "learning_rate": 0.0001596638655462185,
      "loss": 0.6331,
      "step": 1010
    },
    {
      "epoch": 2.022,
      "grad_norm": 0.7273930311203003,
      "learning_rate": 0.00015962384953981592,
      "loss": 0.5013,
      "step": 1011
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.7878036499023438,
      "learning_rate": 0.00015958383353341339,
      "loss": 0.4128,
      "step": 1012
    },
    {
      "epoch": 2.026,
      "grad_norm": 1.0518152713775635,
      "learning_rate": 0.00015954381752701082,
      "loss": 0.6587,
      "step": 1013
    },
    {
      "epoch": 2.028,
      "grad_norm": 0.930223822593689,
      "learning_rate": 0.00015950380152060823,
      "loss": 0.4132,
      "step": 1014
    },
    {
      "epoch": 2.03,
      "grad_norm": 1.1354950666427612,
      "learning_rate": 0.0001594637855142057,
      "loss": 0.4117,
      "step": 1015
    },
    {
      "epoch": 2.032,
      "grad_norm": 1.0797935724258423,
      "learning_rate": 0.00015942376950780313,
      "loss": 0.5898,
      "step": 1016
    },
    {
      "epoch": 2.034,
      "grad_norm": 0.8848299384117126,
      "learning_rate": 0.00015938375350140057,
      "loss": 0.3919,
      "step": 1017
    },
    {
      "epoch": 2.036,
      "grad_norm": 0.9102016091346741,
      "learning_rate": 0.000159343737494998,
      "loss": 0.4144,
      "step": 1018
    },
    {
      "epoch": 2.038,
      "grad_norm": 1.3949012756347656,
      "learning_rate": 0.00015930372148859545,
      "loss": 0.5152,
      "step": 1019
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.1535249948501587,
      "learning_rate": 0.00015926370548219288,
      "loss": 0.518,
      "step": 1020
    },
    {
      "epoch": 2.042,
      "grad_norm": 1.079609751701355,
      "learning_rate": 0.00015922368947579032,
      "loss": 0.3741,
      "step": 1021
    },
    {
      "epoch": 2.044,
      "grad_norm": 0.9914529323577881,
      "learning_rate": 0.00015918367346938776,
      "loss": 0.3561,
      "step": 1022
    },
    {
      "epoch": 2.046,
      "grad_norm": 1.1200329065322876,
      "learning_rate": 0.0001591436574629852,
      "loss": 0.6598,
      "step": 1023
    },
    {
      "epoch": 2.048,
      "grad_norm": 2.971411943435669,
      "learning_rate": 0.00015910364145658263,
      "loss": 0.6152,
      "step": 1024
    },
    {
      "epoch": 2.05,
      "grad_norm": 1.0561589002609253,
      "learning_rate": 0.00015906362545018007,
      "loss": 0.472,
      "step": 1025
    },
    {
      "epoch": 2.052,
      "grad_norm": 1.3613141775131226,
      "learning_rate": 0.00015902360944377753,
      "loss": 0.5467,
      "step": 1026
    },
    {
      "epoch": 2.054,
      "grad_norm": 0.887898325920105,
      "learning_rate": 0.00015898359343737497,
      "loss": 0.3471,
      "step": 1027
    },
    {
      "epoch": 2.056,
      "grad_norm": 1.1139181852340698,
      "learning_rate": 0.00015894357743097238,
      "loss": 0.3827,
      "step": 1028
    },
    {
      "epoch": 2.058,
      "grad_norm": 1.030069351196289,
      "learning_rate": 0.00015890356142456984,
      "loss": 0.5288,
      "step": 1029
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.0296753644943237,
      "learning_rate": 0.00015886354541816728,
      "loss": 0.4798,
      "step": 1030
    },
    {
      "epoch": 2.062,
      "grad_norm": 0.9768093824386597,
      "learning_rate": 0.0001588235294117647,
      "loss": 0.4786,
      "step": 1031
    },
    {
      "epoch": 2.064,
      "grad_norm": 1.1301759481430054,
      "learning_rate": 0.00015878351340536216,
      "loss": 0.5217,
      "step": 1032
    },
    {
      "epoch": 2.066,
      "grad_norm": 0.8428903222084045,
      "learning_rate": 0.0001587434973989596,
      "loss": 0.5142,
      "step": 1033
    },
    {
      "epoch": 2.068,
      "grad_norm": 0.7979689240455627,
      "learning_rate": 0.00015870348139255703,
      "loss": 0.485,
      "step": 1034
    },
    {
      "epoch": 2.07,
      "grad_norm": 1.0936729907989502,
      "learning_rate": 0.00015866346538615447,
      "loss": 0.4189,
      "step": 1035
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.9771217107772827,
      "learning_rate": 0.0001586234493797519,
      "loss": 0.5402,
      "step": 1036
    },
    {
      "epoch": 2.074,
      "grad_norm": 1.0025138854980469,
      "learning_rate": 0.00015858343337334934,
      "loss": 0.3419,
      "step": 1037
    },
    {
      "epoch": 2.076,
      "grad_norm": 0.9099799990653992,
      "learning_rate": 0.00015854341736694678,
      "loss": 0.5973,
      "step": 1038
    },
    {
      "epoch": 2.078,
      "grad_norm": 0.8810722827911377,
      "learning_rate": 0.00015850340136054422,
      "loss": 0.5062,
      "step": 1039
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.1676583290100098,
      "learning_rate": 0.00015846338535414168,
      "loss": 0.3895,
      "step": 1040
    },
    {
      "epoch": 2.082,
      "grad_norm": 1.0499953031539917,
      "learning_rate": 0.0001584233693477391,
      "loss": 0.5007,
      "step": 1041
    },
    {
      "epoch": 2.084,
      "grad_norm": 0.9850282669067383,
      "learning_rate": 0.00015838335334133653,
      "loss": 0.4584,
      "step": 1042
    },
    {
      "epoch": 2.086,
      "grad_norm": 1.0783787965774536,
      "learning_rate": 0.000158343337334934,
      "loss": 0.5406,
      "step": 1043
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.9243510365486145,
      "learning_rate": 0.00015830332132853143,
      "loss": 0.4857,
      "step": 1044
    },
    {
      "epoch": 2.09,
      "grad_norm": 1.0187236070632935,
      "learning_rate": 0.00015826330532212884,
      "loss": 0.5132,
      "step": 1045
    },
    {
      "epoch": 2.092,
      "grad_norm": 1.299297571182251,
      "learning_rate": 0.0001582232893157263,
      "loss": 0.5217,
      "step": 1046
    },
    {
      "epoch": 2.094,
      "grad_norm": 1.2794320583343506,
      "learning_rate": 0.00015818327330932374,
      "loss": 0.4734,
      "step": 1047
    },
    {
      "epoch": 2.096,
      "grad_norm": 1.2751609086990356,
      "learning_rate": 0.00015814325730292118,
      "loss": 0.4354,
      "step": 1048
    },
    {
      "epoch": 2.098,
      "grad_norm": 1.807441234588623,
      "learning_rate": 0.00015810324129651861,
      "loss": 0.5426,
      "step": 1049
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.1846085786819458,
      "learning_rate": 0.00015806322529011605,
      "loss": 0.4985,
      "step": 1050
    },
    {
      "epoch": 2.102,
      "grad_norm": 1.2046180963516235,
      "learning_rate": 0.00015802320928371352,
      "loss": 0.456,
      "step": 1051
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.3256844282150269,
      "learning_rate": 0.00015798319327731093,
      "loss": 0.5283,
      "step": 1052
    },
    {
      "epoch": 2.106,
      "grad_norm": 1.100555419921875,
      "learning_rate": 0.00015794317727090836,
      "loss": 0.5083,
      "step": 1053
    },
    {
      "epoch": 2.108,
      "grad_norm": 1.3652349710464478,
      "learning_rate": 0.00015790316126450583,
      "loss": 0.4545,
      "step": 1054
    },
    {
      "epoch": 2.11,
      "grad_norm": 1.2115588188171387,
      "learning_rate": 0.00015786314525810324,
      "loss": 0.5651,
      "step": 1055
    },
    {
      "epoch": 2.112,
      "grad_norm": 1.0537053346633911,
      "learning_rate": 0.00015782312925170067,
      "loss": 0.3895,
      "step": 1056
    },
    {
      "epoch": 2.114,
      "grad_norm": 1.3716386556625366,
      "learning_rate": 0.00015778311324529814,
      "loss": 0.598,
      "step": 1057
    },
    {
      "epoch": 2.116,
      "grad_norm": 1.3820918798446655,
      "learning_rate": 0.00015774309723889558,
      "loss": 0.6535,
      "step": 1058
    },
    {
      "epoch": 2.118,
      "grad_norm": 1.7381151914596558,
      "learning_rate": 0.000157703081232493,
      "loss": 0.5315,
      "step": 1059
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.1163098812103271,
      "learning_rate": 0.00015766306522609045,
      "loss": 0.4478,
      "step": 1060
    },
    {
      "epoch": 2.122,
      "grad_norm": 1.1661943197250366,
      "learning_rate": 0.0001576230492196879,
      "loss": 0.4789,
      "step": 1061
    },
    {
      "epoch": 2.124,
      "grad_norm": 1.0939100980758667,
      "learning_rate": 0.00015758303321328532,
      "loss": 0.4106,
      "step": 1062
    },
    {
      "epoch": 2.126,
      "grad_norm": 1.1105897426605225,
      "learning_rate": 0.00015754301720688276,
      "loss": 0.4413,
      "step": 1063
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.9914541244506836,
      "learning_rate": 0.0001575030012004802,
      "loss": 0.4001,
      "step": 1064
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.8821014761924744,
      "learning_rate": 0.00015746298519407764,
      "loss": 0.3904,
      "step": 1065
    },
    {
      "epoch": 2.132,
      "grad_norm": 1.079311490058899,
      "learning_rate": 0.00015742296918767507,
      "loss": 0.4486,
      "step": 1066
    },
    {
      "epoch": 2.134,
      "grad_norm": 1.1599774360656738,
      "learning_rate": 0.0001573829531812725,
      "loss": 0.4836,
      "step": 1067
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.9303858280181885,
      "learning_rate": 0.00015734293717486997,
      "loss": 0.3673,
      "step": 1068
    },
    {
      "epoch": 2.138,
      "grad_norm": 1.1260669231414795,
      "learning_rate": 0.00015730292116846738,
      "loss": 0.369,
      "step": 1069
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.418434977531433,
      "learning_rate": 0.00015726290516206482,
      "loss": 0.4545,
      "step": 1070
    },
    {
      "epoch": 2.142,
      "grad_norm": 1.3466216325759888,
      "learning_rate": 0.00015722288915566229,
      "loss": 0.6145,
      "step": 1071
    },
    {
      "epoch": 2.144,
      "grad_norm": 1.0075957775115967,
      "learning_rate": 0.0001571828731492597,
      "loss": 0.3692,
      "step": 1072
    },
    {
      "epoch": 2.146,
      "grad_norm": 0.9377045035362244,
      "learning_rate": 0.00015714285714285716,
      "loss": 0.4315,
      "step": 1073
    },
    {
      "epoch": 2.148,
      "grad_norm": 1.454721450805664,
      "learning_rate": 0.0001571028411364546,
      "loss": 0.4585,
      "step": 1074
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.8827632665634155,
      "learning_rate": 0.00015706282513005203,
      "loss": 0.3336,
      "step": 1075
    },
    {
      "epoch": 2.152,
      "grad_norm": 1.0287723541259766,
      "learning_rate": 0.00015702280912364947,
      "loss": 0.4816,
      "step": 1076
    },
    {
      "epoch": 2.154,
      "grad_norm": 1.1881043910980225,
      "learning_rate": 0.0001569827931172469,
      "loss": 0.5759,
      "step": 1077
    },
    {
      "epoch": 2.156,
      "grad_norm": 1.1969897747039795,
      "learning_rate": 0.00015694277711084435,
      "loss": 0.6063,
      "step": 1078
    },
    {
      "epoch": 2.158,
      "grad_norm": 0.9133182764053345,
      "learning_rate": 0.00015690276110444178,
      "loss": 0.3116,
      "step": 1079
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.3497072458267212,
      "learning_rate": 0.00015686274509803922,
      "loss": 0.5173,
      "step": 1080
    },
    {
      "epoch": 2.162,
      "grad_norm": 1.0069819688796997,
      "learning_rate": 0.00015682272909163666,
      "loss": 0.4798,
      "step": 1081
    },
    {
      "epoch": 2.164,
      "grad_norm": 1.3498568534851074,
      "learning_rate": 0.00015678271308523412,
      "loss": 0.5112,
      "step": 1082
    },
    {
      "epoch": 2.166,
      "grad_norm": 1.0996004343032837,
      "learning_rate": 0.00015674269707883153,
      "loss": 0.4153,
      "step": 1083
    },
    {
      "epoch": 2.168,
      "grad_norm": 1.4771220684051514,
      "learning_rate": 0.00015670268107242897,
      "loss": 0.6432,
      "step": 1084
    },
    {
      "epoch": 2.17,
      "grad_norm": 1.211094856262207,
      "learning_rate": 0.00015666266506602643,
      "loss": 0.4406,
      "step": 1085
    },
    {
      "epoch": 2.172,
      "grad_norm": 0.9909419417381287,
      "learning_rate": 0.00015662264905962384,
      "loss": 0.38,
      "step": 1086
    },
    {
      "epoch": 2.174,
      "grad_norm": 0.932614266872406,
      "learning_rate": 0.0001565826330532213,
      "loss": 0.4012,
      "step": 1087
    },
    {
      "epoch": 2.176,
      "grad_norm": 1.2978562116622925,
      "learning_rate": 0.00015654261704681874,
      "loss": 0.6011,
      "step": 1088
    },
    {
      "epoch": 2.178,
      "grad_norm": 1.3017641305923462,
      "learning_rate": 0.00015650260104041615,
      "loss": 0.441,
      "step": 1089
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.1477705240249634,
      "learning_rate": 0.00015646258503401362,
      "loss": 0.4598,
      "step": 1090
    },
    {
      "epoch": 2.182,
      "grad_norm": 1.07474946975708,
      "learning_rate": 0.00015642256902761106,
      "loss": 0.3991,
      "step": 1091
    },
    {
      "epoch": 2.184,
      "grad_norm": 1.1045503616333008,
      "learning_rate": 0.0001563825530212085,
      "loss": 0.5649,
      "step": 1092
    },
    {
      "epoch": 2.186,
      "grad_norm": 1.2247120141983032,
      "learning_rate": 0.00015634253701480593,
      "loss": 0.5312,
      "step": 1093
    },
    {
      "epoch": 2.188,
      "grad_norm": 1.1614408493041992,
      "learning_rate": 0.00015630252100840337,
      "loss": 0.504,
      "step": 1094
    },
    {
      "epoch": 2.19,
      "grad_norm": 1.0915337800979614,
      "learning_rate": 0.0001562625050020008,
      "loss": 0.4068,
      "step": 1095
    },
    {
      "epoch": 2.192,
      "grad_norm": 1.0956655740737915,
      "learning_rate": 0.00015622248899559824,
      "loss": 0.454,
      "step": 1096
    },
    {
      "epoch": 2.194,
      "grad_norm": 1.1339761018753052,
      "learning_rate": 0.00015618247298919568,
      "loss": 0.5597,
      "step": 1097
    },
    {
      "epoch": 2.196,
      "grad_norm": 1.3015409708023071,
      "learning_rate": 0.00015614245698279312,
      "loss": 0.4973,
      "step": 1098
    },
    {
      "epoch": 2.198,
      "grad_norm": 1.1912561655044556,
      "learning_rate": 0.00015610244097639058,
      "loss": 0.5671,
      "step": 1099
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.2496368885040283,
      "learning_rate": 0.000156062424969988,
      "loss": 0.5088,
      "step": 1100
    },
    {
      "epoch": 2.202,
      "grad_norm": 1.2852985858917236,
      "learning_rate": 0.00015602240896358545,
      "loss": 0.4584,
      "step": 1101
    },
    {
      "epoch": 2.204,
      "grad_norm": 1.1824597120285034,
      "learning_rate": 0.0001559823929571829,
      "loss": 0.4074,
      "step": 1102
    },
    {
      "epoch": 2.206,
      "grad_norm": 1.2641762495040894,
      "learning_rate": 0.0001559423769507803,
      "loss": 0.5483,
      "step": 1103
    },
    {
      "epoch": 2.208,
      "grad_norm": 1.4286266565322876,
      "learning_rate": 0.00015590236094437777,
      "loss": 0.507,
      "step": 1104
    },
    {
      "epoch": 2.21,
      "grad_norm": 1.2120848894119263,
      "learning_rate": 0.0001558623449379752,
      "loss": 0.54,
      "step": 1105
    },
    {
      "epoch": 2.212,
      "grad_norm": 1.3850862979888916,
      "learning_rate": 0.00015582232893157264,
      "loss": 0.5876,
      "step": 1106
    },
    {
      "epoch": 2.214,
      "grad_norm": 1.385394811630249,
      "learning_rate": 0.00015578231292517008,
      "loss": 0.5617,
      "step": 1107
    },
    {
      "epoch": 2.216,
      "grad_norm": 1.2056792974472046,
      "learning_rate": 0.00015574229691876751,
      "loss": 0.5545,
      "step": 1108
    },
    {
      "epoch": 2.218,
      "grad_norm": 1.2184354066848755,
      "learning_rate": 0.00015570228091236495,
      "loss": 0.5192,
      "step": 1109
    },
    {
      "epoch": 2.22,
      "grad_norm": 1.1479566097259521,
      "learning_rate": 0.0001556622649059624,
      "loss": 0.4996,
      "step": 1110
    },
    {
      "epoch": 2.222,
      "grad_norm": 1.150773048400879,
      "learning_rate": 0.00015562224889955983,
      "loss": 0.5053,
      "step": 1111
    },
    {
      "epoch": 2.224,
      "grad_norm": 1.3233007192611694,
      "learning_rate": 0.00015558223289315726,
      "loss": 0.4721,
      "step": 1112
    },
    {
      "epoch": 2.226,
      "grad_norm": 1.3635979890823364,
      "learning_rate": 0.0001555422168867547,
      "loss": 0.6584,
      "step": 1113
    },
    {
      "epoch": 2.228,
      "grad_norm": 1.0315805673599243,
      "learning_rate": 0.00015550220088035214,
      "loss": 0.4547,
      "step": 1114
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.9858506321907043,
      "learning_rate": 0.0001554621848739496,
      "loss": 0.4949,
      "step": 1115
    },
    {
      "epoch": 2.232,
      "grad_norm": 1.0781354904174805,
      "learning_rate": 0.00015542216886754704,
      "loss": 0.3898,
      "step": 1116
    },
    {
      "epoch": 2.234,
      "grad_norm": 1.0288970470428467,
      "learning_rate": 0.00015538215286114445,
      "loss": 0.5092,
      "step": 1117
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 1.1863878965377808,
      "learning_rate": 0.0001553421368547419,
      "loss": 0.4197,
      "step": 1118
    },
    {
      "epoch": 2.238,
      "grad_norm": 1.3088740110397339,
      "learning_rate": 0.00015530212084833935,
      "loss": 0.6246,
      "step": 1119
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.9728412628173828,
      "learning_rate": 0.00015526210484193676,
      "loss": 0.3064,
      "step": 1120
    },
    {
      "epoch": 2.242,
      "grad_norm": 1.4386053085327148,
      "learning_rate": 0.00015522208883553422,
      "loss": 0.5483,
      "step": 1121
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 0.957940936088562,
      "learning_rate": 0.00015518207282913166,
      "loss": 0.356,
      "step": 1122
    },
    {
      "epoch": 2.246,
      "grad_norm": 1.1851922273635864,
      "learning_rate": 0.0001551420568227291,
      "loss": 0.3328,
      "step": 1123
    },
    {
      "epoch": 2.248,
      "grad_norm": 1.0576611757278442,
      "learning_rate": 0.00015510204081632654,
      "loss": 0.5353,
      "step": 1124
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.2993557453155518,
      "learning_rate": 0.00015506202480992397,
      "loss": 0.6204,
      "step": 1125
    },
    {
      "epoch": 2.252,
      "grad_norm": 1.2064802646636963,
      "learning_rate": 0.00015502200880352144,
      "loss": 0.4979,
      "step": 1126
    },
    {
      "epoch": 2.254,
      "grad_norm": 1.2732504606246948,
      "learning_rate": 0.00015498199279711885,
      "loss": 0.4997,
      "step": 1127
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.9909352660179138,
      "learning_rate": 0.00015494197679071628,
      "loss": 0.3804,
      "step": 1128
    },
    {
      "epoch": 2.258,
      "grad_norm": 0.9913284778594971,
      "learning_rate": 0.00015490196078431375,
      "loss": 0.4598,
      "step": 1129
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.1560161113739014,
      "learning_rate": 0.00015486194477791116,
      "loss": 0.4309,
      "step": 1130
    },
    {
      "epoch": 2.262,
      "grad_norm": 1.0362950563430786,
      "learning_rate": 0.0001548219287715086,
      "loss": 0.5757,
      "step": 1131
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 1.1530095338821411,
      "learning_rate": 0.00015478191276510606,
      "loss": 0.372,
      "step": 1132
    },
    {
      "epoch": 2.266,
      "grad_norm": 0.9976474046707153,
      "learning_rate": 0.0001547418967587035,
      "loss": 0.3616,
      "step": 1133
    },
    {
      "epoch": 2.268,
      "grad_norm": 1.2258061170578003,
      "learning_rate": 0.00015470188075230093,
      "loss": 0.6047,
      "step": 1134
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.1536250114440918,
      "learning_rate": 0.00015466186474589837,
      "loss": 0.5425,
      "step": 1135
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 1.2348963022232056,
      "learning_rate": 0.0001546218487394958,
      "loss": 0.4968,
      "step": 1136
    },
    {
      "epoch": 2.274,
      "grad_norm": 1.094547152519226,
      "learning_rate": 0.00015458183273309325,
      "loss": 0.6159,
      "step": 1137
    },
    {
      "epoch": 2.276,
      "grad_norm": 0.9952492117881775,
      "learning_rate": 0.00015454181672669068,
      "loss": 0.3947,
      "step": 1138
    },
    {
      "epoch": 2.278,
      "grad_norm": 1.2198764085769653,
      "learning_rate": 0.00015450180072028812,
      "loss": 0.4927,
      "step": 1139
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.1054636240005493,
      "learning_rate": 0.00015446178471388558,
      "loss": 0.4808,
      "step": 1140
    },
    {
      "epoch": 2.282,
      "grad_norm": 1.4009732007980347,
      "learning_rate": 0.000154421768707483,
      "loss": 0.614,
      "step": 1141
    },
    {
      "epoch": 2.284,
      "grad_norm": 0.9363004565238953,
      "learning_rate": 0.00015438175270108043,
      "loss": 0.5178,
      "step": 1142
    },
    {
      "epoch": 2.286,
      "grad_norm": 1.1792868375778198,
      "learning_rate": 0.0001543417366946779,
      "loss": 0.4805,
      "step": 1143
    },
    {
      "epoch": 2.288,
      "grad_norm": 1.387250542640686,
      "learning_rate": 0.0001543017206882753,
      "loss": 0.5495,
      "step": 1144
    },
    {
      "epoch": 2.29,
      "grad_norm": 1.0677543878555298,
      "learning_rate": 0.00015426170468187274,
      "loss": 0.372,
      "step": 1145
    },
    {
      "epoch": 2.292,
      "grad_norm": 1.2114057540893555,
      "learning_rate": 0.0001542216886754702,
      "loss": 0.4494,
      "step": 1146
    },
    {
      "epoch": 2.294,
      "grad_norm": 1.8646240234375,
      "learning_rate": 0.00015418167266906764,
      "loss": 0.5406,
      "step": 1147
    },
    {
      "epoch": 2.296,
      "grad_norm": 1.2873257398605347,
      "learning_rate": 0.00015414165666266508,
      "loss": 0.4973,
      "step": 1148
    },
    {
      "epoch": 2.298,
      "grad_norm": 1.2287280559539795,
      "learning_rate": 0.00015410164065626252,
      "loss": 0.5217,
      "step": 1149
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.0285059213638306,
      "learning_rate": 0.00015406162464985996,
      "loss": 0.3665,
      "step": 1150
    },
    {
      "epoch": 2.302,
      "grad_norm": 1.5926989316940308,
      "learning_rate": 0.0001540216086434574,
      "loss": 0.3741,
      "step": 1151
    },
    {
      "epoch": 2.304,
      "grad_norm": 1.1073427200317383,
      "learning_rate": 0.00015398159263705483,
      "loss": 0.4548,
      "step": 1152
    },
    {
      "epoch": 2.306,
      "grad_norm": 1.1871566772460938,
      "learning_rate": 0.00015394157663065227,
      "loss": 0.4739,
      "step": 1153
    },
    {
      "epoch": 2.308,
      "grad_norm": 1.1852762699127197,
      "learning_rate": 0.0001539015606242497,
      "loss": 0.5618,
      "step": 1154
    },
    {
      "epoch": 2.31,
      "grad_norm": 1.2089885473251343,
      "learning_rate": 0.00015386154461784714,
      "loss": 0.4345,
      "step": 1155
    },
    {
      "epoch": 2.312,
      "grad_norm": 1.2092864513397217,
      "learning_rate": 0.00015382152861144458,
      "loss": 0.5633,
      "step": 1156
    },
    {
      "epoch": 2.314,
      "grad_norm": 0.9940281510353088,
      "learning_rate": 0.00015378151260504204,
      "loss": 0.4208,
      "step": 1157
    },
    {
      "epoch": 2.316,
      "grad_norm": 1.2415192127227783,
      "learning_rate": 0.00015374149659863945,
      "loss": 0.6134,
      "step": 1158
    },
    {
      "epoch": 2.318,
      "grad_norm": 1.16953444480896,
      "learning_rate": 0.0001537014805922369,
      "loss": 0.5207,
      "step": 1159
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.1371480226516724,
      "learning_rate": 0.00015366146458583435,
      "loss": 0.3947,
      "step": 1160
    },
    {
      "epoch": 2.322,
      "grad_norm": 1.2049057483673096,
      "learning_rate": 0.00015362144857943176,
      "loss": 0.5191,
      "step": 1161
    },
    {
      "epoch": 2.324,
      "grad_norm": 1.3223036527633667,
      "learning_rate": 0.00015358143257302923,
      "loss": 0.5847,
      "step": 1162
    },
    {
      "epoch": 2.326,
      "grad_norm": 1.252976894378662,
      "learning_rate": 0.00015354141656662666,
      "loss": 0.4782,
      "step": 1163
    },
    {
      "epoch": 2.328,
      "grad_norm": 1.377053141593933,
      "learning_rate": 0.0001535014005602241,
      "loss": 0.5096,
      "step": 1164
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.9544422030448914,
      "learning_rate": 0.00015346138455382154,
      "loss": 0.4364,
      "step": 1165
    },
    {
      "epoch": 2.332,
      "grad_norm": 1.1293847560882568,
      "learning_rate": 0.00015342136854741898,
      "loss": 0.3986,
      "step": 1166
    },
    {
      "epoch": 2.334,
      "grad_norm": 0.9738654494285583,
      "learning_rate": 0.0001533813525410164,
      "loss": 0.3735,
      "step": 1167
    },
    {
      "epoch": 2.336,
      "grad_norm": 1.1278618574142456,
      "learning_rate": 0.00015334133653461385,
      "loss": 0.4432,
      "step": 1168
    },
    {
      "epoch": 2.338,
      "grad_norm": 0.8959803581237793,
      "learning_rate": 0.0001533013205282113,
      "loss": 0.4025,
      "step": 1169
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.9893457293510437,
      "learning_rate": 0.00015326130452180872,
      "loss": 0.3734,
      "step": 1170
    },
    {
      "epoch": 2.342,
      "grad_norm": 1.2552595138549805,
      "learning_rate": 0.0001532212885154062,
      "loss": 0.4414,
      "step": 1171
    },
    {
      "epoch": 2.344,
      "grad_norm": 1.179391860961914,
      "learning_rate": 0.0001531812725090036,
      "loss": 0.4282,
      "step": 1172
    },
    {
      "epoch": 2.346,
      "grad_norm": 1.8489619493484497,
      "learning_rate": 0.00015314125650260104,
      "loss": 0.4455,
      "step": 1173
    },
    {
      "epoch": 2.348,
      "grad_norm": 0.950971782207489,
      "learning_rate": 0.0001531012404961985,
      "loss": 0.3852,
      "step": 1174
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.9086323976516724,
      "learning_rate": 0.0001530612244897959,
      "loss": 0.3956,
      "step": 1175
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.9602469205856323,
      "learning_rate": 0.00015302120848339337,
      "loss": 0.3054,
      "step": 1176
    },
    {
      "epoch": 2.354,
      "grad_norm": 1.0417379140853882,
      "learning_rate": 0.0001529811924769908,
      "loss": 0.3554,
      "step": 1177
    },
    {
      "epoch": 2.356,
      "grad_norm": 0.9658034443855286,
      "learning_rate": 0.00015294117647058822,
      "loss": 0.4017,
      "step": 1178
    },
    {
      "epoch": 2.358,
      "grad_norm": 0.9552946090698242,
      "learning_rate": 0.00015290116046418569,
      "loss": 0.3459,
      "step": 1179
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.0464601516723633,
      "learning_rate": 0.00015286114445778312,
      "loss": 0.4259,
      "step": 1180
    },
    {
      "epoch": 2.362,
      "grad_norm": 1.1547024250030518,
      "learning_rate": 0.00015282112845138056,
      "loss": 0.454,
      "step": 1181
    },
    {
      "epoch": 2.364,
      "grad_norm": 1.9809752702713013,
      "learning_rate": 0.000152781112444978,
      "loss": 0.6222,
      "step": 1182
    },
    {
      "epoch": 2.366,
      "grad_norm": 1.1258660554885864,
      "learning_rate": 0.00015274109643857543,
      "loss": 0.5306,
      "step": 1183
    },
    {
      "epoch": 2.368,
      "grad_norm": 1.0762605667114258,
      "learning_rate": 0.00015270108043217287,
      "loss": 0.4942,
      "step": 1184
    },
    {
      "epoch": 2.37,
      "grad_norm": 1.2584251165390015,
      "learning_rate": 0.0001526610644257703,
      "loss": 0.719,
      "step": 1185
    },
    {
      "epoch": 2.372,
      "grad_norm": 0.9495676159858704,
      "learning_rate": 0.00015262104841936775,
      "loss": 0.4537,
      "step": 1186
    },
    {
      "epoch": 2.374,
      "grad_norm": 1.267246961593628,
      "learning_rate": 0.0001525810324129652,
      "loss": 0.5295,
      "step": 1187
    },
    {
      "epoch": 2.376,
      "grad_norm": 1.268702745437622,
      "learning_rate": 0.00015254101640656265,
      "loss": 0.5509,
      "step": 1188
    },
    {
      "epoch": 2.378,
      "grad_norm": 1.153107762336731,
      "learning_rate": 0.00015250100040016006,
      "loss": 0.5571,
      "step": 1189
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.2334355115890503,
      "learning_rate": 0.00015246098439375752,
      "loss": 0.5112,
      "step": 1190
    },
    {
      "epoch": 2.382,
      "grad_norm": 1.131182312965393,
      "learning_rate": 0.00015242096838735496,
      "loss": 0.6131,
      "step": 1191
    },
    {
      "epoch": 2.384,
      "grad_norm": 1.339043378829956,
      "learning_rate": 0.00015238095238095237,
      "loss": 0.5103,
      "step": 1192
    },
    {
      "epoch": 2.386,
      "grad_norm": 1.026638388633728,
      "learning_rate": 0.00015234093637454983,
      "loss": 0.4712,
      "step": 1193
    },
    {
      "epoch": 2.388,
      "grad_norm": 1.491451621055603,
      "learning_rate": 0.00015230092036814727,
      "loss": 0.4765,
      "step": 1194
    },
    {
      "epoch": 2.39,
      "grad_norm": 1.202276349067688,
      "learning_rate": 0.0001522609043617447,
      "loss": 0.5429,
      "step": 1195
    },
    {
      "epoch": 2.392,
      "grad_norm": 1.029751181602478,
      "learning_rate": 0.00015222088835534214,
      "loss": 0.3783,
      "step": 1196
    },
    {
      "epoch": 2.394,
      "grad_norm": 1.0808238983154297,
      "learning_rate": 0.00015218087234893958,
      "loss": 0.5234,
      "step": 1197
    },
    {
      "epoch": 2.396,
      "grad_norm": 1.4477404356002808,
      "learning_rate": 0.00015214085634253702,
      "loss": 0.4295,
      "step": 1198
    },
    {
      "epoch": 2.398,
      "grad_norm": 1.0938161611557007,
      "learning_rate": 0.00015210084033613446,
      "loss": 0.4169,
      "step": 1199
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.1579205989837646,
      "learning_rate": 0.0001520608243297319,
      "loss": 0.6506,
      "step": 1200
    },
    {
      "epoch": 2.402,
      "grad_norm": 1.032814621925354,
      "learning_rate": 0.00015202080832332936,
      "loss": 0.4649,
      "step": 1201
    },
    {
      "epoch": 2.404,
      "grad_norm": 1.4265235662460327,
      "learning_rate": 0.00015198079231692677,
      "loss": 0.4288,
      "step": 1202
    },
    {
      "epoch": 2.406,
      "grad_norm": 0.9608858227729797,
      "learning_rate": 0.0001519407763105242,
      "loss": 0.461,
      "step": 1203
    },
    {
      "epoch": 2.408,
      "grad_norm": 1.7202740907669067,
      "learning_rate": 0.00015190076030412167,
      "loss": 0.5857,
      "step": 1204
    },
    {
      "epoch": 2.41,
      "grad_norm": 1.0103682279586792,
      "learning_rate": 0.0001518607442977191,
      "loss": 0.3407,
      "step": 1205
    },
    {
      "epoch": 2.412,
      "grad_norm": 1.1885625123977661,
      "learning_rate": 0.00015182072829131652,
      "loss": 0.4954,
      "step": 1206
    },
    {
      "epoch": 2.414,
      "grad_norm": 0.9811200499534607,
      "learning_rate": 0.00015178071228491398,
      "loss": 0.4222,
      "step": 1207
    },
    {
      "epoch": 2.416,
      "grad_norm": 1.0245212316513062,
      "learning_rate": 0.00015174069627851142,
      "loss": 0.5204,
      "step": 1208
    },
    {
      "epoch": 2.418,
      "grad_norm": 1.0122151374816895,
      "learning_rate": 0.00015170068027210885,
      "loss": 0.5189,
      "step": 1209
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.9822579026222229,
      "learning_rate": 0.0001516606642657063,
      "loss": 0.4038,
      "step": 1210
    },
    {
      "epoch": 2.422,
      "grad_norm": 1.3153882026672363,
      "learning_rate": 0.00015162064825930373,
      "loss": 0.521,
      "step": 1211
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.9332687258720398,
      "learning_rate": 0.00015158063225290117,
      "loss": 0.5151,
      "step": 1212
    },
    {
      "epoch": 2.426,
      "grad_norm": 0.9777543544769287,
      "learning_rate": 0.0001515406162464986,
      "loss": 0.398,
      "step": 1213
    },
    {
      "epoch": 2.428,
      "grad_norm": 1.1272428035736084,
      "learning_rate": 0.00015150060024009604,
      "loss": 0.4532,
      "step": 1214
    },
    {
      "epoch": 2.43,
      "grad_norm": 1.2272073030471802,
      "learning_rate": 0.0001514605842336935,
      "loss": 0.6761,
      "step": 1215
    },
    {
      "epoch": 2.432,
      "grad_norm": 1.2383506298065186,
      "learning_rate": 0.00015142056822729091,
      "loss": 0.5015,
      "step": 1216
    },
    {
      "epoch": 2.434,
      "grad_norm": 1.09273362159729,
      "learning_rate": 0.00015138055222088835,
      "loss": 0.3583,
      "step": 1217
    },
    {
      "epoch": 2.436,
      "grad_norm": 1.1340798139572144,
      "learning_rate": 0.00015134053621448582,
      "loss": 0.5035,
      "step": 1218
    },
    {
      "epoch": 2.438,
      "grad_norm": 1.173793911933899,
      "learning_rate": 0.00015130052020808323,
      "loss": 0.4629,
      "step": 1219
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.2255210876464844,
      "learning_rate": 0.00015126050420168066,
      "loss": 0.3673,
      "step": 1220
    },
    {
      "epoch": 2.442,
      "grad_norm": 1.1190412044525146,
      "learning_rate": 0.00015122048819527813,
      "loss": 0.4617,
      "step": 1221
    },
    {
      "epoch": 2.444,
      "grad_norm": 0.9862866401672363,
      "learning_rate": 0.00015118047218887556,
      "loss": 0.4253,
      "step": 1222
    },
    {
      "epoch": 2.446,
      "grad_norm": 0.8311227560043335,
      "learning_rate": 0.000151140456182473,
      "loss": 0.401,
      "step": 1223
    },
    {
      "epoch": 2.448,
      "grad_norm": 1.0294747352600098,
      "learning_rate": 0.00015110044017607044,
      "loss": 0.5471,
      "step": 1224
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.937172532081604,
      "learning_rate": 0.00015106042416966788,
      "loss": 0.35,
      "step": 1225
    },
    {
      "epoch": 2.452,
      "grad_norm": 0.9065256714820862,
      "learning_rate": 0.0001510204081632653,
      "loss": 0.4821,
      "step": 1226
    },
    {
      "epoch": 2.454,
      "grad_norm": 1.299333930015564,
      "learning_rate": 0.00015098039215686275,
      "loss": 0.358,
      "step": 1227
    },
    {
      "epoch": 2.456,
      "grad_norm": 1.0068414211273193,
      "learning_rate": 0.0001509403761504602,
      "loss": 0.4461,
      "step": 1228
    },
    {
      "epoch": 2.458,
      "grad_norm": 1.261433720588684,
      "learning_rate": 0.00015090036014405765,
      "loss": 0.4947,
      "step": 1229
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.9662298560142517,
      "learning_rate": 0.00015086034413765506,
      "loss": 0.4357,
      "step": 1230
    },
    {
      "epoch": 2.462,
      "grad_norm": 1.1018686294555664,
      "learning_rate": 0.0001508203281312525,
      "loss": 0.3504,
      "step": 1231
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.9380393624305725,
      "learning_rate": 0.00015078031212484996,
      "loss": 0.3614,
      "step": 1232
    },
    {
      "epoch": 2.466,
      "grad_norm": 1.1741362810134888,
      "learning_rate": 0.00015074029611844737,
      "loss": 0.4466,
      "step": 1233
    },
    {
      "epoch": 2.468,
      "grad_norm": 1.0630775690078735,
      "learning_rate": 0.0001507002801120448,
      "loss": 0.4265,
      "step": 1234
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 0.8878245949745178,
      "learning_rate": 0.00015066026410564227,
      "loss": 0.4346,
      "step": 1235
    },
    {
      "epoch": 2.472,
      "grad_norm": 1.568405032157898,
      "learning_rate": 0.0001506202480992397,
      "loss": 0.5937,
      "step": 1236
    },
    {
      "epoch": 2.474,
      "grad_norm": 1.0781961679458618,
      "learning_rate": 0.00015058023209283715,
      "loss": 0.4369,
      "step": 1237
    },
    {
      "epoch": 2.476,
      "grad_norm": 1.1532464027404785,
      "learning_rate": 0.00015054021608643459,
      "loss": 0.4587,
      "step": 1238
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 1.0696659088134766,
      "learning_rate": 0.00015050020008003202,
      "loss": 0.492,
      "step": 1239
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.9490552544593811,
      "learning_rate": 0.00015046018407362946,
      "loss": 0.4233,
      "step": 1240
    },
    {
      "epoch": 2.482,
      "grad_norm": 1.079837441444397,
      "learning_rate": 0.0001504201680672269,
      "loss": 0.6038,
      "step": 1241
    },
    {
      "epoch": 2.484,
      "grad_norm": 1.0264641046524048,
      "learning_rate": 0.00015038015206082433,
      "loss": 0.5958,
      "step": 1242
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 1.2186554670333862,
      "learning_rate": 0.00015034013605442177,
      "loss": 0.5826,
      "step": 1243
    },
    {
      "epoch": 2.488,
      "grad_norm": 1.0505841970443726,
      "learning_rate": 0.0001503001200480192,
      "loss": 0.4044,
      "step": 1244
    },
    {
      "epoch": 2.49,
      "grad_norm": 1.2094560861587524,
      "learning_rate": 0.00015026010404161665,
      "loss": 0.5281,
      "step": 1245
    },
    {
      "epoch": 2.492,
      "grad_norm": 1.1053961515426636,
      "learning_rate": 0.0001502200880352141,
      "loss": 0.4766,
      "step": 1246
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 1.30107843875885,
      "learning_rate": 0.00015018007202881152,
      "loss": 0.481,
      "step": 1247
    },
    {
      "epoch": 2.496,
      "grad_norm": 1.157131552696228,
      "learning_rate": 0.00015014005602240896,
      "loss": 0.4604,
      "step": 1248
    },
    {
      "epoch": 2.498,
      "grad_norm": 1.4823572635650635,
      "learning_rate": 0.00015010004001600642,
      "loss": 0.3824,
      "step": 1249
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.0064496994018555,
      "learning_rate": 0.00015006002400960383,
      "loss": 0.3649,
      "step": 1250
    },
    {
      "epoch": 2.502,
      "grad_norm": 1.0287686586380005,
      "learning_rate": 0.0001500200080032013,
      "loss": 0.464,
      "step": 1251
    },
    {
      "epoch": 2.504,
      "grad_norm": 1.0992741584777832,
      "learning_rate": 0.00014997999199679873,
      "loss": 0.4428,
      "step": 1252
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 0.9756238460540771,
      "learning_rate": 0.00014993997599039617,
      "loss": 0.2992,
      "step": 1253
    },
    {
      "epoch": 2.508,
      "grad_norm": 1.2741096019744873,
      "learning_rate": 0.0001498999599839936,
      "loss": 0.4621,
      "step": 1254
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.3550493717193604,
      "learning_rate": 0.00014985994397759104,
      "loss": 0.5461,
      "step": 1255
    },
    {
      "epoch": 2.512,
      "grad_norm": 1.4013586044311523,
      "learning_rate": 0.00014981992797118848,
      "loss": 0.4597,
      "step": 1256
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 1.3899989128112793,
      "learning_rate": 0.00014977991196478592,
      "loss": 0.48,
      "step": 1257
    },
    {
      "epoch": 2.516,
      "grad_norm": 1.3994356393814087,
      "learning_rate": 0.00014973989595838336,
      "loss": 0.616,
      "step": 1258
    },
    {
      "epoch": 2.518,
      "grad_norm": 0.9764342308044434,
      "learning_rate": 0.0001496998799519808,
      "loss": 0.4137,
      "step": 1259
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.833457589149475,
      "learning_rate": 0.00014965986394557826,
      "loss": 0.5244,
      "step": 1260
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 1.6795976161956787,
      "learning_rate": 0.00014961984793917567,
      "loss": 0.6434,
      "step": 1261
    },
    {
      "epoch": 2.524,
      "grad_norm": 1.172473430633545,
      "learning_rate": 0.00014957983193277313,
      "loss": 0.4852,
      "step": 1262
    },
    {
      "epoch": 2.526,
      "grad_norm": 1.3138364553451538,
      "learning_rate": 0.00014953981592637057,
      "loss": 0.5195,
      "step": 1263
    },
    {
      "epoch": 2.528,
      "grad_norm": 1.3028336763381958,
      "learning_rate": 0.00014949979991996798,
      "loss": 0.7187,
      "step": 1264
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 1.2529077529907227,
      "learning_rate": 0.00014945978391356544,
      "loss": 0.473,
      "step": 1265
    },
    {
      "epoch": 2.532,
      "grad_norm": 1.547134280204773,
      "learning_rate": 0.00014941976790716288,
      "loss": 0.4077,
      "step": 1266
    },
    {
      "epoch": 2.534,
      "grad_norm": 1.2582199573516846,
      "learning_rate": 0.0001493797519007603,
      "loss": 0.4653,
      "step": 1267
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.9906517267227173,
      "learning_rate": 0.00014933973589435775,
      "loss": 0.4861,
      "step": 1268
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 1.016222357749939,
      "learning_rate": 0.0001492997198879552,
      "loss": 0.4769,
      "step": 1269
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.3057870864868164,
      "learning_rate": 0.00014925970388155263,
      "loss": 0.4996,
      "step": 1270
    },
    {
      "epoch": 2.542,
      "grad_norm": 1.1786044836044312,
      "learning_rate": 0.00014921968787515007,
      "loss": 0.6225,
      "step": 1271
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.9987095594406128,
      "learning_rate": 0.0001491796718687475,
      "loss": 0.5613,
      "step": 1272
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 1.0634018182754517,
      "learning_rate": 0.00014913965586234494,
      "loss": 0.4203,
      "step": 1273
    },
    {
      "epoch": 2.548,
      "grad_norm": 1.2737960815429688,
      "learning_rate": 0.00014909963985594238,
      "loss": 0.6803,
      "step": 1274
    },
    {
      "epoch": 2.55,
      "grad_norm": 1.156888723373413,
      "learning_rate": 0.00014905962384953981,
      "loss": 0.5204,
      "step": 1275
    },
    {
      "epoch": 2.552,
      "grad_norm": 1.0512735843658447,
      "learning_rate": 0.00014901960784313728,
      "loss": 0.4529,
      "step": 1276
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 1.1322511434555054,
      "learning_rate": 0.00014897959183673472,
      "loss": 0.6412,
      "step": 1277
    },
    {
      "epoch": 2.556,
      "grad_norm": 1.0300668478012085,
      "learning_rate": 0.00014893957583033213,
      "loss": 0.4782,
      "step": 1278
    },
    {
      "epoch": 2.558,
      "grad_norm": 1.1718496084213257,
      "learning_rate": 0.0001488995598239296,
      "loss": 0.4488,
      "step": 1279
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.3186925649642944,
      "learning_rate": 0.00014885954381752703,
      "loss": 0.5222,
      "step": 1280
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 1.0280346870422363,
      "learning_rate": 0.00014881952781112444,
      "loss": 0.4283,
      "step": 1281
    },
    {
      "epoch": 2.564,
      "grad_norm": 1.258817434310913,
      "learning_rate": 0.0001487795118047219,
      "loss": 0.5098,
      "step": 1282
    },
    {
      "epoch": 2.566,
      "grad_norm": 1.1600277423858643,
      "learning_rate": 0.00014873949579831934,
      "loss": 0.4623,
      "step": 1283
    },
    {
      "epoch": 2.568,
      "grad_norm": 1.0047930479049683,
      "learning_rate": 0.00014869947979191678,
      "loss": 0.3928,
      "step": 1284
    },
    {
      "epoch": 2.57,
      "grad_norm": 1.0027689933776855,
      "learning_rate": 0.0001486594637855142,
      "loss": 0.3265,
      "step": 1285
    },
    {
      "epoch": 2.572,
      "grad_norm": 0.9684340953826904,
      "learning_rate": 0.00014861944777911165,
      "loss": 0.3718,
      "step": 1286
    },
    {
      "epoch": 2.574,
      "grad_norm": 1.085881233215332,
      "learning_rate": 0.0001485794317727091,
      "loss": 0.4904,
      "step": 1287
    },
    {
      "epoch": 2.576,
      "grad_norm": 1.3796696662902832,
      "learning_rate": 0.00014853941576630652,
      "loss": 0.4339,
      "step": 1288
    },
    {
      "epoch": 2.578,
      "grad_norm": 1.106764316558838,
      "learning_rate": 0.00014849939975990396,
      "loss": 0.5045,
      "step": 1289
    },
    {
      "epoch": 2.58,
      "grad_norm": 1.1094098091125488,
      "learning_rate": 0.00014845938375350142,
      "loss": 0.5005,
      "step": 1290
    },
    {
      "epoch": 2.582,
      "grad_norm": 1.1962579488754272,
      "learning_rate": 0.00014841936774709884,
      "loss": 0.5923,
      "step": 1291
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.947700560092926,
      "learning_rate": 0.00014837935174069627,
      "loss": 0.3765,
      "step": 1292
    },
    {
      "epoch": 2.586,
      "grad_norm": 1.0659687519073486,
      "learning_rate": 0.00014833933573429374,
      "loss": 0.4504,
      "step": 1293
    },
    {
      "epoch": 2.588,
      "grad_norm": 1.4432755708694458,
      "learning_rate": 0.00014829931972789117,
      "loss": 0.6387,
      "step": 1294
    },
    {
      "epoch": 2.59,
      "grad_norm": 1.3611633777618408,
      "learning_rate": 0.00014825930372148858,
      "loss": 0.7669,
      "step": 1295
    },
    {
      "epoch": 2.592,
      "grad_norm": 1.106913685798645,
      "learning_rate": 0.00014821928771508605,
      "loss": 0.4208,
      "step": 1296
    },
    {
      "epoch": 2.594,
      "grad_norm": 1.1770318746566772,
      "learning_rate": 0.00014817927170868348,
      "loss": 0.4362,
      "step": 1297
    },
    {
      "epoch": 2.596,
      "grad_norm": 1.1185061931610107,
      "learning_rate": 0.00014813925570228092,
      "loss": 0.3365,
      "step": 1298
    },
    {
      "epoch": 2.598,
      "grad_norm": 1.3346384763717651,
      "learning_rate": 0.00014809923969587836,
      "loss": 0.4649,
      "step": 1299
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.3327182531356812,
      "learning_rate": 0.0001480592236894758,
      "loss": 0.4868,
      "step": 1300
    },
    {
      "epoch": 2.602,
      "grad_norm": 1.15047025680542,
      "learning_rate": 0.00014801920768307323,
      "loss": 0.4138,
      "step": 1301
    },
    {
      "epoch": 2.604,
      "grad_norm": 1.121382236480713,
      "learning_rate": 0.00014797919167667067,
      "loss": 0.6988,
      "step": 1302
    },
    {
      "epoch": 2.606,
      "grad_norm": 1.0574442148208618,
      "learning_rate": 0.0001479391756702681,
      "loss": 0.3791,
      "step": 1303
    },
    {
      "epoch": 2.608,
      "grad_norm": 1.133632779121399,
      "learning_rate": 0.00014789915966386557,
      "loss": 0.5191,
      "step": 1304
    },
    {
      "epoch": 2.61,
      "grad_norm": 2.1324875354766846,
      "learning_rate": 0.00014785914365746298,
      "loss": 0.6298,
      "step": 1305
    },
    {
      "epoch": 2.612,
      "grad_norm": 1.1035840511322021,
      "learning_rate": 0.00014781912765106042,
      "loss": 0.3636,
      "step": 1306
    },
    {
      "epoch": 2.614,
      "grad_norm": 0.9243736863136292,
      "learning_rate": 0.00014777911164465788,
      "loss": 0.4242,
      "step": 1307
    },
    {
      "epoch": 2.616,
      "grad_norm": 1.3096166849136353,
      "learning_rate": 0.0001477390956382553,
      "loss": 0.4776,
      "step": 1308
    },
    {
      "epoch": 2.618,
      "grad_norm": 1.0121023654937744,
      "learning_rate": 0.00014769907963185273,
      "loss": 0.4724,
      "step": 1309
    },
    {
      "epoch": 2.62,
      "grad_norm": 1.5546728372573853,
      "learning_rate": 0.0001476590636254502,
      "loss": 0.6548,
      "step": 1310
    },
    {
      "epoch": 2.622,
      "grad_norm": 1.1516622304916382,
      "learning_rate": 0.00014761904761904763,
      "loss": 0.5114,
      "step": 1311
    },
    {
      "epoch": 2.624,
      "grad_norm": 1.139492154121399,
      "learning_rate": 0.00014757903161264507,
      "loss": 0.4009,
      "step": 1312
    },
    {
      "epoch": 2.626,
      "grad_norm": 0.9748110771179199,
      "learning_rate": 0.0001475390156062425,
      "loss": 0.4178,
      "step": 1313
    },
    {
      "epoch": 2.628,
      "grad_norm": 1.2678217887878418,
      "learning_rate": 0.00014749899959983994,
      "loss": 0.5531,
      "step": 1314
    },
    {
      "epoch": 2.63,
      "grad_norm": 1.1653441190719604,
      "learning_rate": 0.00014745898359343738,
      "loss": 0.4818,
      "step": 1315
    },
    {
      "epoch": 2.632,
      "grad_norm": 1.1588494777679443,
      "learning_rate": 0.00014741896758703482,
      "loss": 0.4197,
      "step": 1316
    },
    {
      "epoch": 2.634,
      "grad_norm": 1.610925555229187,
      "learning_rate": 0.00014737895158063225,
      "loss": 0.4643,
      "step": 1317
    },
    {
      "epoch": 2.636,
      "grad_norm": 0.9341973662376404,
      "learning_rate": 0.00014733893557422972,
      "loss": 0.4278,
      "step": 1318
    },
    {
      "epoch": 2.638,
      "grad_norm": 1.0660971403121948,
      "learning_rate": 0.00014729891956782713,
      "loss": 0.353,
      "step": 1319
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.0317617654800415,
      "learning_rate": 0.00014725890356142457,
      "loss": 0.3391,
      "step": 1320
    },
    {
      "epoch": 2.642,
      "grad_norm": 1.2924442291259766,
      "learning_rate": 0.00014721888755502203,
      "loss": 0.5054,
      "step": 1321
    },
    {
      "epoch": 2.644,
      "grad_norm": 1.3543593883514404,
      "learning_rate": 0.00014717887154861944,
      "loss": 0.5514,
      "step": 1322
    },
    {
      "epoch": 2.646,
      "grad_norm": 1.1762745380401611,
      "learning_rate": 0.0001471388555422169,
      "loss": 0.4175,
      "step": 1323
    },
    {
      "epoch": 2.648,
      "grad_norm": 1.2058998346328735,
      "learning_rate": 0.00014709883953581434,
      "loss": 0.5137,
      "step": 1324
    },
    {
      "epoch": 2.65,
      "grad_norm": 1.4466228485107422,
      "learning_rate": 0.00014705882352941178,
      "loss": 0.5887,
      "step": 1325
    },
    {
      "epoch": 2.652,
      "grad_norm": 1.1542595624923706,
      "learning_rate": 0.00014701880752300922,
      "loss": 0.4115,
      "step": 1326
    },
    {
      "epoch": 2.654,
      "grad_norm": 1.3759218454360962,
      "learning_rate": 0.00014697879151660665,
      "loss": 0.5961,
      "step": 1327
    },
    {
      "epoch": 2.656,
      "grad_norm": 1.9965778589248657,
      "learning_rate": 0.0001469387755102041,
      "loss": 0.4719,
      "step": 1328
    },
    {
      "epoch": 2.658,
      "grad_norm": 1.3687291145324707,
      "learning_rate": 0.00014689875950380153,
      "loss": 0.535,
      "step": 1329
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.2543405294418335,
      "learning_rate": 0.00014685874349739896,
      "loss": 0.3692,
      "step": 1330
    },
    {
      "epoch": 2.662,
      "grad_norm": 1.4166746139526367,
      "learning_rate": 0.0001468187274909964,
      "loss": 0.491,
      "step": 1331
    },
    {
      "epoch": 2.664,
      "grad_norm": 1.3498603105545044,
      "learning_rate": 0.00014677871148459384,
      "loss": 0.5121,
      "step": 1332
    },
    {
      "epoch": 2.666,
      "grad_norm": 1.3395496606826782,
      "learning_rate": 0.00014673869547819128,
      "loss": 0.574,
      "step": 1333
    },
    {
      "epoch": 2.668,
      "grad_norm": 1.0983949899673462,
      "learning_rate": 0.0001466986794717887,
      "loss": 0.3945,
      "step": 1334
    },
    {
      "epoch": 2.67,
      "grad_norm": 1.0887858867645264,
      "learning_rate": 0.00014665866346538618,
      "loss": 0.5277,
      "step": 1335
    },
    {
      "epoch": 2.672,
      "grad_norm": 1.063891887664795,
      "learning_rate": 0.0001466186474589836,
      "loss": 0.4878,
      "step": 1336
    },
    {
      "epoch": 2.674,
      "grad_norm": 1.1456249952316284,
      "learning_rate": 0.00014657863145258105,
      "loss": 0.4781,
      "step": 1337
    },
    {
      "epoch": 2.676,
      "grad_norm": 0.925160825252533,
      "learning_rate": 0.0001465386154461785,
      "loss": 0.3795,
      "step": 1338
    },
    {
      "epoch": 2.678,
      "grad_norm": 1.3172281980514526,
      "learning_rate": 0.0001464985994397759,
      "loss": 0.5046,
      "step": 1339
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.2444440126419067,
      "learning_rate": 0.00014645858343337336,
      "loss": 0.4172,
      "step": 1340
    },
    {
      "epoch": 2.682,
      "grad_norm": 0.9869884848594666,
      "learning_rate": 0.0001464185674269708,
      "loss": 0.3764,
      "step": 1341
    },
    {
      "epoch": 2.684,
      "grad_norm": 1.1955909729003906,
      "learning_rate": 0.00014637855142056824,
      "loss": 0.5131,
      "step": 1342
    },
    {
      "epoch": 2.686,
      "grad_norm": 1.151916742324829,
      "learning_rate": 0.00014633853541416567,
      "loss": 0.4924,
      "step": 1343
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 1.0694947242736816,
      "learning_rate": 0.0001462985194077631,
      "loss": 0.4499,
      "step": 1344
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.9463847279548645,
      "learning_rate": 0.00014625850340136055,
      "loss": 0.3155,
      "step": 1345
    },
    {
      "epoch": 2.692,
      "grad_norm": 1.3566991090774536,
      "learning_rate": 0.00014621848739495799,
      "loss": 0.4673,
      "step": 1346
    },
    {
      "epoch": 2.694,
      "grad_norm": 1.1709346771240234,
      "learning_rate": 0.00014617847138855542,
      "loss": 0.3766,
      "step": 1347
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 1.0284141302108765,
      "learning_rate": 0.00014613845538215286,
      "loss": 0.4357,
      "step": 1348
    },
    {
      "epoch": 2.698,
      "grad_norm": 0.8736395239830017,
      "learning_rate": 0.00014609843937575032,
      "loss": 0.4381,
      "step": 1349
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.0387773513793945,
      "learning_rate": 0.00014605842336934773,
      "loss": 0.4433,
      "step": 1350
    },
    {
      "epoch": 2.702,
      "grad_norm": 1.0595438480377197,
      "learning_rate": 0.0001460184073629452,
      "loss": 0.4115,
      "step": 1351
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.9150556325912476,
      "learning_rate": 0.00014597839135654264,
      "loss": 0.3256,
      "step": 1352
    },
    {
      "epoch": 2.706,
      "grad_norm": 1.4479752779006958,
      "learning_rate": 0.00014593837535014005,
      "loss": 0.5958,
      "step": 1353
    },
    {
      "epoch": 2.708,
      "grad_norm": 1.2162432670593262,
      "learning_rate": 0.0001458983593437375,
      "loss": 0.5131,
      "step": 1354
    },
    {
      "epoch": 2.71,
      "grad_norm": 1.2443968057632446,
      "learning_rate": 0.00014585834333733495,
      "loss": 0.4333,
      "step": 1355
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 1.167783498764038,
      "learning_rate": 0.00014581832733093236,
      "loss": 0.4179,
      "step": 1356
    },
    {
      "epoch": 2.714,
      "grad_norm": 1.3542295694351196,
      "learning_rate": 0.00014577831132452982,
      "loss": 0.588,
      "step": 1357
    },
    {
      "epoch": 2.716,
      "grad_norm": 1.1376770734786987,
      "learning_rate": 0.00014573829531812726,
      "loss": 0.4594,
      "step": 1358
    },
    {
      "epoch": 2.718,
      "grad_norm": 1.0960502624511719,
      "learning_rate": 0.0001456982793117247,
      "loss": 0.4707,
      "step": 1359
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.2944129705429077,
      "learning_rate": 0.00014565826330532213,
      "loss": 0.6193,
      "step": 1360
    },
    {
      "epoch": 2.722,
      "grad_norm": 0.9391114115715027,
      "learning_rate": 0.00014561824729891957,
      "loss": 0.383,
      "step": 1361
    },
    {
      "epoch": 2.724,
      "grad_norm": 1.2994492053985596,
      "learning_rate": 0.000145578231292517,
      "loss": 0.5576,
      "step": 1362
    },
    {
      "epoch": 2.726,
      "grad_norm": 1.4486074447631836,
      "learning_rate": 0.00014553821528611444,
      "loss": 0.5251,
      "step": 1363
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 1.1850112676620483,
      "learning_rate": 0.00014549819927971188,
      "loss": 0.5414,
      "step": 1364
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.2568281888961792,
      "learning_rate": 0.00014545818327330935,
      "loss": 0.3302,
      "step": 1365
    },
    {
      "epoch": 2.732,
      "grad_norm": 1.6643493175506592,
      "learning_rate": 0.00014541816726690678,
      "loss": 0.5376,
      "step": 1366
    },
    {
      "epoch": 2.734,
      "grad_norm": 1.2975423336029053,
      "learning_rate": 0.0001453781512605042,
      "loss": 0.5907,
      "step": 1367
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 1.4037600755691528,
      "learning_rate": 0.00014533813525410166,
      "loss": 0.6416,
      "step": 1368
    },
    {
      "epoch": 2.738,
      "grad_norm": 0.8996481895446777,
      "learning_rate": 0.0001452981192476991,
      "loss": 0.4708,
      "step": 1369
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.936427652835846,
      "learning_rate": 0.0001452581032412965,
      "loss": 0.3857,
      "step": 1370
    },
    {
      "epoch": 2.742,
      "grad_norm": 1.2176170349121094,
      "learning_rate": 0.00014521808723489397,
      "loss": 0.5506,
      "step": 1371
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 1.403644323348999,
      "learning_rate": 0.0001451780712284914,
      "loss": 0.6118,
      "step": 1372
    },
    {
      "epoch": 2.746,
      "grad_norm": 1.0074597597122192,
      "learning_rate": 0.00014513805522208884,
      "loss": 0.4001,
      "step": 1373
    },
    {
      "epoch": 2.748,
      "grad_norm": 0.8947466611862183,
      "learning_rate": 0.00014509803921568628,
      "loss": 0.4056,
      "step": 1374
    },
    {
      "epoch": 2.75,
      "grad_norm": 1.1525540351867676,
      "learning_rate": 0.00014505802320928372,
      "loss": 0.4646,
      "step": 1375
    },
    {
      "epoch": 2.752,
      "grad_norm": 1.4762920141220093,
      "learning_rate": 0.00014501800720288115,
      "loss": 0.6673,
      "step": 1376
    },
    {
      "epoch": 2.754,
      "grad_norm": 1.2206008434295654,
      "learning_rate": 0.0001449779911964786,
      "loss": 0.5298,
      "step": 1377
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 1.1090818643569946,
      "learning_rate": 0.00014493797519007603,
      "loss": 0.4909,
      "step": 1378
    },
    {
      "epoch": 2.758,
      "grad_norm": 0.9816340208053589,
      "learning_rate": 0.0001448979591836735,
      "loss": 0.5264,
      "step": 1379
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.1949541568756104,
      "learning_rate": 0.0001448579431772709,
      "loss": 0.8386,
      "step": 1380
    },
    {
      "epoch": 2.762,
      "grad_norm": 1.1621270179748535,
      "learning_rate": 0.00014481792717086834,
      "loss": 0.4901,
      "step": 1381
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 1.4628006219863892,
      "learning_rate": 0.0001447779111644658,
      "loss": 0.6638,
      "step": 1382
    },
    {
      "epoch": 2.766,
      "grad_norm": 1.1052898168563843,
      "learning_rate": 0.00014473789515806324,
      "loss": 0.345,
      "step": 1383
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.995895504951477,
      "learning_rate": 0.00014469787915166065,
      "loss": 0.4323,
      "step": 1384
    },
    {
      "epoch": 2.77,
      "grad_norm": 1.296722412109375,
      "learning_rate": 0.00014465786314525812,
      "loss": 0.4612,
      "step": 1385
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 1.3613110780715942,
      "learning_rate": 0.00014461784713885555,
      "loss": 0.5974,
      "step": 1386
    },
    {
      "epoch": 2.774,
      "grad_norm": 1.223502516746521,
      "learning_rate": 0.000144577831132453,
      "loss": 0.5893,
      "step": 1387
    },
    {
      "epoch": 2.776,
      "grad_norm": 1.0734961032867432,
      "learning_rate": 0.00014453781512605043,
      "loss": 0.4521,
      "step": 1388
    },
    {
      "epoch": 2.778,
      "grad_norm": 1.0159531831741333,
      "learning_rate": 0.00014449779911964786,
      "loss": 0.351,
      "step": 1389
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 1.1330556869506836,
      "learning_rate": 0.00014445778311324533,
      "loss": 0.4639,
      "step": 1390
    },
    {
      "epoch": 2.782,
      "grad_norm": 1.4942141771316528,
      "learning_rate": 0.00014441776710684274,
      "loss": 0.5882,
      "step": 1391
    },
    {
      "epoch": 2.784,
      "grad_norm": 1.3765950202941895,
      "learning_rate": 0.00014437775110044018,
      "loss": 0.5575,
      "step": 1392
    },
    {
      "epoch": 2.786,
      "grad_norm": 1.2136112451553345,
      "learning_rate": 0.00014433773509403764,
      "loss": 0.3863,
      "step": 1393
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 1.149264931678772,
      "learning_rate": 0.00014429771908763505,
      "loss": 0.3954,
      "step": 1394
    },
    {
      "epoch": 2.79,
      "grad_norm": 1.0333178043365479,
      "learning_rate": 0.0001442577030812325,
      "loss": 0.4377,
      "step": 1395
    },
    {
      "epoch": 2.792,
      "grad_norm": 1.6677662134170532,
      "learning_rate": 0.00014421768707482995,
      "loss": 0.5933,
      "step": 1396
    },
    {
      "epoch": 2.794,
      "grad_norm": 1.0634437799453735,
      "learning_rate": 0.00014417767106842736,
      "loss": 0.4791,
      "step": 1397
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 0.983200192451477,
      "learning_rate": 0.00014413765506202483,
      "loss": 0.401,
      "step": 1398
    },
    {
      "epoch": 2.798,
      "grad_norm": 1.3608086109161377,
      "learning_rate": 0.00014409763905562226,
      "loss": 0.5061,
      "step": 1399
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.926633894443512,
      "learning_rate": 0.0001440576230492197,
      "loss": 0.4286,
      "step": 1400
    },
    {
      "epoch": 2.802,
      "grad_norm": 1.1819509267807007,
      "learning_rate": 0.00014401760704281714,
      "loss": 0.4936,
      "step": 1401
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 0.9599224328994751,
      "learning_rate": 0.00014397759103641457,
      "loss": 0.389,
      "step": 1402
    },
    {
      "epoch": 2.806,
      "grad_norm": 1.0143768787384033,
      "learning_rate": 0.000143937575030012,
      "loss": 0.4292,
      "step": 1403
    },
    {
      "epoch": 2.808,
      "grad_norm": 1.1438192129135132,
      "learning_rate": 0.00014389755902360945,
      "loss": 0.6069,
      "step": 1404
    },
    {
      "epoch": 2.81,
      "grad_norm": 1.547584891319275,
      "learning_rate": 0.00014385754301720689,
      "loss": 0.4606,
      "step": 1405
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 1.0267254114151,
      "learning_rate": 0.00014381752701080432,
      "loss": 0.3709,
      "step": 1406
    },
    {
      "epoch": 2.814,
      "grad_norm": 1.4683972597122192,
      "learning_rate": 0.0001437775110044018,
      "loss": 0.4094,
      "step": 1407
    },
    {
      "epoch": 2.816,
      "grad_norm": 1.587227702140808,
      "learning_rate": 0.0001437374949979992,
      "loss": 0.6343,
      "step": 1408
    },
    {
      "epoch": 2.818,
      "grad_norm": 1.4788429737091064,
      "learning_rate": 0.00014369747899159663,
      "loss": 0.4776,
      "step": 1409
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.317529320716858,
      "learning_rate": 0.0001436574629851941,
      "loss": 0.5523,
      "step": 1410
    },
    {
      "epoch": 2.822,
      "grad_norm": 1.3122352361679077,
      "learning_rate": 0.0001436174469787915,
      "loss": 0.6791,
      "step": 1411
    },
    {
      "epoch": 2.824,
      "grad_norm": 1.0645217895507812,
      "learning_rate": 0.00014357743097238897,
      "loss": 0.4039,
      "step": 1412
    },
    {
      "epoch": 2.826,
      "grad_norm": 0.9648024439811707,
      "learning_rate": 0.0001435374149659864,
      "loss": 0.517,
      "step": 1413
    },
    {
      "epoch": 2.828,
      "grad_norm": 1.0528302192687988,
      "learning_rate": 0.00014349739895958385,
      "loss": 0.4576,
      "step": 1414
    },
    {
      "epoch": 2.83,
      "grad_norm": 1.1960246562957764,
      "learning_rate": 0.00014345738295318128,
      "loss": 0.7226,
      "step": 1415
    },
    {
      "epoch": 2.832,
      "grad_norm": 1.2920918464660645,
      "learning_rate": 0.00014341736694677872,
      "loss": 0.4797,
      "step": 1416
    },
    {
      "epoch": 2.834,
      "grad_norm": 1.2404779195785522,
      "learning_rate": 0.00014337735094037616,
      "loss": 0.5482,
      "step": 1417
    },
    {
      "epoch": 2.836,
      "grad_norm": 1.2413915395736694,
      "learning_rate": 0.0001433373349339736,
      "loss": 0.4441,
      "step": 1418
    },
    {
      "epoch": 2.838,
      "grad_norm": 1.2924096584320068,
      "learning_rate": 0.00014329731892757103,
      "loss": 0.4229,
      "step": 1419
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.349206566810608,
      "learning_rate": 0.00014325730292116847,
      "loss": 0.5981,
      "step": 1420
    },
    {
      "epoch": 2.842,
      "grad_norm": 1.22091805934906,
      "learning_rate": 0.0001432172869147659,
      "loss": 0.5095,
      "step": 1421
    },
    {
      "epoch": 2.844,
      "grad_norm": 1.165602445602417,
      "learning_rate": 0.00014317727090836334,
      "loss": 0.5107,
      "step": 1422
    },
    {
      "epoch": 2.846,
      "grad_norm": 1.1111438274383545,
      "learning_rate": 0.00014313725490196078,
      "loss": 0.4679,
      "step": 1423
    },
    {
      "epoch": 2.848,
      "grad_norm": 1.413882851600647,
      "learning_rate": 0.00014309723889555825,
      "loss": 0.8298,
      "step": 1424
    },
    {
      "epoch": 2.85,
      "grad_norm": 1.1839059591293335,
      "learning_rate": 0.00014305722288915566,
      "loss": 0.5484,
      "step": 1425
    },
    {
      "epoch": 2.852,
      "grad_norm": 1.275542974472046,
      "learning_rate": 0.00014301720688275312,
      "loss": 0.5293,
      "step": 1426
    },
    {
      "epoch": 2.854,
      "grad_norm": 1.0878429412841797,
      "learning_rate": 0.00014297719087635056,
      "loss": 0.4546,
      "step": 1427
    },
    {
      "epoch": 2.856,
      "grad_norm": 1.055721402168274,
      "learning_rate": 0.00014293717486994797,
      "loss": 0.5342,
      "step": 1428
    },
    {
      "epoch": 2.858,
      "grad_norm": 1.1338192224502563,
      "learning_rate": 0.00014289715886354543,
      "loss": 0.4202,
      "step": 1429
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.436581015586853,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.5537,
      "step": 1430
    },
    {
      "epoch": 2.862,
      "grad_norm": 1.1592046022415161,
      "learning_rate": 0.0001428171268507403,
      "loss": 0.3357,
      "step": 1431
    },
    {
      "epoch": 2.864,
      "grad_norm": 1.2708450555801392,
      "learning_rate": 0.00014277711084433774,
      "loss": 0.5454,
      "step": 1432
    },
    {
      "epoch": 2.866,
      "grad_norm": 1.131203293800354,
      "learning_rate": 0.00014273709483793518,
      "loss": 0.4135,
      "step": 1433
    },
    {
      "epoch": 2.868,
      "grad_norm": 1.3982818126678467,
      "learning_rate": 0.00014269707883153262,
      "loss": 0.4966,
      "step": 1434
    },
    {
      "epoch": 2.87,
      "grad_norm": 1.2471319437026978,
      "learning_rate": 0.00014265706282513005,
      "loss": 0.5255,
      "step": 1435
    },
    {
      "epoch": 2.872,
      "grad_norm": 1.5629862546920776,
      "learning_rate": 0.0001426170468187275,
      "loss": 0.643,
      "step": 1436
    },
    {
      "epoch": 2.874,
      "grad_norm": 1.0928577184677124,
      "learning_rate": 0.00014257703081232493,
      "loss": 0.4518,
      "step": 1437
    },
    {
      "epoch": 2.876,
      "grad_norm": 1.0518991947174072,
      "learning_rate": 0.0001425370148059224,
      "loss": 0.4459,
      "step": 1438
    },
    {
      "epoch": 2.878,
      "grad_norm": 0.969375491142273,
      "learning_rate": 0.0001424969987995198,
      "loss": 0.3004,
      "step": 1439
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.008374810218811,
      "learning_rate": 0.00014245698279311727,
      "loss": 0.3532,
      "step": 1440
    },
    {
      "epoch": 2.882,
      "grad_norm": 1.2958847284317017,
      "learning_rate": 0.0001424169667867147,
      "loss": 0.3751,
      "step": 1441
    },
    {
      "epoch": 2.884,
      "grad_norm": 1.386086106300354,
      "learning_rate": 0.0001423769507803121,
      "loss": 0.6286,
      "step": 1442
    },
    {
      "epoch": 2.886,
      "grad_norm": 1.124407410621643,
      "learning_rate": 0.00014233693477390958,
      "loss": 0.3138,
      "step": 1443
    },
    {
      "epoch": 2.888,
      "grad_norm": 2.2345242500305176,
      "learning_rate": 0.00014229691876750701,
      "loss": 0.7389,
      "step": 1444
    },
    {
      "epoch": 2.89,
      "grad_norm": 1.0555126667022705,
      "learning_rate": 0.00014225690276110442,
      "loss": 0.3586,
      "step": 1445
    },
    {
      "epoch": 2.892,
      "grad_norm": 1.1153857707977295,
      "learning_rate": 0.0001422168867547019,
      "loss": 0.3265,
      "step": 1446
    },
    {
      "epoch": 2.894,
      "grad_norm": 1.1750129461288452,
      "learning_rate": 0.00014217687074829933,
      "loss": 0.5448,
      "step": 1447
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.9274923801422119,
      "learning_rate": 0.00014213685474189676,
      "loss": 0.4293,
      "step": 1448
    },
    {
      "epoch": 2.898,
      "grad_norm": 1.4017648696899414,
      "learning_rate": 0.0001420968387354942,
      "loss": 0.6559,
      "step": 1449
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.4468761682510376,
      "learning_rate": 0.00014205682272909164,
      "loss": 0.4839,
      "step": 1450
    },
    {
      "epoch": 2.902,
      "grad_norm": 1.2026721239089966,
      "learning_rate": 0.0001420168067226891,
      "loss": 0.5191,
      "step": 1451
    },
    {
      "epoch": 2.904,
      "grad_norm": 1.4794743061065674,
      "learning_rate": 0.0001419767907162865,
      "loss": 0.4272,
      "step": 1452
    },
    {
      "epoch": 2.906,
      "grad_norm": 1.0305821895599365,
      "learning_rate": 0.00014193677470988395,
      "loss": 0.3368,
      "step": 1453
    },
    {
      "epoch": 2.908,
      "grad_norm": 1.0707321166992188,
      "learning_rate": 0.0001418967587034814,
      "loss": 0.445,
      "step": 1454
    },
    {
      "epoch": 2.91,
      "grad_norm": 1.2101436853408813,
      "learning_rate": 0.00014185674269707885,
      "loss": 0.4024,
      "step": 1455
    },
    {
      "epoch": 2.912,
      "grad_norm": 1.62497079372406,
      "learning_rate": 0.00014181672669067626,
      "loss": 0.5228,
      "step": 1456
    },
    {
      "epoch": 2.914,
      "grad_norm": 1.069913625717163,
      "learning_rate": 0.00014177671068427372,
      "loss": 0.4716,
      "step": 1457
    },
    {
      "epoch": 2.916,
      "grad_norm": 1.6610257625579834,
      "learning_rate": 0.00014173669467787116,
      "loss": 0.5008,
      "step": 1458
    },
    {
      "epoch": 2.918,
      "grad_norm": 0.8545729517936707,
      "learning_rate": 0.0001416966786714686,
      "loss": 0.3375,
      "step": 1459
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.9530057311058044,
      "learning_rate": 0.00014165666266506604,
      "loss": 0.4237,
      "step": 1460
    },
    {
      "epoch": 2.922,
      "grad_norm": 1.147986650466919,
      "learning_rate": 0.00014161664665866347,
      "loss": 0.5641,
      "step": 1461
    },
    {
      "epoch": 2.924,
      "grad_norm": 1.359420657157898,
      "learning_rate": 0.0001415766306522609,
      "loss": 0.4859,
      "step": 1462
    },
    {
      "epoch": 2.926,
      "grad_norm": 1.3641239404678345,
      "learning_rate": 0.00014153661464585835,
      "loss": 0.5163,
      "step": 1463
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.9876838326454163,
      "learning_rate": 0.00014149659863945578,
      "loss": 0.35,
      "step": 1464
    },
    {
      "epoch": 2.93,
      "grad_norm": 1.2076472043991089,
      "learning_rate": 0.00014145658263305325,
      "loss": 0.4463,
      "step": 1465
    },
    {
      "epoch": 2.932,
      "grad_norm": 1.0342440605163574,
      "learning_rate": 0.00014141656662665066,
      "loss": 0.3923,
      "step": 1466
    },
    {
      "epoch": 2.934,
      "grad_norm": 1.0862298011779785,
      "learning_rate": 0.0001413765506202481,
      "loss": 0.3526,
      "step": 1467
    },
    {
      "epoch": 2.936,
      "grad_norm": 1.5053558349609375,
      "learning_rate": 0.00014133653461384556,
      "loss": 0.5035,
      "step": 1468
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 1.2164510488510132,
      "learning_rate": 0.00014129651860744297,
      "loss": 0.4769,
      "step": 1469
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.0373947620391846,
      "learning_rate": 0.0001412565026010404,
      "loss": 0.4144,
      "step": 1470
    },
    {
      "epoch": 2.942,
      "grad_norm": 1.2083719968795776,
      "learning_rate": 0.00014121648659463787,
      "loss": 0.52,
      "step": 1471
    },
    {
      "epoch": 2.944,
      "grad_norm": 1.3022863864898682,
      "learning_rate": 0.0001411764705882353,
      "loss": 0.5122,
      "step": 1472
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 1.1333695650100708,
      "learning_rate": 0.00014113645458183275,
      "loss": 0.458,
      "step": 1473
    },
    {
      "epoch": 2.948,
      "grad_norm": 1.8456854820251465,
      "learning_rate": 0.00014109643857543018,
      "loss": 0.5754,
      "step": 1474
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.9785756468772888,
      "learning_rate": 0.00014105642256902762,
      "loss": 0.3788,
      "step": 1475
    },
    {
      "epoch": 2.952,
      "grad_norm": 1.1077452898025513,
      "learning_rate": 0.00014101640656262506,
      "loss": 0.5708,
      "step": 1476
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 1.2557092905044556,
      "learning_rate": 0.0001409763905562225,
      "loss": 0.476,
      "step": 1477
    },
    {
      "epoch": 2.956,
      "grad_norm": 1.447188138961792,
      "learning_rate": 0.00014093637454981993,
      "loss": 0.5145,
      "step": 1478
    },
    {
      "epoch": 2.958,
      "grad_norm": 1.2375727891921997,
      "learning_rate": 0.0001408963585434174,
      "loss": 0.5135,
      "step": 1479
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.215585708618164,
      "learning_rate": 0.0001408563425370148,
      "loss": 0.3516,
      "step": 1480
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 1.3184670209884644,
      "learning_rate": 0.00014081632653061224,
      "loss": 0.3866,
      "step": 1481
    },
    {
      "epoch": 2.964,
      "grad_norm": 1.174908995628357,
      "learning_rate": 0.0001407763105242097,
      "loss": 0.3046,
      "step": 1482
    },
    {
      "epoch": 2.966,
      "grad_norm": 1.323540210723877,
      "learning_rate": 0.00014073629451780712,
      "loss": 0.525,
      "step": 1483
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.9521952867507935,
      "learning_rate": 0.00014069627851140455,
      "loss": 0.3994,
      "step": 1484
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 1.682967185974121,
      "learning_rate": 0.00014065626250500202,
      "loss": 0.5325,
      "step": 1485
    },
    {
      "epoch": 2.972,
      "grad_norm": 1.5318894386291504,
      "learning_rate": 0.00014061624649859943,
      "loss": 0.4817,
      "step": 1486
    },
    {
      "epoch": 2.974,
      "grad_norm": 1.2833715677261353,
      "learning_rate": 0.0001405762304921969,
      "loss": 0.4725,
      "step": 1487
    },
    {
      "epoch": 2.976,
      "grad_norm": 1.1234009265899658,
      "learning_rate": 0.00014053621448579433,
      "loss": 0.4546,
      "step": 1488
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 1.1979527473449707,
      "learning_rate": 0.00014049619847939177,
      "loss": 0.4473,
      "step": 1489
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.9417118430137634,
      "learning_rate": 0.0001404561824729892,
      "loss": 0.3588,
      "step": 1490
    },
    {
      "epoch": 2.982,
      "grad_norm": 1.0324208736419678,
      "learning_rate": 0.00014041616646658664,
      "loss": 0.5665,
      "step": 1491
    },
    {
      "epoch": 2.984,
      "grad_norm": 1.2214487791061401,
      "learning_rate": 0.00014037615046018408,
      "loss": 0.3869,
      "step": 1492
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 1.2757420539855957,
      "learning_rate": 0.00014033613445378152,
      "loss": 0.5579,
      "step": 1493
    },
    {
      "epoch": 2.988,
      "grad_norm": 1.1327375173568726,
      "learning_rate": 0.00014029611844737895,
      "loss": 0.539,
      "step": 1494
    },
    {
      "epoch": 2.99,
      "grad_norm": 1.144386887550354,
      "learning_rate": 0.0001402561024409764,
      "loss": 0.5408,
      "step": 1495
    },
    {
      "epoch": 2.992,
      "grad_norm": 1.1346626281738281,
      "learning_rate": 0.00014021608643457385,
      "loss": 0.4865,
      "step": 1496
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 1.169288992881775,
      "learning_rate": 0.00014017607042817126,
      "loss": 0.4577,
      "step": 1497
    },
    {
      "epoch": 2.996,
      "grad_norm": 1.2504682540893555,
      "learning_rate": 0.0001401360544217687,
      "loss": 0.5576,
      "step": 1498
    },
    {
      "epoch": 2.998,
      "grad_norm": 1.4691354036331177,
      "learning_rate": 0.00014009603841536617,
      "loss": 0.5293,
      "step": 1499
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.1352837085723877,
      "learning_rate": 0.00014005602240896358,
      "loss": 0.4968,
      "step": 1500
    },
    {
      "epoch": 3.002,
      "grad_norm": 0.9837898015975952,
      "learning_rate": 0.00014001600640256104,
      "loss": 0.5378,
      "step": 1501
    },
    {
      "epoch": 3.004,
      "grad_norm": 0.8693671226501465,
      "learning_rate": 0.00013997599039615848,
      "loss": 0.3028,
      "step": 1502
    },
    {
      "epoch": 3.006,
      "grad_norm": 0.914856493473053,
      "learning_rate": 0.00013993597438975591,
      "loss": 0.3673,
      "step": 1503
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.8055444955825806,
      "learning_rate": 0.00013989595838335335,
      "loss": 0.2611,
      "step": 1504
    },
    {
      "epoch": 3.01,
      "grad_norm": 1.0330718755722046,
      "learning_rate": 0.0001398559423769508,
      "loss": 0.3904,
      "step": 1505
    },
    {
      "epoch": 3.012,
      "grad_norm": 0.7004712820053101,
      "learning_rate": 0.00013981592637054823,
      "loss": 0.2612,
      "step": 1506
    },
    {
      "epoch": 3.014,
      "grad_norm": 1.598130464553833,
      "learning_rate": 0.00013977591036414566,
      "loss": 0.334,
      "step": 1507
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.891148567199707,
      "learning_rate": 0.0001397358943577431,
      "loss": 0.3195,
      "step": 1508
    },
    {
      "epoch": 3.018,
      "grad_norm": 1.1413390636444092,
      "learning_rate": 0.00013969587835134054,
      "loss": 0.3695,
      "step": 1509
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.9815623760223389,
      "learning_rate": 0.00013965586234493797,
      "loss": 0.3013,
      "step": 1510
    },
    {
      "epoch": 3.022,
      "grad_norm": 0.8551973104476929,
      "learning_rate": 0.0001396158463385354,
      "loss": 0.3593,
      "step": 1511
    },
    {
      "epoch": 3.024,
      "grad_norm": 1.2160617113113403,
      "learning_rate": 0.00013957583033213285,
      "loss": 0.3291,
      "step": 1512
    },
    {
      "epoch": 3.026,
      "grad_norm": 1.2265080213546753,
      "learning_rate": 0.0001395358143257303,
      "loss": 0.3559,
      "step": 1513
    },
    {
      "epoch": 3.028,
      "grad_norm": 1.2051728963851929,
      "learning_rate": 0.00013949579831932772,
      "loss": 0.4164,
      "step": 1514
    },
    {
      "epoch": 3.03,
      "grad_norm": 1.259799838066101,
      "learning_rate": 0.0001394557823129252,
      "loss": 0.306,
      "step": 1515
    },
    {
      "epoch": 3.032,
      "grad_norm": 1.98952054977417,
      "learning_rate": 0.00013941576630652262,
      "loss": 0.4227,
      "step": 1516
    },
    {
      "epoch": 3.034,
      "grad_norm": 1.4723126888275146,
      "learning_rate": 0.00013937575030012003,
      "loss": 0.3237,
      "step": 1517
    },
    {
      "epoch": 3.036,
      "grad_norm": 1.545746922492981,
      "learning_rate": 0.0001393357342937175,
      "loss": 0.3502,
      "step": 1518
    },
    {
      "epoch": 3.038,
      "grad_norm": 1.7454229593276978,
      "learning_rate": 0.00013929571828731494,
      "loss": 0.3746,
      "step": 1519
    },
    {
      "epoch": 3.04,
      "grad_norm": 2.1251914501190186,
      "learning_rate": 0.00013925570228091237,
      "loss": 0.3618,
      "step": 1520
    },
    {
      "epoch": 3.042,
      "grad_norm": 1.443361520767212,
      "learning_rate": 0.0001392156862745098,
      "loss": 0.2484,
      "step": 1521
    },
    {
      "epoch": 3.044,
      "grad_norm": 1.6593377590179443,
      "learning_rate": 0.00013917567026810725,
      "loss": 0.4024,
      "step": 1522
    },
    {
      "epoch": 3.046,
      "grad_norm": 1.4409102201461792,
      "learning_rate": 0.00013913565426170468,
      "loss": 0.3218,
      "step": 1523
    },
    {
      "epoch": 3.048,
      "grad_norm": 1.7516223192214966,
      "learning_rate": 0.00013909563825530212,
      "loss": 0.4582,
      "step": 1524
    },
    {
      "epoch": 3.05,
      "grad_norm": 1.4623217582702637,
      "learning_rate": 0.00013905562224889956,
      "loss": 0.3728,
      "step": 1525
    },
    {
      "epoch": 3.052,
      "grad_norm": 1.3337639570236206,
      "learning_rate": 0.00013901560624249702,
      "loss": 0.3279,
      "step": 1526
    },
    {
      "epoch": 3.054,
      "grad_norm": 1.5102105140686035,
      "learning_rate": 0.00013897559023609446,
      "loss": 0.3221,
      "step": 1527
    },
    {
      "epoch": 3.056,
      "grad_norm": 1.3995167016983032,
      "learning_rate": 0.00013893557422969187,
      "loss": 0.3679,
      "step": 1528
    },
    {
      "epoch": 3.058,
      "grad_norm": 1.1445285081863403,
      "learning_rate": 0.00013889555822328933,
      "loss": 0.2927,
      "step": 1529
    },
    {
      "epoch": 3.06,
      "grad_norm": 1.1233985424041748,
      "learning_rate": 0.00013885554221688677,
      "loss": 0.2787,
      "step": 1530
    },
    {
      "epoch": 3.062,
      "grad_norm": 1.2024168968200684,
      "learning_rate": 0.00013881552621048418,
      "loss": 0.266,
      "step": 1531
    },
    {
      "epoch": 3.064,
      "grad_norm": 1.0247105360031128,
      "learning_rate": 0.00013877551020408165,
      "loss": 0.3157,
      "step": 1532
    },
    {
      "epoch": 3.066,
      "grad_norm": 1.4218003749847412,
      "learning_rate": 0.00013873549419767908,
      "loss": 0.3025,
      "step": 1533
    },
    {
      "epoch": 3.068,
      "grad_norm": 1.3419650793075562,
      "learning_rate": 0.00013869547819127652,
      "loss": 0.3763,
      "step": 1534
    },
    {
      "epoch": 3.07,
      "grad_norm": 1.0944838523864746,
      "learning_rate": 0.00013865546218487396,
      "loss": 0.2433,
      "step": 1535
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.7407094836235046,
      "learning_rate": 0.0001386154461784714,
      "loss": 0.2195,
      "step": 1536
    },
    {
      "epoch": 3.074,
      "grad_norm": 1.5207148790359497,
      "learning_rate": 0.00013857543017206883,
      "loss": 0.3559,
      "step": 1537
    },
    {
      "epoch": 3.076,
      "grad_norm": 1.0664743185043335,
      "learning_rate": 0.00013853541416566627,
      "loss": 0.2176,
      "step": 1538
    },
    {
      "epoch": 3.078,
      "grad_norm": 1.6401571035385132,
      "learning_rate": 0.0001384953981592637,
      "loss": 0.3616,
      "step": 1539
    },
    {
      "epoch": 3.08,
      "grad_norm": 1.596439242362976,
      "learning_rate": 0.00013845538215286117,
      "loss": 0.3872,
      "step": 1540
    },
    {
      "epoch": 3.082,
      "grad_norm": 1.6654057502746582,
      "learning_rate": 0.00013841536614645858,
      "loss": 0.4195,
      "step": 1541
    },
    {
      "epoch": 3.084,
      "grad_norm": 1.3469351530075073,
      "learning_rate": 0.00013837535014005602,
      "loss": 0.2661,
      "step": 1542
    },
    {
      "epoch": 3.086,
      "grad_norm": 1.6382375955581665,
      "learning_rate": 0.00013833533413365348,
      "loss": 0.3089,
      "step": 1543
    },
    {
      "epoch": 3.088,
      "grad_norm": 1.3852323293685913,
      "learning_rate": 0.00013829531812725092,
      "loss": 0.2985,
      "step": 1544
    },
    {
      "epoch": 3.09,
      "grad_norm": 1.1150012016296387,
      "learning_rate": 0.00013825530212084833,
      "loss": 0.2635,
      "step": 1545
    },
    {
      "epoch": 3.092,
      "grad_norm": 1.6560674905776978,
      "learning_rate": 0.0001382152861144458,
      "loss": 0.2729,
      "step": 1546
    },
    {
      "epoch": 3.094,
      "grad_norm": 1.9143182039260864,
      "learning_rate": 0.00013817527010804323,
      "loss": 0.328,
      "step": 1547
    },
    {
      "epoch": 3.096,
      "grad_norm": 1.125479817390442,
      "learning_rate": 0.00013813525410164067,
      "loss": 0.2569,
      "step": 1548
    },
    {
      "epoch": 3.098,
      "grad_norm": 1.1959940195083618,
      "learning_rate": 0.0001380952380952381,
      "loss": 0.3281,
      "step": 1549
    },
    {
      "epoch": 3.1,
      "grad_norm": 1.4121553897857666,
      "learning_rate": 0.00013805522208883554,
      "loss": 0.337,
      "step": 1550
    },
    {
      "epoch": 3.102,
      "grad_norm": 1.439408302307129,
      "learning_rate": 0.00013801520608243298,
      "loss": 0.3564,
      "step": 1551
    },
    {
      "epoch": 3.104,
      "grad_norm": 1.582249641418457,
      "learning_rate": 0.00013797519007603042,
      "loss": 0.3334,
      "step": 1552
    },
    {
      "epoch": 3.106,
      "grad_norm": 2.2577059268951416,
      "learning_rate": 0.00013793517406962785,
      "loss": 0.3926,
      "step": 1553
    },
    {
      "epoch": 3.108,
      "grad_norm": 1.500162959098816,
      "learning_rate": 0.00013789515806322532,
      "loss": 0.4924,
      "step": 1554
    },
    {
      "epoch": 3.11,
      "grad_norm": 1.6341036558151245,
      "learning_rate": 0.00013785514205682273,
      "loss": 0.3865,
      "step": 1555
    },
    {
      "epoch": 3.112,
      "grad_norm": 1.501268982887268,
      "learning_rate": 0.00013781512605042016,
      "loss": 0.4493,
      "step": 1556
    },
    {
      "epoch": 3.114,
      "grad_norm": 1.808112621307373,
      "learning_rate": 0.00013777511004401763,
      "loss": 0.4181,
      "step": 1557
    },
    {
      "epoch": 3.116,
      "grad_norm": 1.3088291883468628,
      "learning_rate": 0.00013773509403761504,
      "loss": 0.2945,
      "step": 1558
    },
    {
      "epoch": 3.118,
      "grad_norm": 1.4046109914779663,
      "learning_rate": 0.00013769507803121248,
      "loss": 0.3912,
      "step": 1559
    },
    {
      "epoch": 3.12,
      "grad_norm": 1.5376088619232178,
      "learning_rate": 0.00013765506202480994,
      "loss": 0.4285,
      "step": 1560
    },
    {
      "epoch": 3.122,
      "grad_norm": 1.4290204048156738,
      "learning_rate": 0.00013761504601840738,
      "loss": 0.3017,
      "step": 1561
    },
    {
      "epoch": 3.124,
      "grad_norm": 1.331870198249817,
      "learning_rate": 0.00013757503001200481,
      "loss": 0.4304,
      "step": 1562
    },
    {
      "epoch": 3.126,
      "grad_norm": 1.2298519611358643,
      "learning_rate": 0.00013753501400560225,
      "loss": 0.3174,
      "step": 1563
    },
    {
      "epoch": 3.128,
      "grad_norm": 1.2055004835128784,
      "learning_rate": 0.0001374949979991997,
      "loss": 0.3219,
      "step": 1564
    },
    {
      "epoch": 3.13,
      "grad_norm": 1.5294429063796997,
      "learning_rate": 0.00013745498199279713,
      "loss": 0.3883,
      "step": 1565
    },
    {
      "epoch": 3.132,
      "grad_norm": 1.0850425958633423,
      "learning_rate": 0.00013741496598639456,
      "loss": 0.33,
      "step": 1566
    },
    {
      "epoch": 3.134,
      "grad_norm": 1.3401838541030884,
      "learning_rate": 0.000137374949979992,
      "loss": 0.441,
      "step": 1567
    },
    {
      "epoch": 3.136,
      "grad_norm": 1.012557029724121,
      "learning_rate": 0.00013733493397358946,
      "loss": 0.2723,
      "step": 1568
    },
    {
      "epoch": 3.138,
      "grad_norm": 1.5775421857833862,
      "learning_rate": 0.00013729491796718687,
      "loss": 0.3766,
      "step": 1569
    },
    {
      "epoch": 3.14,
      "grad_norm": 1.9846397638320923,
      "learning_rate": 0.0001372549019607843,
      "loss": 0.425,
      "step": 1570
    },
    {
      "epoch": 3.142,
      "grad_norm": 1.2094855308532715,
      "learning_rate": 0.00013721488595438177,
      "loss": 0.3061,
      "step": 1571
    },
    {
      "epoch": 3.144,
      "grad_norm": 1.3442435264587402,
      "learning_rate": 0.00013717486994797919,
      "loss": 0.3613,
      "step": 1572
    },
    {
      "epoch": 3.146,
      "grad_norm": 1.3877164125442505,
      "learning_rate": 0.00013713485394157662,
      "loss": 0.3532,
      "step": 1573
    },
    {
      "epoch": 3.148,
      "grad_norm": 1.4467449188232422,
      "learning_rate": 0.00013709483793517409,
      "loss": 0.3404,
      "step": 1574
    },
    {
      "epoch": 3.15,
      "grad_norm": 1.1415711641311646,
      "learning_rate": 0.00013705482192877152,
      "loss": 0.2832,
      "step": 1575
    },
    {
      "epoch": 3.152,
      "grad_norm": 1.6464319229125977,
      "learning_rate": 0.00013701480592236896,
      "loss": 0.3112,
      "step": 1576
    },
    {
      "epoch": 3.154,
      "grad_norm": 1.229085922241211,
      "learning_rate": 0.0001369747899159664,
      "loss": 0.2888,
      "step": 1577
    },
    {
      "epoch": 3.156,
      "grad_norm": 1.1151292324066162,
      "learning_rate": 0.00013693477390956383,
      "loss": 0.2282,
      "step": 1578
    },
    {
      "epoch": 3.158,
      "grad_norm": 1.3680987358093262,
      "learning_rate": 0.00013689475790316127,
      "loss": 0.3618,
      "step": 1579
    },
    {
      "epoch": 3.16,
      "grad_norm": 1.5414520502090454,
      "learning_rate": 0.0001368547418967587,
      "loss": 0.3487,
      "step": 1580
    },
    {
      "epoch": 3.162,
      "grad_norm": 1.640637755393982,
      "learning_rate": 0.00013681472589035615,
      "loss": 0.3923,
      "step": 1581
    },
    {
      "epoch": 3.164,
      "grad_norm": 1.6630351543426514,
      "learning_rate": 0.00013677470988395358,
      "loss": 0.4325,
      "step": 1582
    },
    {
      "epoch": 3.166,
      "grad_norm": 1.628748893737793,
      "learning_rate": 0.00013673469387755102,
      "loss": 0.4077,
      "step": 1583
    },
    {
      "epoch": 3.168,
      "grad_norm": 1.479952096939087,
      "learning_rate": 0.00013669467787114846,
      "loss": 0.3562,
      "step": 1584
    },
    {
      "epoch": 3.17,
      "grad_norm": 1.0023019313812256,
      "learning_rate": 0.00013665466186474592,
      "loss": 0.28,
      "step": 1585
    },
    {
      "epoch": 3.172,
      "grad_norm": 1.2976824045181274,
      "learning_rate": 0.00013661464585834333,
      "loss": 0.3731,
      "step": 1586
    },
    {
      "epoch": 3.174,
      "grad_norm": 1.0917731523513794,
      "learning_rate": 0.0001365746298519408,
      "loss": 0.3257,
      "step": 1587
    },
    {
      "epoch": 3.176,
      "grad_norm": 1.6193839311599731,
      "learning_rate": 0.00013653461384553823,
      "loss": 0.4544,
      "step": 1588
    },
    {
      "epoch": 3.178,
      "grad_norm": 1.2721774578094482,
      "learning_rate": 0.00013649459783913564,
      "loss": 0.3642,
      "step": 1589
    },
    {
      "epoch": 3.18,
      "grad_norm": 1.0736128091812134,
      "learning_rate": 0.0001364545818327331,
      "loss": 0.271,
      "step": 1590
    },
    {
      "epoch": 3.182,
      "grad_norm": 1.0687975883483887,
      "learning_rate": 0.00013641456582633054,
      "loss": 0.31,
      "step": 1591
    },
    {
      "epoch": 3.184,
      "grad_norm": 1.3193411827087402,
      "learning_rate": 0.00013637454981992798,
      "loss": 0.3616,
      "step": 1592
    },
    {
      "epoch": 3.186,
      "grad_norm": 1.2883434295654297,
      "learning_rate": 0.00013633453381352542,
      "loss": 0.3578,
      "step": 1593
    },
    {
      "epoch": 3.188,
      "grad_norm": 1.5772899389266968,
      "learning_rate": 0.00013629451780712286,
      "loss": 0.3016,
      "step": 1594
    },
    {
      "epoch": 3.19,
      "grad_norm": 1.583703637123108,
      "learning_rate": 0.0001362545018007203,
      "loss": 0.4566,
      "step": 1595
    },
    {
      "epoch": 3.192,
      "grad_norm": 1.2726151943206787,
      "learning_rate": 0.00013621448579431773,
      "loss": 0.3525,
      "step": 1596
    },
    {
      "epoch": 3.194,
      "grad_norm": 1.286300539970398,
      "learning_rate": 0.00013617446978791517,
      "loss": 0.2805,
      "step": 1597
    },
    {
      "epoch": 3.196,
      "grad_norm": 1.1954525709152222,
      "learning_rate": 0.0001361344537815126,
      "loss": 0.294,
      "step": 1598
    },
    {
      "epoch": 3.198,
      "grad_norm": 1.3344063758850098,
      "learning_rate": 0.00013609443777511004,
      "loss": 0.3365,
      "step": 1599
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.6259417533874512,
      "learning_rate": 0.00013605442176870748,
      "loss": 0.3306,
      "step": 1600
    },
    {
      "epoch": 3.202,
      "grad_norm": 1.2528059482574463,
      "learning_rate": 0.00013601440576230494,
      "loss": 0.3021,
      "step": 1601
    },
    {
      "epoch": 3.204,
      "grad_norm": 1.3302754163742065,
      "learning_rate": 0.00013597438975590238,
      "loss": 0.2866,
      "step": 1602
    },
    {
      "epoch": 3.206,
      "grad_norm": 1.5388695001602173,
      "learning_rate": 0.0001359343737494998,
      "loss": 0.3751,
      "step": 1603
    },
    {
      "epoch": 3.208,
      "grad_norm": 1.3522950410842896,
      "learning_rate": 0.00013589435774309725,
      "loss": 0.3631,
      "step": 1604
    },
    {
      "epoch": 3.21,
      "grad_norm": 1.5431275367736816,
      "learning_rate": 0.0001358543417366947,
      "loss": 0.3387,
      "step": 1605
    },
    {
      "epoch": 3.212,
      "grad_norm": 1.8452973365783691,
      "learning_rate": 0.0001358143257302921,
      "loss": 0.4786,
      "step": 1606
    },
    {
      "epoch": 3.214,
      "grad_norm": 1.4087615013122559,
      "learning_rate": 0.00013577430972388957,
      "loss": 0.2855,
      "step": 1607
    },
    {
      "epoch": 3.216,
      "grad_norm": 1.2058504819869995,
      "learning_rate": 0.000135734293717487,
      "loss": 0.2938,
      "step": 1608
    },
    {
      "epoch": 3.218,
      "grad_norm": 1.458244800567627,
      "learning_rate": 0.00013569427771108444,
      "loss": 0.3594,
      "step": 1609
    },
    {
      "epoch": 3.22,
      "grad_norm": 1.332070231437683,
      "learning_rate": 0.00013565426170468188,
      "loss": 0.3121,
      "step": 1610
    },
    {
      "epoch": 3.222,
      "grad_norm": 1.2430237531661987,
      "learning_rate": 0.00013561424569827931,
      "loss": 0.3436,
      "step": 1611
    },
    {
      "epoch": 3.224,
      "grad_norm": 1.5473644733428955,
      "learning_rate": 0.00013557422969187675,
      "loss": 0.421,
      "step": 1612
    },
    {
      "epoch": 3.226,
      "grad_norm": 1.470682978630066,
      "learning_rate": 0.0001355342136854742,
      "loss": 0.3235,
      "step": 1613
    },
    {
      "epoch": 3.228,
      "grad_norm": 1.6386789083480835,
      "learning_rate": 0.00013549419767907163,
      "loss": 0.4973,
      "step": 1614
    },
    {
      "epoch": 3.23,
      "grad_norm": 1.3633434772491455,
      "learning_rate": 0.0001354541816726691,
      "loss": 0.3629,
      "step": 1615
    },
    {
      "epoch": 3.232,
      "grad_norm": 1.415592074394226,
      "learning_rate": 0.00013541416566626653,
      "loss": 0.4087,
      "step": 1616
    },
    {
      "epoch": 3.234,
      "grad_norm": 1.1127067804336548,
      "learning_rate": 0.00013537414965986394,
      "loss": 0.2608,
      "step": 1617
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 1.3104157447814941,
      "learning_rate": 0.0001353341336534614,
      "loss": 0.3337,
      "step": 1618
    },
    {
      "epoch": 3.238,
      "grad_norm": 1.2479854822158813,
      "learning_rate": 0.00013529411764705884,
      "loss": 0.3248,
      "step": 1619
    },
    {
      "epoch": 3.24,
      "grad_norm": 1.4545671939849854,
      "learning_rate": 0.00013525410164065625,
      "loss": 0.3304,
      "step": 1620
    },
    {
      "epoch": 3.242,
      "grad_norm": 1.4112465381622314,
      "learning_rate": 0.0001352140856342537,
      "loss": 0.3282,
      "step": 1621
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 1.7021571397781372,
      "learning_rate": 0.00013517406962785115,
      "loss": 0.4726,
      "step": 1622
    },
    {
      "epoch": 3.246,
      "grad_norm": 1.3238803148269653,
      "learning_rate": 0.0001351340536214486,
      "loss": 0.3834,
      "step": 1623
    },
    {
      "epoch": 3.248,
      "grad_norm": 1.2020483016967773,
      "learning_rate": 0.00013509403761504602,
      "loss": 0.2967,
      "step": 1624
    },
    {
      "epoch": 3.25,
      "grad_norm": 1.3832305669784546,
      "learning_rate": 0.00013505402160864346,
      "loss": 0.3934,
      "step": 1625
    },
    {
      "epoch": 3.252,
      "grad_norm": 1.2467124462127686,
      "learning_rate": 0.0001350140056022409,
      "loss": 0.3339,
      "step": 1626
    },
    {
      "epoch": 3.254,
      "grad_norm": 1.5849648714065552,
      "learning_rate": 0.00013497398959583834,
      "loss": 0.4441,
      "step": 1627
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 1.2853740453720093,
      "learning_rate": 0.00013493397358943577,
      "loss": 0.3049,
      "step": 1628
    },
    {
      "epoch": 3.258,
      "grad_norm": 1.3138508796691895,
      "learning_rate": 0.00013489395758303324,
      "loss": 0.3218,
      "step": 1629
    },
    {
      "epoch": 3.26,
      "grad_norm": 1.7603541612625122,
      "learning_rate": 0.00013485394157663065,
      "loss": 0.4336,
      "step": 1630
    },
    {
      "epoch": 3.262,
      "grad_norm": 2.0694756507873535,
      "learning_rate": 0.00013481392557022808,
      "loss": 0.3131,
      "step": 1631
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 1.651574730873108,
      "learning_rate": 0.00013477390956382555,
      "loss": 0.3881,
      "step": 1632
    },
    {
      "epoch": 3.266,
      "grad_norm": 1.6562899351119995,
      "learning_rate": 0.00013473389355742299,
      "loss": 0.2989,
      "step": 1633
    },
    {
      "epoch": 3.268,
      "grad_norm": 1.4795820713043213,
      "learning_rate": 0.0001346938775510204,
      "loss": 0.3389,
      "step": 1634
    },
    {
      "epoch": 3.27,
      "grad_norm": 1.2290632724761963,
      "learning_rate": 0.00013465386154461786,
      "loss": 0.2473,
      "step": 1635
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 1.3476841449737549,
      "learning_rate": 0.0001346138455382153,
      "loss": 0.2696,
      "step": 1636
    },
    {
      "epoch": 3.274,
      "grad_norm": 1.3641270399093628,
      "learning_rate": 0.00013457382953181273,
      "loss": 0.3229,
      "step": 1637
    },
    {
      "epoch": 3.276,
      "grad_norm": 1.3031351566314697,
      "learning_rate": 0.00013453381352541017,
      "loss": 0.372,
      "step": 1638
    },
    {
      "epoch": 3.278,
      "grad_norm": 1.4758429527282715,
      "learning_rate": 0.0001344937975190076,
      "loss": 0.3355,
      "step": 1639
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 1.8857195377349854,
      "learning_rate": 0.00013445378151260507,
      "loss": 0.3966,
      "step": 1640
    },
    {
      "epoch": 3.282,
      "grad_norm": 1.3325589895248413,
      "learning_rate": 0.00013441376550620248,
      "loss": 0.3427,
      "step": 1641
    },
    {
      "epoch": 3.284,
      "grad_norm": 1.6363118886947632,
      "learning_rate": 0.00013437374949979992,
      "loss": 0.3209,
      "step": 1642
    },
    {
      "epoch": 3.286,
      "grad_norm": 1.0921825170516968,
      "learning_rate": 0.00013433373349339738,
      "loss": 0.2935,
      "step": 1643
    },
    {
      "epoch": 3.288,
      "grad_norm": 1.0841642618179321,
      "learning_rate": 0.0001342937174869948,
      "loss": 0.276,
      "step": 1644
    },
    {
      "epoch": 3.29,
      "grad_norm": 1.5250550508499146,
      "learning_rate": 0.00013425370148059223,
      "loss": 0.4222,
      "step": 1645
    },
    {
      "epoch": 3.292,
      "grad_norm": 1.4689282178878784,
      "learning_rate": 0.0001342136854741897,
      "loss": 0.3741,
      "step": 1646
    },
    {
      "epoch": 3.294,
      "grad_norm": 1.431784987449646,
      "learning_rate": 0.0001341736694677871,
      "loss": 0.4333,
      "step": 1647
    },
    {
      "epoch": 3.296,
      "grad_norm": 1.447986125946045,
      "learning_rate": 0.00013413365346138454,
      "loss": 0.4072,
      "step": 1648
    },
    {
      "epoch": 3.298,
      "grad_norm": 1.081866979598999,
      "learning_rate": 0.000134093637454982,
      "loss": 0.3063,
      "step": 1649
    },
    {
      "epoch": 3.3,
      "grad_norm": 1.3127427101135254,
      "learning_rate": 0.00013405362144857944,
      "loss": 0.3139,
      "step": 1650
    },
    {
      "epoch": 3.302,
      "grad_norm": 1.106582522392273,
      "learning_rate": 0.00013401360544217688,
      "loss": 0.3647,
      "step": 1651
    },
    {
      "epoch": 3.304,
      "grad_norm": 1.330178141593933,
      "learning_rate": 0.00013397358943577432,
      "loss": 0.2968,
      "step": 1652
    },
    {
      "epoch": 3.306,
      "grad_norm": 1.3669118881225586,
      "learning_rate": 0.00013393357342937176,
      "loss": 0.3495,
      "step": 1653
    },
    {
      "epoch": 3.308,
      "grad_norm": 1.2230355739593506,
      "learning_rate": 0.0001338935574229692,
      "loss": 0.3602,
      "step": 1654
    },
    {
      "epoch": 3.31,
      "grad_norm": 1.0420207977294922,
      "learning_rate": 0.00013385354141656663,
      "loss": 0.3119,
      "step": 1655
    },
    {
      "epoch": 3.312,
      "grad_norm": 1.071303367614746,
      "learning_rate": 0.00013381352541016407,
      "loss": 0.3128,
      "step": 1656
    },
    {
      "epoch": 3.314,
      "grad_norm": 1.274357557296753,
      "learning_rate": 0.00013377350940376153,
      "loss": 0.2778,
      "step": 1657
    },
    {
      "epoch": 3.316,
      "grad_norm": 1.3891901969909668,
      "learning_rate": 0.00013373349339735894,
      "loss": 0.3769,
      "step": 1658
    },
    {
      "epoch": 3.318,
      "grad_norm": 1.2631657123565674,
      "learning_rate": 0.00013369347739095638,
      "loss": 0.3051,
      "step": 1659
    },
    {
      "epoch": 3.32,
      "grad_norm": 1.2903871536254883,
      "learning_rate": 0.00013365346138455384,
      "loss": 0.2392,
      "step": 1660
    },
    {
      "epoch": 3.322,
      "grad_norm": 1.445781946182251,
      "learning_rate": 0.00013361344537815125,
      "loss": 0.3173,
      "step": 1661
    },
    {
      "epoch": 3.324,
      "grad_norm": 1.5189824104309082,
      "learning_rate": 0.00013357342937174872,
      "loss": 0.5133,
      "step": 1662
    },
    {
      "epoch": 3.326,
      "grad_norm": 1.5917131900787354,
      "learning_rate": 0.00013353341336534615,
      "loss": 0.3094,
      "step": 1663
    },
    {
      "epoch": 3.328,
      "grad_norm": 1.2281721830368042,
      "learning_rate": 0.0001334933973589436,
      "loss": 0.3886,
      "step": 1664
    },
    {
      "epoch": 3.33,
      "grad_norm": 1.6695717573165894,
      "learning_rate": 0.00013345338135254103,
      "loss": 0.4422,
      "step": 1665
    },
    {
      "epoch": 3.332,
      "grad_norm": 1.4829930067062378,
      "learning_rate": 0.00013341336534613847,
      "loss": 0.3644,
      "step": 1666
    },
    {
      "epoch": 3.334,
      "grad_norm": 0.9438629150390625,
      "learning_rate": 0.0001333733493397359,
      "loss": 0.2796,
      "step": 1667
    },
    {
      "epoch": 3.336,
      "grad_norm": 1.4087393283843994,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.4023,
      "step": 1668
    },
    {
      "epoch": 3.338,
      "grad_norm": 1.3367475271224976,
      "learning_rate": 0.00013329331732693078,
      "loss": 0.3203,
      "step": 1669
    },
    {
      "epoch": 3.34,
      "grad_norm": 1.3473117351531982,
      "learning_rate": 0.00013325330132052821,
      "loss": 0.4032,
      "step": 1670
    },
    {
      "epoch": 3.342,
      "grad_norm": 1.3119380474090576,
      "learning_rate": 0.00013321328531412565,
      "loss": 0.3362,
      "step": 1671
    },
    {
      "epoch": 3.344,
      "grad_norm": 1.4125492572784424,
      "learning_rate": 0.0001331732693077231,
      "loss": 0.299,
      "step": 1672
    },
    {
      "epoch": 3.346,
      "grad_norm": 1.3821543455123901,
      "learning_rate": 0.00013313325330132053,
      "loss": 0.3542,
      "step": 1673
    },
    {
      "epoch": 3.348,
      "grad_norm": 2.135887861251831,
      "learning_rate": 0.000133093237294918,
      "loss": 0.3032,
      "step": 1674
    },
    {
      "epoch": 3.35,
      "grad_norm": 1.3623247146606445,
      "learning_rate": 0.0001330532212885154,
      "loss": 0.3625,
      "step": 1675
    },
    {
      "epoch": 3.352,
      "grad_norm": 1.408437967300415,
      "learning_rate": 0.00013301320528211286,
      "loss": 0.3581,
      "step": 1676
    },
    {
      "epoch": 3.354,
      "grad_norm": 1.6096974611282349,
      "learning_rate": 0.0001329731892757103,
      "loss": 0.4953,
      "step": 1677
    },
    {
      "epoch": 3.356,
      "grad_norm": 1.3557838201522827,
      "learning_rate": 0.0001329331732693077,
      "loss": 0.3093,
      "step": 1678
    },
    {
      "epoch": 3.358,
      "grad_norm": 1.2155965566635132,
      "learning_rate": 0.00013289315726290518,
      "loss": 0.2972,
      "step": 1679
    },
    {
      "epoch": 3.36,
      "grad_norm": 1.1640641689300537,
      "learning_rate": 0.0001328531412565026,
      "loss": 0.2594,
      "step": 1680
    },
    {
      "epoch": 3.362,
      "grad_norm": 1.3662711381912231,
      "learning_rate": 0.00013281312525010005,
      "loss": 0.3702,
      "step": 1681
    },
    {
      "epoch": 3.364,
      "grad_norm": 1.3558553457260132,
      "learning_rate": 0.0001327731092436975,
      "loss": 0.3585,
      "step": 1682
    },
    {
      "epoch": 3.366,
      "grad_norm": 1.3294737339019775,
      "learning_rate": 0.00013273309323729492,
      "loss": 0.3007,
      "step": 1683
    },
    {
      "epoch": 3.368,
      "grad_norm": 1.549931526184082,
      "learning_rate": 0.00013269307723089236,
      "loss": 0.3825,
      "step": 1684
    },
    {
      "epoch": 3.37,
      "grad_norm": 1.2580775022506714,
      "learning_rate": 0.0001326530612244898,
      "loss": 0.2584,
      "step": 1685
    },
    {
      "epoch": 3.372,
      "grad_norm": 1.333414077758789,
      "learning_rate": 0.00013261304521808724,
      "loss": 0.3475,
      "step": 1686
    },
    {
      "epoch": 3.374,
      "grad_norm": 1.1338067054748535,
      "learning_rate": 0.00013257302921168467,
      "loss": 0.2993,
      "step": 1687
    },
    {
      "epoch": 3.376,
      "grad_norm": 2.030491590499878,
      "learning_rate": 0.0001325330132052821,
      "loss": 0.4328,
      "step": 1688
    },
    {
      "epoch": 3.378,
      "grad_norm": 1.371346354484558,
      "learning_rate": 0.00013249299719887955,
      "loss": 0.3018,
      "step": 1689
    },
    {
      "epoch": 3.38,
      "grad_norm": 1.548217535018921,
      "learning_rate": 0.000132452981192477,
      "loss": 0.3836,
      "step": 1690
    },
    {
      "epoch": 3.382,
      "grad_norm": 1.4480020999908447,
      "learning_rate": 0.00013241296518607445,
      "loss": 0.3631,
      "step": 1691
    },
    {
      "epoch": 3.384,
      "grad_norm": 1.5459452867507935,
      "learning_rate": 0.00013237294917967186,
      "loss": 0.2619,
      "step": 1692
    },
    {
      "epoch": 3.386,
      "grad_norm": 1.3509891033172607,
      "learning_rate": 0.00013233293317326932,
      "loss": 0.3812,
      "step": 1693
    },
    {
      "epoch": 3.388,
      "grad_norm": 1.3926554918289185,
      "learning_rate": 0.00013229291716686676,
      "loss": 0.313,
      "step": 1694
    },
    {
      "epoch": 3.39,
      "grad_norm": 1.340761661529541,
      "learning_rate": 0.00013225290116046417,
      "loss": 0.3809,
      "step": 1695
    },
    {
      "epoch": 3.392,
      "grad_norm": 1.5494346618652344,
      "learning_rate": 0.00013221288515406163,
      "loss": 0.3702,
      "step": 1696
    },
    {
      "epoch": 3.394,
      "grad_norm": 1.4412697553634644,
      "learning_rate": 0.00013217286914765907,
      "loss": 0.3613,
      "step": 1697
    },
    {
      "epoch": 3.396,
      "grad_norm": 1.257110834121704,
      "learning_rate": 0.0001321328531412565,
      "loss": 0.3109,
      "step": 1698
    },
    {
      "epoch": 3.398,
      "grad_norm": 1.3033006191253662,
      "learning_rate": 0.00013209283713485395,
      "loss": 0.3517,
      "step": 1699
    },
    {
      "epoch": 3.4,
      "grad_norm": 1.4731707572937012,
      "learning_rate": 0.00013205282112845138,
      "loss": 0.399,
      "step": 1700
    },
    {
      "epoch": 3.402,
      "grad_norm": 1.8088740110397339,
      "learning_rate": 0.00013201280512204882,
      "loss": 0.4539,
      "step": 1701
    },
    {
      "epoch": 3.404,
      "grad_norm": 1.4688129425048828,
      "learning_rate": 0.00013197278911564626,
      "loss": 0.355,
      "step": 1702
    },
    {
      "epoch": 3.406,
      "grad_norm": 1.2462201118469238,
      "learning_rate": 0.0001319327731092437,
      "loss": 0.349,
      "step": 1703
    },
    {
      "epoch": 3.408,
      "grad_norm": 1.3958430290222168,
      "learning_rate": 0.00013189275710284116,
      "loss": 0.4297,
      "step": 1704
    },
    {
      "epoch": 3.41,
      "grad_norm": 1.1600725650787354,
      "learning_rate": 0.0001318527410964386,
      "loss": 0.2889,
      "step": 1705
    },
    {
      "epoch": 3.412,
      "grad_norm": 1.1210354566574097,
      "learning_rate": 0.000131812725090036,
      "loss": 0.3558,
      "step": 1706
    },
    {
      "epoch": 3.414,
      "grad_norm": 1.0621355772018433,
      "learning_rate": 0.00013177270908363347,
      "loss": 0.3407,
      "step": 1707
    },
    {
      "epoch": 3.416,
      "grad_norm": 1.183271050453186,
      "learning_rate": 0.0001317326930772309,
      "loss": 0.3593,
      "step": 1708
    },
    {
      "epoch": 3.418,
      "grad_norm": 1.5127387046813965,
      "learning_rate": 0.00013169267707082832,
      "loss": 0.4227,
      "step": 1709
    },
    {
      "epoch": 3.42,
      "grad_norm": 1.1774731874465942,
      "learning_rate": 0.00013165266106442578,
      "loss": 0.3214,
      "step": 1710
    },
    {
      "epoch": 3.422,
      "grad_norm": 1.0410165786743164,
      "learning_rate": 0.00013161264505802322,
      "loss": 0.2912,
      "step": 1711
    },
    {
      "epoch": 3.424,
      "grad_norm": 1.2132648229599,
      "learning_rate": 0.00013157262905162065,
      "loss": 0.3389,
      "step": 1712
    },
    {
      "epoch": 3.426,
      "grad_norm": 1.1994918584823608,
      "learning_rate": 0.0001315326130452181,
      "loss": 0.3368,
      "step": 1713
    },
    {
      "epoch": 3.428,
      "grad_norm": 1.1032755374908447,
      "learning_rate": 0.00013149259703881553,
      "loss": 0.275,
      "step": 1714
    },
    {
      "epoch": 3.43,
      "grad_norm": 1.2642295360565186,
      "learning_rate": 0.000131452581032413,
      "loss": 0.3401,
      "step": 1715
    },
    {
      "epoch": 3.432,
      "grad_norm": 1.984068751335144,
      "learning_rate": 0.0001314125650260104,
      "loss": 0.4832,
      "step": 1716
    },
    {
      "epoch": 3.434,
      "grad_norm": 1.2975459098815918,
      "learning_rate": 0.00013137254901960784,
      "loss": 0.3106,
      "step": 1717
    },
    {
      "epoch": 3.436,
      "grad_norm": 1.427114725112915,
      "learning_rate": 0.0001313325330132053,
      "loss": 0.3589,
      "step": 1718
    },
    {
      "epoch": 3.438,
      "grad_norm": 1.734694004058838,
      "learning_rate": 0.00013129251700680271,
      "loss": 0.3621,
      "step": 1719
    },
    {
      "epoch": 3.44,
      "grad_norm": 1.9246877431869507,
      "learning_rate": 0.00013125250100040015,
      "loss": 0.4664,
      "step": 1720
    },
    {
      "epoch": 3.442,
      "grad_norm": 1.2319966554641724,
      "learning_rate": 0.00013121248499399762,
      "loss": 0.2945,
      "step": 1721
    },
    {
      "epoch": 3.444,
      "grad_norm": 2.7516326904296875,
      "learning_rate": 0.00013117246898759505,
      "loss": 0.5038,
      "step": 1722
    },
    {
      "epoch": 3.446,
      "grad_norm": 1.4652080535888672,
      "learning_rate": 0.0001311324529811925,
      "loss": 0.3172,
      "step": 1723
    },
    {
      "epoch": 3.448,
      "grad_norm": 1.9380580186843872,
      "learning_rate": 0.00013109243697478993,
      "loss": 0.3999,
      "step": 1724
    },
    {
      "epoch": 3.45,
      "grad_norm": 1.5748554468154907,
      "learning_rate": 0.00013105242096838736,
      "loss": 0.3465,
      "step": 1725
    },
    {
      "epoch": 3.452,
      "grad_norm": 1.6300137042999268,
      "learning_rate": 0.0001310124049619848,
      "loss": 0.3552,
      "step": 1726
    },
    {
      "epoch": 3.454,
      "grad_norm": 2.1398391723632812,
      "learning_rate": 0.00013097238895558224,
      "loss": 0.3961,
      "step": 1727
    },
    {
      "epoch": 3.456,
      "grad_norm": 1.5689977407455444,
      "learning_rate": 0.00013093237294917968,
      "loss": 0.3171,
      "step": 1728
    },
    {
      "epoch": 3.458,
      "grad_norm": 1.3185280561447144,
      "learning_rate": 0.00013089235694277714,
      "loss": 0.2634,
      "step": 1729
    },
    {
      "epoch": 3.46,
      "grad_norm": 1.0124530792236328,
      "learning_rate": 0.00013085234093637455,
      "loss": 0.2337,
      "step": 1730
    },
    {
      "epoch": 3.462,
      "grad_norm": 1.3277044296264648,
      "learning_rate": 0.000130812324929972,
      "loss": 0.2999,
      "step": 1731
    },
    {
      "epoch": 3.464,
      "grad_norm": 1.3152554035186768,
      "learning_rate": 0.00013077230892356945,
      "loss": 0.315,
      "step": 1732
    },
    {
      "epoch": 3.466,
      "grad_norm": 2.1912124156951904,
      "learning_rate": 0.00013073229291716686,
      "loss": 0.4708,
      "step": 1733
    },
    {
      "epoch": 3.468,
      "grad_norm": 1.5589948892593384,
      "learning_rate": 0.0001306922769107643,
      "loss": 0.3648,
      "step": 1734
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 1.6879169940948486,
      "learning_rate": 0.00013065226090436176,
      "loss": 0.3744,
      "step": 1735
    },
    {
      "epoch": 3.472,
      "grad_norm": 1.4116798639297485,
      "learning_rate": 0.00013061224489795917,
      "loss": 0.3379,
      "step": 1736
    },
    {
      "epoch": 3.474,
      "grad_norm": 1.1746225357055664,
      "learning_rate": 0.00013057222889155664,
      "loss": 0.2794,
      "step": 1737
    },
    {
      "epoch": 3.476,
      "grad_norm": 1.4323841333389282,
      "learning_rate": 0.00013053221288515407,
      "loss": 0.322,
      "step": 1738
    },
    {
      "epoch": 3.4779999999999998,
      "grad_norm": 1.7930803298950195,
      "learning_rate": 0.0001304921968787515,
      "loss": 0.4811,
      "step": 1739
    },
    {
      "epoch": 3.48,
      "grad_norm": 1.1599024534225464,
      "learning_rate": 0.00013045218087234895,
      "loss": 0.3045,
      "step": 1740
    },
    {
      "epoch": 3.482,
      "grad_norm": 1.563369870185852,
      "learning_rate": 0.00013041216486594639,
      "loss": 0.3481,
      "step": 1741
    },
    {
      "epoch": 3.484,
      "grad_norm": 1.4218361377716064,
      "learning_rate": 0.00013037214885954382,
      "loss": 0.4107,
      "step": 1742
    },
    {
      "epoch": 3.4859999999999998,
      "grad_norm": 1.418275237083435,
      "learning_rate": 0.00013033213285314126,
      "loss": 0.3638,
      "step": 1743
    },
    {
      "epoch": 3.488,
      "grad_norm": 1.474552035331726,
      "learning_rate": 0.0001302921168467387,
      "loss": 0.3426,
      "step": 1744
    },
    {
      "epoch": 3.49,
      "grad_norm": 1.2237365245819092,
      "learning_rate": 0.00013025210084033613,
      "loss": 0.2783,
      "step": 1745
    },
    {
      "epoch": 3.492,
      "grad_norm": 1.2629576921463013,
      "learning_rate": 0.0001302120848339336,
      "loss": 0.3468,
      "step": 1746
    },
    {
      "epoch": 3.4939999999999998,
      "grad_norm": 1.8975597620010376,
      "learning_rate": 0.000130172068827531,
      "loss": 0.4269,
      "step": 1747
    },
    {
      "epoch": 3.496,
      "grad_norm": 1.3047716617584229,
      "learning_rate": 0.00013013205282112845,
      "loss": 0.3702,
      "step": 1748
    },
    {
      "epoch": 3.498,
      "grad_norm": 1.359619140625,
      "learning_rate": 0.0001300920368147259,
      "loss": 0.3429,
      "step": 1749
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.089328646659851,
      "learning_rate": 0.00013005202080832332,
      "loss": 0.3204,
      "step": 1750
    },
    {
      "epoch": 3.502,
      "grad_norm": 1.8518911600112915,
      "learning_rate": 0.00013001200480192078,
      "loss": 0.3959,
      "step": 1751
    },
    {
      "epoch": 3.504,
      "grad_norm": 1.5494431257247925,
      "learning_rate": 0.00012997198879551822,
      "loss": 0.4036,
      "step": 1752
    },
    {
      "epoch": 3.5060000000000002,
      "grad_norm": 1.387000560760498,
      "learning_rate": 0.00012993197278911566,
      "loss": 0.402,
      "step": 1753
    },
    {
      "epoch": 3.508,
      "grad_norm": 1.2613922357559204,
      "learning_rate": 0.0001298919567827131,
      "loss": 0.298,
      "step": 1754
    },
    {
      "epoch": 3.51,
      "grad_norm": 1.3804935216903687,
      "learning_rate": 0.00012985194077631053,
      "loss": 0.3699,
      "step": 1755
    },
    {
      "epoch": 3.512,
      "grad_norm": 1.5809009075164795,
      "learning_rate": 0.00012981192476990797,
      "loss": 0.3951,
      "step": 1756
    },
    {
      "epoch": 3.5140000000000002,
      "grad_norm": 1.0378575325012207,
      "learning_rate": 0.0001297719087635054,
      "loss": 0.2895,
      "step": 1757
    },
    {
      "epoch": 3.516,
      "grad_norm": 1.338761329650879,
      "learning_rate": 0.00012973189275710284,
      "loss": 0.3755,
      "step": 1758
    },
    {
      "epoch": 3.518,
      "grad_norm": 1.4073470830917358,
      "learning_rate": 0.00012969187675070028,
      "loss": 0.3295,
      "step": 1759
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.575453519821167,
      "learning_rate": 0.00012965186074429772,
      "loss": 0.3874,
      "step": 1760
    },
    {
      "epoch": 3.5220000000000002,
      "grad_norm": 1.3298062086105347,
      "learning_rate": 0.00012961184473789516,
      "loss": 0.3578,
      "step": 1761
    },
    {
      "epoch": 3.524,
      "grad_norm": 1.6449525356292725,
      "learning_rate": 0.0001295718287314926,
      "loss": 0.398,
      "step": 1762
    },
    {
      "epoch": 3.526,
      "grad_norm": 1.647911787033081,
      "learning_rate": 0.00012953181272509006,
      "loss": 0.3672,
      "step": 1763
    },
    {
      "epoch": 3.528,
      "grad_norm": 1.6875176429748535,
      "learning_rate": 0.00012949179671868747,
      "loss": 0.378,
      "step": 1764
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 1.428768277168274,
      "learning_rate": 0.00012945178071228493,
      "loss": 0.3578,
      "step": 1765
    },
    {
      "epoch": 3.532,
      "grad_norm": 1.3989028930664062,
      "learning_rate": 0.00012941176470588237,
      "loss": 0.3709,
      "step": 1766
    },
    {
      "epoch": 3.534,
      "grad_norm": 1.1201071739196777,
      "learning_rate": 0.00012937174869947978,
      "loss": 0.2802,
      "step": 1767
    },
    {
      "epoch": 3.536,
      "grad_norm": 1.4128233194351196,
      "learning_rate": 0.00012933173269307724,
      "loss": 0.2891,
      "step": 1768
    },
    {
      "epoch": 3.5380000000000003,
      "grad_norm": 1.4539276361465454,
      "learning_rate": 0.00012929171668667468,
      "loss": 0.397,
      "step": 1769
    },
    {
      "epoch": 3.54,
      "grad_norm": 1.053464651107788,
      "learning_rate": 0.00012925170068027212,
      "loss": 0.2751,
      "step": 1770
    },
    {
      "epoch": 3.542,
      "grad_norm": 1.4084666967391968,
      "learning_rate": 0.00012921168467386955,
      "loss": 0.3793,
      "step": 1771
    },
    {
      "epoch": 3.544,
      "grad_norm": 1.6194196939468384,
      "learning_rate": 0.000129171668667467,
      "loss": 0.2989,
      "step": 1772
    },
    {
      "epoch": 3.5460000000000003,
      "grad_norm": 1.6885778903961182,
      "learning_rate": 0.00012913165266106443,
      "loss": 0.3196,
      "step": 1773
    },
    {
      "epoch": 3.548,
      "grad_norm": 1.3569775819778442,
      "learning_rate": 0.00012909163665466187,
      "loss": 0.3215,
      "step": 1774
    },
    {
      "epoch": 3.55,
      "grad_norm": 1.6369361877441406,
      "learning_rate": 0.0001290516206482593,
      "loss": 0.3505,
      "step": 1775
    },
    {
      "epoch": 3.552,
      "grad_norm": 1.678700566291809,
      "learning_rate": 0.00012901160464185677,
      "loss": 0.3444,
      "step": 1776
    },
    {
      "epoch": 3.5540000000000003,
      "grad_norm": 2.1118969917297363,
      "learning_rate": 0.00012897158863545418,
      "loss": 0.3789,
      "step": 1777
    },
    {
      "epoch": 3.556,
      "grad_norm": 1.6200387477874756,
      "learning_rate": 0.00012893157262905161,
      "loss": 0.3041,
      "step": 1778
    },
    {
      "epoch": 3.558,
      "grad_norm": 1.8499948978424072,
      "learning_rate": 0.00012889155662264908,
      "loss": 0.4374,
      "step": 1779
    },
    {
      "epoch": 3.56,
      "grad_norm": 1.4711277484893799,
      "learning_rate": 0.00012885154061624652,
      "loss": 0.3237,
      "step": 1780
    },
    {
      "epoch": 3.5620000000000003,
      "grad_norm": 1.3016853332519531,
      "learning_rate": 0.00012881152460984393,
      "loss": 0.3646,
      "step": 1781
    },
    {
      "epoch": 3.564,
      "grad_norm": 1.324873924255371,
      "learning_rate": 0.0001287715086034414,
      "loss": 0.3597,
      "step": 1782
    },
    {
      "epoch": 3.566,
      "grad_norm": 1.3021494150161743,
      "learning_rate": 0.00012873149259703883,
      "loss": 0.3647,
      "step": 1783
    },
    {
      "epoch": 3.568,
      "grad_norm": 1.171790599822998,
      "learning_rate": 0.00012869147659063626,
      "loss": 0.2956,
      "step": 1784
    },
    {
      "epoch": 3.57,
      "grad_norm": 1.2728949785232544,
      "learning_rate": 0.0001286514605842337,
      "loss": 0.4274,
      "step": 1785
    },
    {
      "epoch": 3.572,
      "grad_norm": 1.2982597351074219,
      "learning_rate": 0.00012861144457783114,
      "loss": 0.302,
      "step": 1786
    },
    {
      "epoch": 3.574,
      "grad_norm": 1.3287452459335327,
      "learning_rate": 0.00012857142857142858,
      "loss": 0.3131,
      "step": 1787
    },
    {
      "epoch": 3.576,
      "grad_norm": 1.2443549633026123,
      "learning_rate": 0.000128531412565026,
      "loss": 0.3279,
      "step": 1788
    },
    {
      "epoch": 3.578,
      "grad_norm": 0.9213747978210449,
      "learning_rate": 0.00012849139655862345,
      "loss": 0.2603,
      "step": 1789
    },
    {
      "epoch": 3.58,
      "grad_norm": 1.318677306175232,
      "learning_rate": 0.00012845138055222091,
      "loss": 0.3302,
      "step": 1790
    },
    {
      "epoch": 3.582,
      "grad_norm": 1.4469985961914062,
      "learning_rate": 0.00012841136454581832,
      "loss": 0.359,
      "step": 1791
    },
    {
      "epoch": 3.584,
      "grad_norm": 1.264919400215149,
      "learning_rate": 0.00012837134853941576,
      "loss": 0.3951,
      "step": 1792
    },
    {
      "epoch": 3.586,
      "grad_norm": 1.0185006856918335,
      "learning_rate": 0.00012833133253301323,
      "loss": 0.2927,
      "step": 1793
    },
    {
      "epoch": 3.588,
      "grad_norm": 1.3735032081604004,
      "learning_rate": 0.00012829131652661066,
      "loss": 0.3301,
      "step": 1794
    },
    {
      "epoch": 3.59,
      "grad_norm": 1.7113776206970215,
      "learning_rate": 0.00012825130052020807,
      "loss": 0.334,
      "step": 1795
    },
    {
      "epoch": 3.592,
      "grad_norm": 1.7278661727905273,
      "learning_rate": 0.00012821128451380554,
      "loss": 0.4167,
      "step": 1796
    },
    {
      "epoch": 3.594,
      "grad_norm": 1.6162132024765015,
      "learning_rate": 0.00012817126850740297,
      "loss": 0.388,
      "step": 1797
    },
    {
      "epoch": 3.596,
      "grad_norm": 1.2704908847808838,
      "learning_rate": 0.0001281312525010004,
      "loss": 0.3386,
      "step": 1798
    },
    {
      "epoch": 3.598,
      "grad_norm": 1.4653737545013428,
      "learning_rate": 0.00012809123649459785,
      "loss": 0.3854,
      "step": 1799
    },
    {
      "epoch": 3.6,
      "grad_norm": 1.474524736404419,
      "learning_rate": 0.00012805122048819529,
      "loss": 0.357,
      "step": 1800
    },
    {
      "epoch": 3.602,
      "grad_norm": 1.5745621919631958,
      "learning_rate": 0.00012801120448179272,
      "loss": 0.4068,
      "step": 1801
    },
    {
      "epoch": 3.604,
      "grad_norm": 1.1703336238861084,
      "learning_rate": 0.00012797118847539016,
      "loss": 0.3284,
      "step": 1802
    },
    {
      "epoch": 3.606,
      "grad_norm": 1.4018243551254272,
      "learning_rate": 0.0001279311724689876,
      "loss": 0.4492,
      "step": 1803
    },
    {
      "epoch": 3.608,
      "grad_norm": 1.697874903678894,
      "learning_rate": 0.00012789115646258506,
      "loss": 0.4804,
      "step": 1804
    },
    {
      "epoch": 3.61,
      "grad_norm": 1.674378752708435,
      "learning_rate": 0.00012785114045618247,
      "loss": 0.4824,
      "step": 1805
    },
    {
      "epoch": 3.612,
      "grad_norm": 1.5448498725891113,
      "learning_rate": 0.0001278111244497799,
      "loss": 0.3072,
      "step": 1806
    },
    {
      "epoch": 3.614,
      "grad_norm": 1.3705991506576538,
      "learning_rate": 0.00012777110844337737,
      "loss": 0.4273,
      "step": 1807
    },
    {
      "epoch": 3.616,
      "grad_norm": 1.2602049112319946,
      "learning_rate": 0.00012773109243697478,
      "loss": 0.3788,
      "step": 1808
    },
    {
      "epoch": 3.618,
      "grad_norm": 1.9227405786514282,
      "learning_rate": 0.00012769107643057222,
      "loss": 0.3582,
      "step": 1809
    },
    {
      "epoch": 3.62,
      "grad_norm": 1.6297738552093506,
      "learning_rate": 0.00012765106042416968,
      "loss": 0.3112,
      "step": 1810
    },
    {
      "epoch": 3.622,
      "grad_norm": 1.379428744316101,
      "learning_rate": 0.00012761104441776712,
      "loss": 0.3166,
      "step": 1811
    },
    {
      "epoch": 3.624,
      "grad_norm": 1.4215747117996216,
      "learning_rate": 0.00012757102841136456,
      "loss": 0.307,
      "step": 1812
    },
    {
      "epoch": 3.626,
      "grad_norm": 1.6996334791183472,
      "learning_rate": 0.000127531012404962,
      "loss": 0.4362,
      "step": 1813
    },
    {
      "epoch": 3.628,
      "grad_norm": 1.7228431701660156,
      "learning_rate": 0.00012749099639855943,
      "loss": 0.3308,
      "step": 1814
    },
    {
      "epoch": 3.63,
      "grad_norm": 1.1227149963378906,
      "learning_rate": 0.00012745098039215687,
      "loss": 0.3738,
      "step": 1815
    },
    {
      "epoch": 3.632,
      "grad_norm": 1.267104983329773,
      "learning_rate": 0.0001274109643857543,
      "loss": 0.2881,
      "step": 1816
    },
    {
      "epoch": 3.634,
      "grad_norm": 1.8724350929260254,
      "learning_rate": 0.00012737094837935174,
      "loss": 0.3262,
      "step": 1817
    },
    {
      "epoch": 3.636,
      "grad_norm": 1.2171878814697266,
      "learning_rate": 0.0001273309323729492,
      "loss": 0.2962,
      "step": 1818
    },
    {
      "epoch": 3.638,
      "grad_norm": 1.455914855003357,
      "learning_rate": 0.00012729091636654662,
      "loss": 0.3252,
      "step": 1819
    },
    {
      "epoch": 3.64,
      "grad_norm": 1.3045728206634521,
      "learning_rate": 0.00012725090036014406,
      "loss": 0.3457,
      "step": 1820
    },
    {
      "epoch": 3.642,
      "grad_norm": 1.0072216987609863,
      "learning_rate": 0.00012721088435374152,
      "loss": 0.2824,
      "step": 1821
    },
    {
      "epoch": 3.644,
      "grad_norm": 1.713156819343567,
      "learning_rate": 0.00012717086834733893,
      "loss": 0.4781,
      "step": 1822
    },
    {
      "epoch": 3.646,
      "grad_norm": 1.289931058883667,
      "learning_rate": 0.00012713085234093637,
      "loss": 0.2924,
      "step": 1823
    },
    {
      "epoch": 3.648,
      "grad_norm": 1.2239596843719482,
      "learning_rate": 0.00012709083633453383,
      "loss": 0.3206,
      "step": 1824
    },
    {
      "epoch": 3.65,
      "grad_norm": 1.6985293626785278,
      "learning_rate": 0.00012705082032813124,
      "loss": 0.5574,
      "step": 1825
    },
    {
      "epoch": 3.652,
      "grad_norm": 1.3394666910171509,
      "learning_rate": 0.0001270108043217287,
      "loss": 0.3769,
      "step": 1826
    },
    {
      "epoch": 3.654,
      "grad_norm": 1.8310068845748901,
      "learning_rate": 0.00012697078831532614,
      "loss": 0.4654,
      "step": 1827
    },
    {
      "epoch": 3.656,
      "grad_norm": 1.443730354309082,
      "learning_rate": 0.00012693077230892358,
      "loss": 0.3788,
      "step": 1828
    },
    {
      "epoch": 3.658,
      "grad_norm": 1.383257269859314,
      "learning_rate": 0.00012689075630252102,
      "loss": 0.3129,
      "step": 1829
    },
    {
      "epoch": 3.66,
      "grad_norm": 1.6083911657333374,
      "learning_rate": 0.00012685074029611845,
      "loss": 0.3446,
      "step": 1830
    },
    {
      "epoch": 3.662,
      "grad_norm": 0.923766553401947,
      "learning_rate": 0.0001268107242897159,
      "loss": 0.2565,
      "step": 1831
    },
    {
      "epoch": 3.664,
      "grad_norm": 1.8426461219787598,
      "learning_rate": 0.00012677070828331333,
      "loss": 0.4397,
      "step": 1832
    },
    {
      "epoch": 3.666,
      "grad_norm": 1.194136619567871,
      "learning_rate": 0.00012673069227691077,
      "loss": 0.2832,
      "step": 1833
    },
    {
      "epoch": 3.668,
      "grad_norm": 1.1672842502593994,
      "learning_rate": 0.0001266906762705082,
      "loss": 0.3181,
      "step": 1834
    },
    {
      "epoch": 3.67,
      "grad_norm": 1.4020525217056274,
      "learning_rate": 0.00012665066026410567,
      "loss": 0.3516,
      "step": 1835
    },
    {
      "epoch": 3.672,
      "grad_norm": 1.4071587324142456,
      "learning_rate": 0.00012661064425770308,
      "loss": 0.357,
      "step": 1836
    },
    {
      "epoch": 3.674,
      "grad_norm": 1.0974489450454712,
      "learning_rate": 0.00012657062825130051,
      "loss": 0.3268,
      "step": 1837
    },
    {
      "epoch": 3.676,
      "grad_norm": 1.4038561582565308,
      "learning_rate": 0.00012653061224489798,
      "loss": 0.3308,
      "step": 1838
    },
    {
      "epoch": 3.678,
      "grad_norm": 1.586448073387146,
      "learning_rate": 0.0001264905962384954,
      "loss": 0.3474,
      "step": 1839
    },
    {
      "epoch": 3.68,
      "grad_norm": 1.2786078453063965,
      "learning_rate": 0.00012645058023209285,
      "loss": 0.2996,
      "step": 1840
    },
    {
      "epoch": 3.682,
      "grad_norm": 1.6075313091278076,
      "learning_rate": 0.0001264105642256903,
      "loss": 0.3636,
      "step": 1841
    },
    {
      "epoch": 3.684,
      "grad_norm": 1.1318061351776123,
      "learning_rate": 0.00012637054821928773,
      "loss": 0.3054,
      "step": 1842
    },
    {
      "epoch": 3.686,
      "grad_norm": 1.3671635389328003,
      "learning_rate": 0.00012633053221288516,
      "loss": 0.3084,
      "step": 1843
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 1.214860439300537,
      "learning_rate": 0.0001262905162064826,
      "loss": 0.2942,
      "step": 1844
    },
    {
      "epoch": 3.69,
      "grad_norm": 1.1437492370605469,
      "learning_rate": 0.00012625050020008004,
      "loss": 0.3193,
      "step": 1845
    },
    {
      "epoch": 3.692,
      "grad_norm": 1.0283913612365723,
      "learning_rate": 0.00012621048419367748,
      "loss": 0.2749,
      "step": 1846
    },
    {
      "epoch": 3.694,
      "grad_norm": 1.71024489402771,
      "learning_rate": 0.0001261704681872749,
      "loss": 0.3461,
      "step": 1847
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 1.26078200340271,
      "learning_rate": 0.00012613045218087235,
      "loss": 0.3527,
      "step": 1848
    },
    {
      "epoch": 3.698,
      "grad_norm": 1.56098210811615,
      "learning_rate": 0.0001260904361744698,
      "loss": 0.3434,
      "step": 1849
    },
    {
      "epoch": 3.7,
      "grad_norm": 1.0742902755737305,
      "learning_rate": 0.00012605042016806722,
      "loss": 0.2813,
      "step": 1850
    },
    {
      "epoch": 3.702,
      "grad_norm": 1.5107425451278687,
      "learning_rate": 0.0001260104041616647,
      "loss": 0.4934,
      "step": 1851
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 1.6232311725616455,
      "learning_rate": 0.00012597038815526212,
      "loss": 0.3728,
      "step": 1852
    },
    {
      "epoch": 3.706,
      "grad_norm": 1.2273269891738892,
      "learning_rate": 0.00012593037214885953,
      "loss": 0.237,
      "step": 1853
    },
    {
      "epoch": 3.708,
      "grad_norm": 1.298546314239502,
      "learning_rate": 0.000125890356142457,
      "loss": 0.3235,
      "step": 1854
    },
    {
      "epoch": 3.71,
      "grad_norm": 1.1532424688339233,
      "learning_rate": 0.00012585034013605444,
      "loss": 0.2772,
      "step": 1855
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 1.2796216011047363,
      "learning_rate": 0.00012581032412965185,
      "loss": 0.3445,
      "step": 1856
    },
    {
      "epoch": 3.714,
      "grad_norm": 1.4659953117370605,
      "learning_rate": 0.0001257703081232493,
      "loss": 0.4091,
      "step": 1857
    },
    {
      "epoch": 3.716,
      "grad_norm": 1.3959500789642334,
      "learning_rate": 0.00012573029211684675,
      "loss": 0.3815,
      "step": 1858
    },
    {
      "epoch": 3.718,
      "grad_norm": 1.7342458963394165,
      "learning_rate": 0.00012569027611044418,
      "loss": 0.412,
      "step": 1859
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 1.2905055284500122,
      "learning_rate": 0.00012565026010404162,
      "loss": 0.3132,
      "step": 1860
    },
    {
      "epoch": 3.722,
      "grad_norm": 1.2063484191894531,
      "learning_rate": 0.00012561024409763906,
      "loss": 0.2969,
      "step": 1861
    },
    {
      "epoch": 3.724,
      "grad_norm": 1.2387208938598633,
      "learning_rate": 0.0001255702280912365,
      "loss": 0.4052,
      "step": 1862
    },
    {
      "epoch": 3.726,
      "grad_norm": 1.4725652933120728,
      "learning_rate": 0.00012553021208483393,
      "loss": 0.3415,
      "step": 1863
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 1.6581480503082275,
      "learning_rate": 0.00012549019607843137,
      "loss": 0.3808,
      "step": 1864
    },
    {
      "epoch": 3.73,
      "grad_norm": 1.215015172958374,
      "learning_rate": 0.00012545018007202883,
      "loss": 0.2665,
      "step": 1865
    },
    {
      "epoch": 3.732,
      "grad_norm": 1.1366358995437622,
      "learning_rate": 0.00012541016406562624,
      "loss": 0.3106,
      "step": 1866
    },
    {
      "epoch": 3.734,
      "grad_norm": 1.5102955102920532,
      "learning_rate": 0.00012537014805922368,
      "loss": 0.3723,
      "step": 1867
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 1.640894889831543,
      "learning_rate": 0.00012533013205282115,
      "loss": 0.3234,
      "step": 1868
    },
    {
      "epoch": 3.738,
      "grad_norm": 1.9702829122543335,
      "learning_rate": 0.00012529011604641858,
      "loss": 0.3092,
      "step": 1869
    },
    {
      "epoch": 3.74,
      "grad_norm": 2.354053020477295,
      "learning_rate": 0.000125250100040016,
      "loss": 0.4364,
      "step": 1870
    },
    {
      "epoch": 3.742,
      "grad_norm": 1.4232399463653564,
      "learning_rate": 0.00012521008403361346,
      "loss": 0.3329,
      "step": 1871
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 1.6033942699432373,
      "learning_rate": 0.0001251700680272109,
      "loss": 0.4328,
      "step": 1872
    },
    {
      "epoch": 3.746,
      "grad_norm": 1.4465482234954834,
      "learning_rate": 0.00012513005202080833,
      "loss": 0.348,
      "step": 1873
    },
    {
      "epoch": 3.748,
      "grad_norm": 1.2983694076538086,
      "learning_rate": 0.00012509003601440577,
      "loss": 0.3367,
      "step": 1874
    },
    {
      "epoch": 3.75,
      "grad_norm": 1.186554193496704,
      "learning_rate": 0.0001250500200080032,
      "loss": 0.2777,
      "step": 1875
    },
    {
      "epoch": 3.752,
      "grad_norm": 1.0327380895614624,
      "learning_rate": 0.00012501000400160064,
      "loss": 0.2826,
      "step": 1876
    },
    {
      "epoch": 3.754,
      "grad_norm": 1.1231426000595093,
      "learning_rate": 0.00012496998799519808,
      "loss": 0.3542,
      "step": 1877
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 1.272228717803955,
      "learning_rate": 0.00012492997198879552,
      "loss": 0.3478,
      "step": 1878
    },
    {
      "epoch": 3.758,
      "grad_norm": 1.4351811408996582,
      "learning_rate": 0.00012488995598239298,
      "loss": 0.3609,
      "step": 1879
    },
    {
      "epoch": 3.76,
      "grad_norm": 1.6873325109481812,
      "learning_rate": 0.0001248499399759904,
      "loss": 0.3167,
      "step": 1880
    },
    {
      "epoch": 3.762,
      "grad_norm": 1.0985267162322998,
      "learning_rate": 0.00012480992396958783,
      "loss": 0.3125,
      "step": 1881
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 1.4239652156829834,
      "learning_rate": 0.0001247699079631853,
      "loss": 0.3994,
      "step": 1882
    },
    {
      "epoch": 3.766,
      "grad_norm": 1.5879532098770142,
      "learning_rate": 0.00012472989195678273,
      "loss": 0.3589,
      "step": 1883
    },
    {
      "epoch": 3.768,
      "grad_norm": 1.362804889678955,
      "learning_rate": 0.00012468987595038014,
      "loss": 0.2834,
      "step": 1884
    },
    {
      "epoch": 3.77,
      "grad_norm": 1.7342814207077026,
      "learning_rate": 0.0001246498599439776,
      "loss": 0.43,
      "step": 1885
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 1.803245186805725,
      "learning_rate": 0.00012460984393757504,
      "loss": 0.4833,
      "step": 1886
    },
    {
      "epoch": 3.774,
      "grad_norm": 1.4334856271743774,
      "learning_rate": 0.00012456982793117248,
      "loss": 0.3352,
      "step": 1887
    },
    {
      "epoch": 3.776,
      "grad_norm": 1.392037034034729,
      "learning_rate": 0.00012452981192476992,
      "loss": 0.3185,
      "step": 1888
    },
    {
      "epoch": 3.778,
      "grad_norm": 1.2346431016921997,
      "learning_rate": 0.00012448979591836735,
      "loss": 0.3286,
      "step": 1889
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 1.1421359777450562,
      "learning_rate": 0.0001244497799119648,
      "loss": 0.2932,
      "step": 1890
    },
    {
      "epoch": 3.782,
      "grad_norm": 1.255560278892517,
      "learning_rate": 0.00012440976390556223,
      "loss": 0.298,
      "step": 1891
    },
    {
      "epoch": 3.784,
      "grad_norm": 1.763351321220398,
      "learning_rate": 0.00012436974789915966,
      "loss": 0.4414,
      "step": 1892
    },
    {
      "epoch": 3.786,
      "grad_norm": 1.2441494464874268,
      "learning_rate": 0.00012432973189275713,
      "loss": 0.3289,
      "step": 1893
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 1.6480754613876343,
      "learning_rate": 0.00012428971588635454,
      "loss": 0.3547,
      "step": 1894
    },
    {
      "epoch": 3.79,
      "grad_norm": 1.381506323814392,
      "learning_rate": 0.00012424969987995198,
      "loss": 0.3248,
      "step": 1895
    },
    {
      "epoch": 3.792,
      "grad_norm": 1.1621747016906738,
      "learning_rate": 0.00012420968387354944,
      "loss": 0.368,
      "step": 1896
    },
    {
      "epoch": 3.794,
      "grad_norm": 1.3096060752868652,
      "learning_rate": 0.00012416966786714685,
      "loss": 0.3844,
      "step": 1897
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 0.9631454944610596,
      "learning_rate": 0.0001241296518607443,
      "loss": 0.2685,
      "step": 1898
    },
    {
      "epoch": 3.798,
      "grad_norm": 1.1924448013305664,
      "learning_rate": 0.00012408963585434175,
      "loss": 0.3915,
      "step": 1899
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.1225529909133911,
      "learning_rate": 0.0001240496198479392,
      "loss": 0.2399,
      "step": 1900
    },
    {
      "epoch": 3.802,
      "grad_norm": 1.2531757354736328,
      "learning_rate": 0.00012400960384153663,
      "loss": 0.3378,
      "step": 1901
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 1.642244577407837,
      "learning_rate": 0.00012396958783513406,
      "loss": 0.4106,
      "step": 1902
    },
    {
      "epoch": 3.806,
      "grad_norm": 1.1309778690338135,
      "learning_rate": 0.0001239295718287315,
      "loss": 0.3356,
      "step": 1903
    },
    {
      "epoch": 3.808,
      "grad_norm": 1.5147075653076172,
      "learning_rate": 0.00012388955582232894,
      "loss": 0.4198,
      "step": 1904
    },
    {
      "epoch": 3.81,
      "grad_norm": 1.1746826171875,
      "learning_rate": 0.00012384953981592637,
      "loss": 0.3075,
      "step": 1905
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 1.4847127199172974,
      "learning_rate": 0.0001238095238095238,
      "loss": 0.4399,
      "step": 1906
    },
    {
      "epoch": 3.814,
      "grad_norm": 1.588660717010498,
      "learning_rate": 0.00012376950780312128,
      "loss": 0.5232,
      "step": 1907
    },
    {
      "epoch": 3.816,
      "grad_norm": 1.4837989807128906,
      "learning_rate": 0.00012372949179671869,
      "loss": 0.3589,
      "step": 1908
    },
    {
      "epoch": 3.818,
      "grad_norm": 1.308065414428711,
      "learning_rate": 0.00012368947579031612,
      "loss": 0.3662,
      "step": 1909
    },
    {
      "epoch": 3.82,
      "grad_norm": 1.1619702577590942,
      "learning_rate": 0.0001236494597839136,
      "loss": 0.2379,
      "step": 1910
    },
    {
      "epoch": 3.822,
      "grad_norm": 1.4208508729934692,
      "learning_rate": 0.000123609443777511,
      "loss": 0.3754,
      "step": 1911
    },
    {
      "epoch": 3.824,
      "grad_norm": 1.8495969772338867,
      "learning_rate": 0.00012356942777110846,
      "loss": 0.3692,
      "step": 1912
    },
    {
      "epoch": 3.826,
      "grad_norm": 1.1444238424301147,
      "learning_rate": 0.0001235294117647059,
      "loss": 0.3165,
      "step": 1913
    },
    {
      "epoch": 3.828,
      "grad_norm": 1.3025718927383423,
      "learning_rate": 0.0001234893957583033,
      "loss": 0.3225,
      "step": 1914
    },
    {
      "epoch": 3.83,
      "grad_norm": 1.6044747829437256,
      "learning_rate": 0.00012344937975190077,
      "loss": 0.4029,
      "step": 1915
    },
    {
      "epoch": 3.832,
      "grad_norm": 1.2823859453201294,
      "learning_rate": 0.0001234093637454982,
      "loss": 0.3895,
      "step": 1916
    },
    {
      "epoch": 3.834,
      "grad_norm": 1.6246600151062012,
      "learning_rate": 0.00012336934773909565,
      "loss": 0.3792,
      "step": 1917
    },
    {
      "epoch": 3.836,
      "grad_norm": 1.4033854007720947,
      "learning_rate": 0.00012332933173269308,
      "loss": 0.3874,
      "step": 1918
    },
    {
      "epoch": 3.838,
      "grad_norm": 1.142608880996704,
      "learning_rate": 0.00012328931572629052,
      "loss": 0.285,
      "step": 1919
    },
    {
      "epoch": 3.84,
      "grad_norm": 1.18728506565094,
      "learning_rate": 0.00012324929971988796,
      "loss": 0.2698,
      "step": 1920
    },
    {
      "epoch": 3.842,
      "grad_norm": 1.1338589191436768,
      "learning_rate": 0.0001232092837134854,
      "loss": 0.3168,
      "step": 1921
    },
    {
      "epoch": 3.844,
      "grad_norm": 1.6629208326339722,
      "learning_rate": 0.00012316926770708283,
      "loss": 0.4,
      "step": 1922
    },
    {
      "epoch": 3.846,
      "grad_norm": 1.476973533630371,
      "learning_rate": 0.00012312925170068027,
      "loss": 0.3874,
      "step": 1923
    },
    {
      "epoch": 3.848,
      "grad_norm": 1.3435577154159546,
      "learning_rate": 0.00012308923569427773,
      "loss": 0.327,
      "step": 1924
    },
    {
      "epoch": 3.85,
      "grad_norm": 1.3315812349319458,
      "learning_rate": 0.00012304921968787514,
      "loss": 0.3461,
      "step": 1925
    },
    {
      "epoch": 3.852,
      "grad_norm": 1.550759196281433,
      "learning_rate": 0.0001230092036814726,
      "loss": 0.5063,
      "step": 1926
    },
    {
      "epoch": 3.854,
      "grad_norm": 1.3433043956756592,
      "learning_rate": 0.00012296918767507005,
      "loss": 0.3962,
      "step": 1927
    },
    {
      "epoch": 3.856,
      "grad_norm": 1.1583203077316284,
      "learning_rate": 0.00012292917166866746,
      "loss": 0.3253,
      "step": 1928
    },
    {
      "epoch": 3.858,
      "grad_norm": 1.1796280145645142,
      "learning_rate": 0.00012288915566226492,
      "loss": 0.294,
      "step": 1929
    },
    {
      "epoch": 3.86,
      "grad_norm": 1.2112289667129517,
      "learning_rate": 0.00012284913965586236,
      "loss": 0.3096,
      "step": 1930
    },
    {
      "epoch": 3.862,
      "grad_norm": 1.502604603767395,
      "learning_rate": 0.0001228091236494598,
      "loss": 0.3659,
      "step": 1931
    },
    {
      "epoch": 3.864,
      "grad_norm": 1.2018318176269531,
      "learning_rate": 0.00012276910764305723,
      "loss": 0.2765,
      "step": 1932
    },
    {
      "epoch": 3.866,
      "grad_norm": 1.2678865194320679,
      "learning_rate": 0.00012272909163665467,
      "loss": 0.3566,
      "step": 1933
    },
    {
      "epoch": 3.868,
      "grad_norm": 1.113604187965393,
      "learning_rate": 0.0001226890756302521,
      "loss": 0.3492,
      "step": 1934
    },
    {
      "epoch": 3.87,
      "grad_norm": 1.1977885961532593,
      "learning_rate": 0.00012264905962384954,
      "loss": 0.2972,
      "step": 1935
    },
    {
      "epoch": 3.872,
      "grad_norm": 1.0215363502502441,
      "learning_rate": 0.00012260904361744698,
      "loss": 0.2727,
      "step": 1936
    },
    {
      "epoch": 3.874,
      "grad_norm": 1.2181880474090576,
      "learning_rate": 0.00012256902761104442,
      "loss": 0.3166,
      "step": 1937
    },
    {
      "epoch": 3.876,
      "grad_norm": 1.371077299118042,
      "learning_rate": 0.00012252901160464185,
      "loss": 0.3795,
      "step": 1938
    },
    {
      "epoch": 3.878,
      "grad_norm": 1.2941479682922363,
      "learning_rate": 0.0001224889955982393,
      "loss": 0.3422,
      "step": 1939
    },
    {
      "epoch": 3.88,
      "grad_norm": 1.3397244215011597,
      "learning_rate": 0.00012244897959183676,
      "loss": 0.3888,
      "step": 1940
    },
    {
      "epoch": 3.882,
      "grad_norm": 1.751847505569458,
      "learning_rate": 0.0001224089635854342,
      "loss": 0.5325,
      "step": 1941
    },
    {
      "epoch": 3.884,
      "grad_norm": 1.2804310321807861,
      "learning_rate": 0.0001223689475790316,
      "loss": 0.3221,
      "step": 1942
    },
    {
      "epoch": 3.886,
      "grad_norm": 1.1577982902526855,
      "learning_rate": 0.00012232893157262907,
      "loss": 0.2662,
      "step": 1943
    },
    {
      "epoch": 3.888,
      "grad_norm": 1.483272910118103,
      "learning_rate": 0.0001222889155662265,
      "loss": 0.3718,
      "step": 1944
    },
    {
      "epoch": 3.89,
      "grad_norm": 1.3950436115264893,
      "learning_rate": 0.00012224889955982391,
      "loss": 0.2761,
      "step": 1945
    },
    {
      "epoch": 3.892,
      "grad_norm": 1.799680233001709,
      "learning_rate": 0.00012220888355342138,
      "loss": 0.4999,
      "step": 1946
    },
    {
      "epoch": 3.894,
      "grad_norm": 1.3874495029449463,
      "learning_rate": 0.00012216886754701882,
      "loss": 0.3299,
      "step": 1947
    },
    {
      "epoch": 3.896,
      "grad_norm": 2.3100128173828125,
      "learning_rate": 0.00012212885154061625,
      "loss": 0.491,
      "step": 1948
    },
    {
      "epoch": 3.898,
      "grad_norm": 1.2461681365966797,
      "learning_rate": 0.0001220888355342137,
      "loss": 0.3293,
      "step": 1949
    },
    {
      "epoch": 3.9,
      "grad_norm": 1.1909772157669067,
      "learning_rate": 0.00012204881952781113,
      "loss": 0.3374,
      "step": 1950
    },
    {
      "epoch": 3.902,
      "grad_norm": 1.1780216693878174,
      "learning_rate": 0.00012200880352140858,
      "loss": 0.7228,
      "step": 1951
    },
    {
      "epoch": 3.904,
      "grad_norm": 1.194506049156189,
      "learning_rate": 0.000121968787515006,
      "loss": 0.3866,
      "step": 1952
    },
    {
      "epoch": 3.906,
      "grad_norm": 1.8019368648529053,
      "learning_rate": 0.00012192877150860345,
      "loss": 0.3658,
      "step": 1953
    },
    {
      "epoch": 3.908,
      "grad_norm": 1.582909107208252,
      "learning_rate": 0.00012188875550220089,
      "loss": 0.3133,
      "step": 1954
    },
    {
      "epoch": 3.91,
      "grad_norm": 1.2935019731521606,
      "learning_rate": 0.00012184873949579831,
      "loss": 0.2802,
      "step": 1955
    },
    {
      "epoch": 3.912,
      "grad_norm": 1.4915882349014282,
      "learning_rate": 0.00012180872348939576,
      "loss": 0.3538,
      "step": 1956
    },
    {
      "epoch": 3.914,
      "grad_norm": 1.5611246824264526,
      "learning_rate": 0.0001217687074829932,
      "loss": 0.4605,
      "step": 1957
    },
    {
      "epoch": 3.916,
      "grad_norm": 1.3786602020263672,
      "learning_rate": 0.00012172869147659065,
      "loss": 0.4058,
      "step": 1958
    },
    {
      "epoch": 3.918,
      "grad_norm": 1.497367024421692,
      "learning_rate": 0.00012168867547018807,
      "loss": 0.3453,
      "step": 1959
    },
    {
      "epoch": 3.92,
      "grad_norm": 1.336409091949463,
      "learning_rate": 0.00012164865946378553,
      "loss": 0.3257,
      "step": 1960
    },
    {
      "epoch": 3.922,
      "grad_norm": 1.3921606540679932,
      "learning_rate": 0.00012160864345738296,
      "loss": 0.3822,
      "step": 1961
    },
    {
      "epoch": 3.924,
      "grad_norm": 0.9539583325386047,
      "learning_rate": 0.00012156862745098039,
      "loss": 0.2747,
      "step": 1962
    },
    {
      "epoch": 3.926,
      "grad_norm": 1.390287160873413,
      "learning_rate": 0.00012152861144457784,
      "loss": 0.484,
      "step": 1963
    },
    {
      "epoch": 3.928,
      "grad_norm": 1.5206270217895508,
      "learning_rate": 0.00012148859543817527,
      "loss": 0.3691,
      "step": 1964
    },
    {
      "epoch": 3.93,
      "grad_norm": 1.547695517539978,
      "learning_rate": 0.00012144857943177272,
      "loss": 0.3564,
      "step": 1965
    },
    {
      "epoch": 3.932,
      "grad_norm": 1.4936342239379883,
      "learning_rate": 0.00012140856342537015,
      "loss": 0.4692,
      "step": 1966
    },
    {
      "epoch": 3.934,
      "grad_norm": 2.0496978759765625,
      "learning_rate": 0.0001213685474189676,
      "loss": 0.4542,
      "step": 1967
    },
    {
      "epoch": 3.936,
      "grad_norm": 1.9621731042861938,
      "learning_rate": 0.00012132853141256504,
      "loss": 0.4071,
      "step": 1968
    },
    {
      "epoch": 3.9379999999999997,
      "grad_norm": 1.6949297189712524,
      "learning_rate": 0.00012128851540616246,
      "loss": 0.3649,
      "step": 1969
    },
    {
      "epoch": 3.94,
      "grad_norm": 1.1408660411834717,
      "learning_rate": 0.00012124849939975991,
      "loss": 0.3167,
      "step": 1970
    },
    {
      "epoch": 3.942,
      "grad_norm": 1.600568413734436,
      "learning_rate": 0.00012120848339335735,
      "loss": 0.3739,
      "step": 1971
    },
    {
      "epoch": 3.944,
      "grad_norm": 1.5079939365386963,
      "learning_rate": 0.0001211684673869548,
      "loss": 0.3903,
      "step": 1972
    },
    {
      "epoch": 3.9459999999999997,
      "grad_norm": 1.1767528057098389,
      "learning_rate": 0.00012112845138055222,
      "loss": 0.2595,
      "step": 1973
    },
    {
      "epoch": 3.948,
      "grad_norm": 1.2185221910476685,
      "learning_rate": 0.00012108843537414967,
      "loss": 0.3093,
      "step": 1974
    },
    {
      "epoch": 3.95,
      "grad_norm": 1.6949589252471924,
      "learning_rate": 0.00012104841936774711,
      "loss": 0.4368,
      "step": 1975
    },
    {
      "epoch": 3.952,
      "grad_norm": 1.3898013830184937,
      "learning_rate": 0.00012100840336134453,
      "loss": 0.3154,
      "step": 1976
    },
    {
      "epoch": 3.9539999999999997,
      "grad_norm": 1.6729800701141357,
      "learning_rate": 0.00012096838735494198,
      "loss": 0.4756,
      "step": 1977
    },
    {
      "epoch": 3.956,
      "grad_norm": 1.5250964164733887,
      "learning_rate": 0.00012092837134853942,
      "loss": 0.3398,
      "step": 1978
    },
    {
      "epoch": 3.958,
      "grad_norm": 1.1622378826141357,
      "learning_rate": 0.00012088835534213684,
      "loss": 0.25,
      "step": 1979
    },
    {
      "epoch": 3.96,
      "grad_norm": 1.3181579113006592,
      "learning_rate": 0.0001208483393357343,
      "loss": 0.2888,
      "step": 1980
    },
    {
      "epoch": 3.9619999999999997,
      "grad_norm": 1.4204370975494385,
      "learning_rate": 0.00012080832332933175,
      "loss": 0.3864,
      "step": 1981
    },
    {
      "epoch": 3.964,
      "grad_norm": 1.328444480895996,
      "learning_rate": 0.00012076830732292918,
      "loss": 0.2676,
      "step": 1982
    },
    {
      "epoch": 3.966,
      "grad_norm": 1.6735581159591675,
      "learning_rate": 0.0001207282913165266,
      "loss": 0.4905,
      "step": 1983
    },
    {
      "epoch": 3.968,
      "grad_norm": 1.2911434173583984,
      "learning_rate": 0.00012068827531012406,
      "loss": 0.2869,
      "step": 1984
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 1.3585633039474487,
      "learning_rate": 0.0001206482593037215,
      "loss": 0.3206,
      "step": 1985
    },
    {
      "epoch": 3.972,
      "grad_norm": 1.4752378463745117,
      "learning_rate": 0.00012060824329731892,
      "loss": 0.3508,
      "step": 1986
    },
    {
      "epoch": 3.974,
      "grad_norm": 1.5888516902923584,
      "learning_rate": 0.00012056822729091637,
      "loss": 0.3455,
      "step": 1987
    },
    {
      "epoch": 3.976,
      "grad_norm": 1.7641946077346802,
      "learning_rate": 0.00012052821128451382,
      "loss": 0.3604,
      "step": 1988
    },
    {
      "epoch": 3.9779999999999998,
      "grad_norm": 2.101015090942383,
      "learning_rate": 0.00012048819527811126,
      "loss": 0.377,
      "step": 1989
    },
    {
      "epoch": 3.98,
      "grad_norm": 1.6489109992980957,
      "learning_rate": 0.00012044817927170868,
      "loss": 0.3743,
      "step": 1990
    },
    {
      "epoch": 3.982,
      "grad_norm": 1.7014446258544922,
      "learning_rate": 0.00012040816326530613,
      "loss": 0.3477,
      "step": 1991
    },
    {
      "epoch": 3.984,
      "grad_norm": 1.5226200819015503,
      "learning_rate": 0.00012036814725890357,
      "loss": 0.3594,
      "step": 1992
    },
    {
      "epoch": 3.9859999999999998,
      "grad_norm": 1.739799976348877,
      "learning_rate": 0.00012032813125250099,
      "loss": 0.3306,
      "step": 1993
    },
    {
      "epoch": 3.988,
      "grad_norm": 1.901292324066162,
      "learning_rate": 0.00012028811524609844,
      "loss": 0.4603,
      "step": 1994
    },
    {
      "epoch": 3.99,
      "grad_norm": 1.3458187580108643,
      "learning_rate": 0.00012024809923969589,
      "loss": 0.3519,
      "step": 1995
    },
    {
      "epoch": 3.992,
      "grad_norm": 1.4999972581863403,
      "learning_rate": 0.00012020808323329333,
      "loss": 0.4506,
      "step": 1996
    },
    {
      "epoch": 3.9939999999999998,
      "grad_norm": 1.0308924913406372,
      "learning_rate": 0.00012016806722689075,
      "loss": 0.225,
      "step": 1997
    },
    {
      "epoch": 3.996,
      "grad_norm": 1.0847042798995972,
      "learning_rate": 0.0001201280512204882,
      "loss": 0.2577,
      "step": 1998
    },
    {
      "epoch": 3.998,
      "grad_norm": 1.61339271068573,
      "learning_rate": 0.00012008803521408564,
      "loss": 0.3645,
      "step": 1999
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.201343297958374,
      "learning_rate": 0.00012004801920768306,
      "loss": 0.306,
      "step": 2000
    },
    {
      "epoch": 4.002,
      "grad_norm": 0.935948371887207,
      "learning_rate": 0.00012000800320128052,
      "loss": 0.226,
      "step": 2001
    },
    {
      "epoch": 4.004,
      "grad_norm": 1.0831056833267212,
      "learning_rate": 0.00011996798719487797,
      "loss": 0.2369,
      "step": 2002
    },
    {
      "epoch": 4.006,
      "grad_norm": 1.204866647720337,
      "learning_rate": 0.00011992797118847539,
      "loss": 0.2622,
      "step": 2003
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.5955187082290649,
      "learning_rate": 0.00011988795518207283,
      "loss": 0.1674,
      "step": 2004
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.8257813453674316,
      "learning_rate": 0.00011984793917567028,
      "loss": 0.2255,
      "step": 2005
    },
    {
      "epoch": 4.012,
      "grad_norm": 1.0104191303253174,
      "learning_rate": 0.00011980792316926773,
      "loss": 0.2595,
      "step": 2006
    },
    {
      "epoch": 4.014,
      "grad_norm": 1.0945453643798828,
      "learning_rate": 0.00011976790716286514,
      "loss": 0.2173,
      "step": 2007
    },
    {
      "epoch": 4.016,
      "grad_norm": 1.3101990222930908,
      "learning_rate": 0.00011972789115646259,
      "loss": 0.2309,
      "step": 2008
    },
    {
      "epoch": 4.018,
      "grad_norm": 1.139054775238037,
      "learning_rate": 0.00011968787515006004,
      "loss": 0.2271,
      "step": 2009
    },
    {
      "epoch": 4.02,
      "grad_norm": 1.0884912014007568,
      "learning_rate": 0.00011964785914365746,
      "loss": 0.2049,
      "step": 2010
    },
    {
      "epoch": 4.022,
      "grad_norm": 1.2580500841140747,
      "learning_rate": 0.0001196078431372549,
      "loss": 0.226,
      "step": 2011
    },
    {
      "epoch": 4.024,
      "grad_norm": 1.090331792831421,
      "learning_rate": 0.00011956782713085235,
      "loss": 0.2541,
      "step": 2012
    },
    {
      "epoch": 4.026,
      "grad_norm": 2.061843156814575,
      "learning_rate": 0.0001195278111244498,
      "loss": 0.2942,
      "step": 2013
    },
    {
      "epoch": 4.028,
      "grad_norm": 1.1585533618927002,
      "learning_rate": 0.00011948779511804723,
      "loss": 0.2017,
      "step": 2014
    },
    {
      "epoch": 4.03,
      "grad_norm": 1.2473645210266113,
      "learning_rate": 0.00011944777911164466,
      "loss": 0.2132,
      "step": 2015
    },
    {
      "epoch": 4.032,
      "grad_norm": 1.427720308303833,
      "learning_rate": 0.00011940776310524211,
      "loss": 0.2227,
      "step": 2016
    },
    {
      "epoch": 4.034,
      "grad_norm": 1.2683665752410889,
      "learning_rate": 0.00011936774709883954,
      "loss": 0.2604,
      "step": 2017
    },
    {
      "epoch": 4.036,
      "grad_norm": 1.5876142978668213,
      "learning_rate": 0.00011932773109243697,
      "loss": 0.2995,
      "step": 2018
    },
    {
      "epoch": 4.038,
      "grad_norm": 2.0085721015930176,
      "learning_rate": 0.00011928771508603442,
      "loss": 0.2618,
      "step": 2019
    },
    {
      "epoch": 4.04,
      "grad_norm": 1.6961959600448608,
      "learning_rate": 0.00011924769907963188,
      "loss": 0.3275,
      "step": 2020
    },
    {
      "epoch": 4.042,
      "grad_norm": 1.7594022750854492,
      "learning_rate": 0.0001192076830732293,
      "loss": 0.2001,
      "step": 2021
    },
    {
      "epoch": 4.044,
      "grad_norm": 1.6084306240081787,
      "learning_rate": 0.00011916766706682674,
      "loss": 0.1903,
      "step": 2022
    },
    {
      "epoch": 4.046,
      "grad_norm": 1.513965368270874,
      "learning_rate": 0.00011912765106042419,
      "loss": 0.2096,
      "step": 2023
    },
    {
      "epoch": 4.048,
      "grad_norm": 1.9096009731292725,
      "learning_rate": 0.00011908763505402161,
      "loss": 0.2306,
      "step": 2024
    },
    {
      "epoch": 4.05,
      "grad_norm": 1.5335865020751953,
      "learning_rate": 0.00011904761904761905,
      "loss": 0.2447,
      "step": 2025
    },
    {
      "epoch": 4.052,
      "grad_norm": 1.9349308013916016,
      "learning_rate": 0.0001190076030412165,
      "loss": 0.2425,
      "step": 2026
    },
    {
      "epoch": 4.054,
      "grad_norm": 1.4577444791793823,
      "learning_rate": 0.00011896758703481392,
      "loss": 0.2396,
      "step": 2027
    },
    {
      "epoch": 4.056,
      "grad_norm": 1.383720874786377,
      "learning_rate": 0.00011892757102841137,
      "loss": 0.2541,
      "step": 2028
    },
    {
      "epoch": 4.058,
      "grad_norm": 1.8993971347808838,
      "learning_rate": 0.00011888755502200881,
      "loss": 0.1806,
      "step": 2029
    },
    {
      "epoch": 4.06,
      "grad_norm": 1.5060007572174072,
      "learning_rate": 0.00011884753901560626,
      "loss": 0.2392,
      "step": 2030
    },
    {
      "epoch": 4.062,
      "grad_norm": 1.3525310754776,
      "learning_rate": 0.00011880752300920368,
      "loss": 0.2822,
      "step": 2031
    },
    {
      "epoch": 4.064,
      "grad_norm": 1.3799973726272583,
      "learning_rate": 0.00011876750700280112,
      "loss": 0.2267,
      "step": 2032
    },
    {
      "epoch": 4.066,
      "grad_norm": 1.220215916633606,
      "learning_rate": 0.00011872749099639857,
      "loss": 0.1882,
      "step": 2033
    },
    {
      "epoch": 4.068,
      "grad_norm": 1.514207363128662,
      "learning_rate": 0.000118687474989996,
      "loss": 0.3308,
      "step": 2034
    },
    {
      "epoch": 4.07,
      "grad_norm": 2.100475311279297,
      "learning_rate": 0.00011864745898359345,
      "loss": 0.2985,
      "step": 2035
    },
    {
      "epoch": 4.072,
      "grad_norm": 1.4827765226364136,
      "learning_rate": 0.00011860744297719088,
      "loss": 0.2102,
      "step": 2036
    },
    {
      "epoch": 4.074,
      "grad_norm": 1.6277587413787842,
      "learning_rate": 0.00011856742697078833,
      "loss": 0.2471,
      "step": 2037
    },
    {
      "epoch": 4.076,
      "grad_norm": 0.923531711101532,
      "learning_rate": 0.00011852741096438576,
      "loss": 0.1923,
      "step": 2038
    },
    {
      "epoch": 4.078,
      "grad_norm": 1.0726419687271118,
      "learning_rate": 0.0001184873949579832,
      "loss": 0.2348,
      "step": 2039
    },
    {
      "epoch": 4.08,
      "grad_norm": 1.8810584545135498,
      "learning_rate": 0.00011844737895158065,
      "loss": 0.198,
      "step": 2040
    },
    {
      "epoch": 4.082,
      "grad_norm": 1.4510672092437744,
      "learning_rate": 0.00011840736294517807,
      "loss": 0.2261,
      "step": 2041
    },
    {
      "epoch": 4.084,
      "grad_norm": 1.4838616847991943,
      "learning_rate": 0.00011836734693877552,
      "loss": 0.2439,
      "step": 2042
    },
    {
      "epoch": 4.086,
      "grad_norm": 1.252100944519043,
      "learning_rate": 0.00011832733093237296,
      "loss": 0.2069,
      "step": 2043
    },
    {
      "epoch": 4.088,
      "grad_norm": 1.1669973134994507,
      "learning_rate": 0.00011828731492597038,
      "loss": 0.2119,
      "step": 2044
    },
    {
      "epoch": 4.09,
      "grad_norm": 1.2125591039657593,
      "learning_rate": 0.00011824729891956783,
      "loss": 0.2058,
      "step": 2045
    },
    {
      "epoch": 4.092,
      "grad_norm": 1.0934385061264038,
      "learning_rate": 0.00011820728291316527,
      "loss": 0.2053,
      "step": 2046
    },
    {
      "epoch": 4.094,
      "grad_norm": 1.080151915550232,
      "learning_rate": 0.00011816726690676272,
      "loss": 0.2023,
      "step": 2047
    },
    {
      "epoch": 4.096,
      "grad_norm": 1.4982603788375854,
      "learning_rate": 0.00011812725090036014,
      "loss": 0.2583,
      "step": 2048
    },
    {
      "epoch": 4.098,
      "grad_norm": 1.1554384231567383,
      "learning_rate": 0.00011808723489395759,
      "loss": 0.1862,
      "step": 2049
    },
    {
      "epoch": 4.1,
      "grad_norm": 1.0687854290008545,
      "learning_rate": 0.00011804721888755503,
      "loss": 0.1602,
      "step": 2050
    },
    {
      "epoch": 4.102,
      "grad_norm": 1.3915948867797852,
      "learning_rate": 0.00011800720288115245,
      "loss": 0.2377,
      "step": 2051
    },
    {
      "epoch": 4.104,
      "grad_norm": 1.5858073234558105,
      "learning_rate": 0.0001179671868747499,
      "loss": 0.2542,
      "step": 2052
    },
    {
      "epoch": 4.106,
      "grad_norm": 1.3515629768371582,
      "learning_rate": 0.00011792717086834734,
      "loss": 0.1998,
      "step": 2053
    },
    {
      "epoch": 4.108,
      "grad_norm": 1.0178900957107544,
      "learning_rate": 0.00011788715486194479,
      "loss": 0.2028,
      "step": 2054
    },
    {
      "epoch": 4.11,
      "grad_norm": 1.3660376071929932,
      "learning_rate": 0.00011784713885554222,
      "loss": 0.1886,
      "step": 2055
    },
    {
      "epoch": 4.112,
      "grad_norm": 1.6028305292129517,
      "learning_rate": 0.00011780712284913967,
      "loss": 0.2567,
      "step": 2056
    },
    {
      "epoch": 4.114,
      "grad_norm": 2.0176708698272705,
      "learning_rate": 0.0001177671068427371,
      "loss": 0.2203,
      "step": 2057
    },
    {
      "epoch": 4.116,
      "grad_norm": 1.034651756286621,
      "learning_rate": 0.00011772709083633453,
      "loss": 0.1845,
      "step": 2058
    },
    {
      "epoch": 4.118,
      "grad_norm": 1.6618645191192627,
      "learning_rate": 0.00011768707482993198,
      "loss": 0.2598,
      "step": 2059
    },
    {
      "epoch": 4.12,
      "grad_norm": 1.4717007875442505,
      "learning_rate": 0.00011764705882352942,
      "loss": 0.1973,
      "step": 2060
    },
    {
      "epoch": 4.122,
      "grad_norm": 1.9274992942810059,
      "learning_rate": 0.00011760704281712687,
      "loss": 0.2194,
      "step": 2061
    },
    {
      "epoch": 4.124,
      "grad_norm": 1.1255689859390259,
      "learning_rate": 0.00011756702681072429,
      "loss": 0.208,
      "step": 2062
    },
    {
      "epoch": 4.126,
      "grad_norm": 1.2505009174346924,
      "learning_rate": 0.00011752701080432174,
      "loss": 0.164,
      "step": 2063
    },
    {
      "epoch": 4.128,
      "grad_norm": 1.9178814888000488,
      "learning_rate": 0.00011748699479791918,
      "loss": 0.2588,
      "step": 2064
    },
    {
      "epoch": 4.13,
      "grad_norm": 1.5624908208847046,
      "learning_rate": 0.0001174469787915166,
      "loss": 0.2246,
      "step": 2065
    },
    {
      "epoch": 4.132,
      "grad_norm": 1.5621298551559448,
      "learning_rate": 0.00011740696278511405,
      "loss": 0.2214,
      "step": 2066
    },
    {
      "epoch": 4.134,
      "grad_norm": 1.348738670349121,
      "learning_rate": 0.00011736694677871149,
      "loss": 0.2194,
      "step": 2067
    },
    {
      "epoch": 4.136,
      "grad_norm": 1.231156826019287,
      "learning_rate": 0.00011732693077230891,
      "loss": 0.2181,
      "step": 2068
    },
    {
      "epoch": 4.138,
      "grad_norm": 1.954917073249817,
      "learning_rate": 0.00011728691476590636,
      "loss": 0.2359,
      "step": 2069
    },
    {
      "epoch": 4.14,
      "grad_norm": 1.702149510383606,
      "learning_rate": 0.00011724689875950381,
      "loss": 0.2403,
      "step": 2070
    },
    {
      "epoch": 4.142,
      "grad_norm": 1.1713258028030396,
      "learning_rate": 0.00011720688275310125,
      "loss": 0.2106,
      "step": 2071
    },
    {
      "epoch": 4.144,
      "grad_norm": 1.2978347539901733,
      "learning_rate": 0.00011716686674669867,
      "loss": 0.1864,
      "step": 2072
    },
    {
      "epoch": 4.146,
      "grad_norm": 1.43488609790802,
      "learning_rate": 0.00011712685074029612,
      "loss": 0.2726,
      "step": 2073
    },
    {
      "epoch": 4.148,
      "grad_norm": 1.3195409774780273,
      "learning_rate": 0.00011708683473389358,
      "loss": 0.2202,
      "step": 2074
    },
    {
      "epoch": 4.15,
      "grad_norm": 1.377856969833374,
      "learning_rate": 0.00011704681872749099,
      "loss": 0.2803,
      "step": 2075
    },
    {
      "epoch": 4.152,
      "grad_norm": 1.2192384004592896,
      "learning_rate": 0.00011700680272108844,
      "loss": 0.2251,
      "step": 2076
    },
    {
      "epoch": 4.154,
      "grad_norm": 1.198432207107544,
      "learning_rate": 0.00011696678671468589,
      "loss": 0.2143,
      "step": 2077
    },
    {
      "epoch": 4.156,
      "grad_norm": 0.8090105056762695,
      "learning_rate": 0.00011692677070828332,
      "loss": 0.1737,
      "step": 2078
    },
    {
      "epoch": 4.158,
      "grad_norm": 1.133512020111084,
      "learning_rate": 0.00011688675470188075,
      "loss": 0.1883,
      "step": 2079
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.7930982112884521,
      "learning_rate": 0.0001168467386954782,
      "loss": 0.1727,
      "step": 2080
    },
    {
      "epoch": 4.162,
      "grad_norm": 1.1674084663391113,
      "learning_rate": 0.00011680672268907565,
      "loss": 0.2181,
      "step": 2081
    },
    {
      "epoch": 4.164,
      "grad_norm": 1.6298091411590576,
      "learning_rate": 0.00011676670668267307,
      "loss": 0.2439,
      "step": 2082
    },
    {
      "epoch": 4.166,
      "grad_norm": 1.8205302953720093,
      "learning_rate": 0.00011672669067627051,
      "loss": 0.2907,
      "step": 2083
    },
    {
      "epoch": 4.168,
      "grad_norm": 1.5060901641845703,
      "learning_rate": 0.00011668667466986796,
      "loss": 0.2432,
      "step": 2084
    },
    {
      "epoch": 4.17,
      "grad_norm": 1.2430561780929565,
      "learning_rate": 0.0001166466586634654,
      "loss": 0.203,
      "step": 2085
    },
    {
      "epoch": 4.172,
      "grad_norm": 1.5962491035461426,
      "learning_rate": 0.00011660664265706282,
      "loss": 0.2631,
      "step": 2086
    },
    {
      "epoch": 4.174,
      "grad_norm": 1.6678187847137451,
      "learning_rate": 0.00011656662665066027,
      "loss": 0.2172,
      "step": 2087
    },
    {
      "epoch": 4.176,
      "grad_norm": 1.1394062042236328,
      "learning_rate": 0.00011652661064425772,
      "loss": 0.1829,
      "step": 2088
    },
    {
      "epoch": 4.178,
      "grad_norm": 1.09774649143219,
      "learning_rate": 0.00011648659463785515,
      "loss": 0.2026,
      "step": 2089
    },
    {
      "epoch": 4.18,
      "grad_norm": 2.1315853595733643,
      "learning_rate": 0.00011644657863145258,
      "loss": 0.2482,
      "step": 2090
    },
    {
      "epoch": 4.182,
      "grad_norm": 1.6214861869812012,
      "learning_rate": 0.00011640656262505003,
      "loss": 0.2148,
      "step": 2091
    },
    {
      "epoch": 4.184,
      "grad_norm": 1.3430564403533936,
      "learning_rate": 0.00011636654661864746,
      "loss": 0.2143,
      "step": 2092
    },
    {
      "epoch": 4.186,
      "grad_norm": 1.337826132774353,
      "learning_rate": 0.0001163265306122449,
      "loss": 0.1808,
      "step": 2093
    },
    {
      "epoch": 4.188,
      "grad_norm": 1.7977724075317383,
      "learning_rate": 0.00011628651460584235,
      "loss": 0.2304,
      "step": 2094
    },
    {
      "epoch": 4.19,
      "grad_norm": 1.3882112503051758,
      "learning_rate": 0.0001162464985994398,
      "loss": 0.2608,
      "step": 2095
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.9333101511001587,
      "learning_rate": 0.00011620648259303722,
      "loss": 0.1528,
      "step": 2096
    },
    {
      "epoch": 4.194,
      "grad_norm": 1.0873053073883057,
      "learning_rate": 0.00011616646658663466,
      "loss": 0.2073,
      "step": 2097
    },
    {
      "epoch": 4.196,
      "grad_norm": 1.3752228021621704,
      "learning_rate": 0.00011612645058023211,
      "loss": 0.2351,
      "step": 2098
    },
    {
      "epoch": 4.198,
      "grad_norm": 1.4911025762557983,
      "learning_rate": 0.00011608643457382953,
      "loss": 0.2614,
      "step": 2099
    },
    {
      "epoch": 4.2,
      "grad_norm": 1.435056209564209,
      "learning_rate": 0.00011604641856742697,
      "loss": 0.2381,
      "step": 2100
    },
    {
      "epoch": 4.202,
      "grad_norm": 1.5221890211105347,
      "learning_rate": 0.00011600640256102442,
      "loss": 0.2625,
      "step": 2101
    },
    {
      "epoch": 4.204,
      "grad_norm": 1.245976209640503,
      "learning_rate": 0.00011596638655462187,
      "loss": 0.2589,
      "step": 2102
    },
    {
      "epoch": 4.206,
      "grad_norm": 1.5897276401519775,
      "learning_rate": 0.00011592637054821929,
      "loss": 0.2455,
      "step": 2103
    },
    {
      "epoch": 4.208,
      "grad_norm": 1.379425287246704,
      "learning_rate": 0.00011588635454181673,
      "loss": 0.565,
      "step": 2104
    },
    {
      "epoch": 4.21,
      "grad_norm": 1.5061588287353516,
      "learning_rate": 0.00011584633853541418,
      "loss": 0.2407,
      "step": 2105
    },
    {
      "epoch": 4.212,
      "grad_norm": 1.104042410850525,
      "learning_rate": 0.0001158063225290116,
      "loss": 0.2034,
      "step": 2106
    },
    {
      "epoch": 4.214,
      "grad_norm": 1.5643796920776367,
      "learning_rate": 0.00011576630652260904,
      "loss": 0.2406,
      "step": 2107
    },
    {
      "epoch": 4.216,
      "grad_norm": 1.7806885242462158,
      "learning_rate": 0.00011572629051620649,
      "loss": 0.218,
      "step": 2108
    },
    {
      "epoch": 4.218,
      "grad_norm": 1.1995962858200073,
      "learning_rate": 0.00011568627450980394,
      "loss": 0.238,
      "step": 2109
    },
    {
      "epoch": 4.22,
      "grad_norm": 1.3928468227386475,
      "learning_rate": 0.00011564625850340137,
      "loss": 0.1819,
      "step": 2110
    },
    {
      "epoch": 4.222,
      "grad_norm": 1.0791151523590088,
      "learning_rate": 0.0001156062424969988,
      "loss": 0.1855,
      "step": 2111
    },
    {
      "epoch": 4.224,
      "grad_norm": 1.12618887424469,
      "learning_rate": 0.00011556622649059625,
      "loss": 0.1964,
      "step": 2112
    },
    {
      "epoch": 4.226,
      "grad_norm": 0.9037773609161377,
      "learning_rate": 0.00011552621048419368,
      "loss": 0.169,
      "step": 2113
    },
    {
      "epoch": 4.228,
      "grad_norm": 1.6791945695877075,
      "learning_rate": 0.00011548619447779112,
      "loss": 0.2067,
      "step": 2114
    },
    {
      "epoch": 4.23,
      "grad_norm": 1.6688677072525024,
      "learning_rate": 0.00011544617847138857,
      "loss": 0.2838,
      "step": 2115
    },
    {
      "epoch": 4.232,
      "grad_norm": 1.3392910957336426,
      "learning_rate": 0.00011540616246498599,
      "loss": 0.2184,
      "step": 2116
    },
    {
      "epoch": 4.234,
      "grad_norm": 1.6031495332717896,
      "learning_rate": 0.00011536614645858344,
      "loss": 0.2579,
      "step": 2117
    },
    {
      "epoch": 4.236,
      "grad_norm": 1.4519503116607666,
      "learning_rate": 0.00011532613045218088,
      "loss": 0.2499,
      "step": 2118
    },
    {
      "epoch": 4.2379999999999995,
      "grad_norm": 1.2642072439193726,
      "learning_rate": 0.00011528611444577833,
      "loss": 0.1877,
      "step": 2119
    },
    {
      "epoch": 4.24,
      "grad_norm": 1.2550820112228394,
      "learning_rate": 0.00011524609843937575,
      "loss": 0.2055,
      "step": 2120
    },
    {
      "epoch": 4.242,
      "grad_norm": 1.8614568710327148,
      "learning_rate": 0.00011520608243297319,
      "loss": 0.302,
      "step": 2121
    },
    {
      "epoch": 4.244,
      "grad_norm": 1.5881465673446655,
      "learning_rate": 0.00011516606642657064,
      "loss": 0.2649,
      "step": 2122
    },
    {
      "epoch": 4.246,
      "grad_norm": 1.7070072889328003,
      "learning_rate": 0.00011512605042016806,
      "loss": 0.2586,
      "step": 2123
    },
    {
      "epoch": 4.248,
      "grad_norm": 1.052453875541687,
      "learning_rate": 0.00011508603441376551,
      "loss": 0.1877,
      "step": 2124
    },
    {
      "epoch": 4.25,
      "grad_norm": 1.3145465850830078,
      "learning_rate": 0.00011504601840736295,
      "loss": 0.22,
      "step": 2125
    },
    {
      "epoch": 4.252,
      "grad_norm": 1.0561845302581787,
      "learning_rate": 0.0001150060024009604,
      "loss": 0.1978,
      "step": 2126
    },
    {
      "epoch": 4.254,
      "grad_norm": 1.1281331777572632,
      "learning_rate": 0.00011496598639455783,
      "loss": 0.169,
      "step": 2127
    },
    {
      "epoch": 4.256,
      "grad_norm": 1.309763789176941,
      "learning_rate": 0.00011492597038815526,
      "loss": 0.2324,
      "step": 2128
    },
    {
      "epoch": 4.258,
      "grad_norm": 1.2680728435516357,
      "learning_rate": 0.00011488595438175271,
      "loss": 0.2591,
      "step": 2129
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.9577030539512634,
      "learning_rate": 0.00011484593837535014,
      "loss": 0.179,
      "step": 2130
    },
    {
      "epoch": 4.2620000000000005,
      "grad_norm": 1.0930107831954956,
      "learning_rate": 0.00011480592236894759,
      "loss": 0.2011,
      "step": 2131
    },
    {
      "epoch": 4.264,
      "grad_norm": 1.194766640663147,
      "learning_rate": 0.00011476590636254502,
      "loss": 0.221,
      "step": 2132
    },
    {
      "epoch": 4.266,
      "grad_norm": 1.30433988571167,
      "learning_rate": 0.00011472589035614245,
      "loss": 0.2266,
      "step": 2133
    },
    {
      "epoch": 4.268,
      "grad_norm": 0.8553952574729919,
      "learning_rate": 0.0001146858743497399,
      "loss": 0.215,
      "step": 2134
    },
    {
      "epoch": 4.27,
      "grad_norm": 1.1637755632400513,
      "learning_rate": 0.00011464585834333734,
      "loss": 0.1425,
      "step": 2135
    },
    {
      "epoch": 4.272,
      "grad_norm": 1.5604287385940552,
      "learning_rate": 0.00011460584233693479,
      "loss": 0.2505,
      "step": 2136
    },
    {
      "epoch": 4.274,
      "grad_norm": 1.39583420753479,
      "learning_rate": 0.00011456582633053221,
      "loss": 0.2437,
      "step": 2137
    },
    {
      "epoch": 4.276,
      "grad_norm": 2.2783830165863037,
      "learning_rate": 0.00011452581032412966,
      "loss": 0.2976,
      "step": 2138
    },
    {
      "epoch": 4.2780000000000005,
      "grad_norm": 1.6349444389343262,
      "learning_rate": 0.0001144857943177271,
      "loss": 0.2382,
      "step": 2139
    },
    {
      "epoch": 4.28,
      "grad_norm": 1.7056479454040527,
      "learning_rate": 0.00011444577831132452,
      "loss": 0.2812,
      "step": 2140
    },
    {
      "epoch": 4.282,
      "grad_norm": 1.6432727575302124,
      "learning_rate": 0.00011440576230492197,
      "loss": 0.2194,
      "step": 2141
    },
    {
      "epoch": 4.284,
      "grad_norm": 3.228670120239258,
      "learning_rate": 0.00011436574629851942,
      "loss": 0.2576,
      "step": 2142
    },
    {
      "epoch": 4.286,
      "grad_norm": 1.7200449705123901,
      "learning_rate": 0.00011432573029211686,
      "loss": 0.2328,
      "step": 2143
    },
    {
      "epoch": 4.288,
      "grad_norm": 1.5220365524291992,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.2029,
      "step": 2144
    },
    {
      "epoch": 4.29,
      "grad_norm": 1.3984655141830444,
      "learning_rate": 0.00011424569827931173,
      "loss": 0.2432,
      "step": 2145
    },
    {
      "epoch": 4.292,
      "grad_norm": 1.7235065698623657,
      "learning_rate": 0.00011420568227290917,
      "loss": 0.2905,
      "step": 2146
    },
    {
      "epoch": 4.294,
      "grad_norm": 1.6706287860870361,
      "learning_rate": 0.0001141656662665066,
      "loss": 0.2239,
      "step": 2147
    },
    {
      "epoch": 4.296,
      "grad_norm": 1.219423770904541,
      "learning_rate": 0.00011412565026010405,
      "loss": 0.2728,
      "step": 2148
    },
    {
      "epoch": 4.298,
      "grad_norm": 1.1204606294631958,
      "learning_rate": 0.0001140856342537015,
      "loss": 0.2306,
      "step": 2149
    },
    {
      "epoch": 4.3,
      "grad_norm": 1.5635868310928345,
      "learning_rate": 0.00011404561824729893,
      "loss": 0.2733,
      "step": 2150
    },
    {
      "epoch": 4.302,
      "grad_norm": 1.1804622411727905,
      "learning_rate": 0.00011400560224089636,
      "loss": 0.2181,
      "step": 2151
    },
    {
      "epoch": 4.304,
      "grad_norm": 1.9618752002716064,
      "learning_rate": 0.00011396558623449381,
      "loss": 0.2273,
      "step": 2152
    },
    {
      "epoch": 4.306,
      "grad_norm": 1.3775770664215088,
      "learning_rate": 0.00011392557022809124,
      "loss": 0.2179,
      "step": 2153
    },
    {
      "epoch": 4.308,
      "grad_norm": 1.2097891569137573,
      "learning_rate": 0.00011388555422168867,
      "loss": 0.2066,
      "step": 2154
    },
    {
      "epoch": 4.31,
      "grad_norm": 1.1043813228607178,
      "learning_rate": 0.00011384553821528612,
      "loss": 0.218,
      "step": 2155
    },
    {
      "epoch": 4.312,
      "grad_norm": 1.3551418781280518,
      "learning_rate": 0.00011380552220888357,
      "loss": 0.222,
      "step": 2156
    },
    {
      "epoch": 4.314,
      "grad_norm": 1.9697937965393066,
      "learning_rate": 0.000113765506202481,
      "loss": 0.2482,
      "step": 2157
    },
    {
      "epoch": 4.316,
      "grad_norm": 1.1247938871383667,
      "learning_rate": 0.00011372549019607843,
      "loss": 0.2052,
      "step": 2158
    },
    {
      "epoch": 4.318,
      "grad_norm": 1.3211758136749268,
      "learning_rate": 0.00011368547418967588,
      "loss": 0.2258,
      "step": 2159
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.6420836448669434,
      "learning_rate": 0.00011364545818327332,
      "loss": 0.2659,
      "step": 2160
    },
    {
      "epoch": 4.322,
      "grad_norm": 1.2766797542572021,
      "learning_rate": 0.00011360544217687074,
      "loss": 0.2254,
      "step": 2161
    },
    {
      "epoch": 4.324,
      "grad_norm": 1.6584858894348145,
      "learning_rate": 0.00011356542617046819,
      "loss": 0.2126,
      "step": 2162
    },
    {
      "epoch": 4.326,
      "grad_norm": 1.6599838733673096,
      "learning_rate": 0.00011352541016406564,
      "loss": 0.2791,
      "step": 2163
    },
    {
      "epoch": 4.328,
      "grad_norm": 1.3494654893875122,
      "learning_rate": 0.00011348539415766307,
      "loss": 0.2012,
      "step": 2164
    },
    {
      "epoch": 4.33,
      "grad_norm": 1.8535507917404175,
      "learning_rate": 0.0001134453781512605,
      "loss": 0.2688,
      "step": 2165
    },
    {
      "epoch": 4.332,
      "grad_norm": 1.3546652793884277,
      "learning_rate": 0.00011340536214485795,
      "loss": 0.2389,
      "step": 2166
    },
    {
      "epoch": 4.334,
      "grad_norm": 1.570008397102356,
      "learning_rate": 0.00011336534613845539,
      "loss": 0.2434,
      "step": 2167
    },
    {
      "epoch": 4.336,
      "grad_norm": 1.6942880153656006,
      "learning_rate": 0.00011332533013205282,
      "loss": 0.2173,
      "step": 2168
    },
    {
      "epoch": 4.338,
      "grad_norm": 1.2948843240737915,
      "learning_rate": 0.00011328531412565027,
      "loss": 0.2055,
      "step": 2169
    },
    {
      "epoch": 4.34,
      "grad_norm": 1.3212116956710815,
      "learning_rate": 0.00011324529811924772,
      "loss": 0.2342,
      "step": 2170
    },
    {
      "epoch": 4.342,
      "grad_norm": 1.8324066400527954,
      "learning_rate": 0.00011320528211284514,
      "loss": 0.2233,
      "step": 2171
    },
    {
      "epoch": 4.344,
      "grad_norm": 1.3992252349853516,
      "learning_rate": 0.00011316526610644258,
      "loss": 0.2386,
      "step": 2172
    },
    {
      "epoch": 4.346,
      "grad_norm": 1.941870927810669,
      "learning_rate": 0.00011312525010004003,
      "loss": 0.3455,
      "step": 2173
    },
    {
      "epoch": 4.348,
      "grad_norm": 1.4365699291229248,
      "learning_rate": 0.00011308523409363747,
      "loss": 0.2475,
      "step": 2174
    },
    {
      "epoch": 4.35,
      "grad_norm": 1.3612056970596313,
      "learning_rate": 0.00011304521808723489,
      "loss": 0.2613,
      "step": 2175
    },
    {
      "epoch": 4.352,
      "grad_norm": 1.1509886980056763,
      "learning_rate": 0.00011300520208083234,
      "loss": 0.2491,
      "step": 2176
    },
    {
      "epoch": 4.354,
      "grad_norm": 1.2729771137237549,
      "learning_rate": 0.00011296518607442979,
      "loss": 0.1988,
      "step": 2177
    },
    {
      "epoch": 4.356,
      "grad_norm": 1.2126970291137695,
      "learning_rate": 0.00011292517006802721,
      "loss": 0.1683,
      "step": 2178
    },
    {
      "epoch": 4.358,
      "grad_norm": 1.0435904264450073,
      "learning_rate": 0.00011288515406162465,
      "loss": 0.2196,
      "step": 2179
    },
    {
      "epoch": 4.36,
      "grad_norm": 1.0350230932235718,
      "learning_rate": 0.0001128451380552221,
      "loss": 0.1941,
      "step": 2180
    },
    {
      "epoch": 4.362,
      "grad_norm": 0.7158880829811096,
      "learning_rate": 0.00011280512204881953,
      "loss": 0.1585,
      "step": 2181
    },
    {
      "epoch": 4.364,
      "grad_norm": 1.7156442403793335,
      "learning_rate": 0.00011276510604241696,
      "loss": 0.2464,
      "step": 2182
    },
    {
      "epoch": 4.366,
      "grad_norm": 1.4493509531021118,
      "learning_rate": 0.00011272509003601441,
      "loss": 0.3162,
      "step": 2183
    },
    {
      "epoch": 4.368,
      "grad_norm": 1.5761724710464478,
      "learning_rate": 0.00011268507402961186,
      "loss": 0.2674,
      "step": 2184
    },
    {
      "epoch": 4.37,
      "grad_norm": 1.1961332559585571,
      "learning_rate": 0.00011264505802320929,
      "loss": 0.2034,
      "step": 2185
    },
    {
      "epoch": 4.372,
      "grad_norm": 1.366637945175171,
      "learning_rate": 0.00011260504201680672,
      "loss": 0.2559,
      "step": 2186
    },
    {
      "epoch": 4.374,
      "grad_norm": 1.5071264505386353,
      "learning_rate": 0.00011256502601040418,
      "loss": 0.2549,
      "step": 2187
    },
    {
      "epoch": 4.376,
      "grad_norm": 1.3999654054641724,
      "learning_rate": 0.0001125250100040016,
      "loss": 0.2648,
      "step": 2188
    },
    {
      "epoch": 4.378,
      "grad_norm": 1.1649755239486694,
      "learning_rate": 0.00011248499399759904,
      "loss": 0.2613,
      "step": 2189
    },
    {
      "epoch": 4.38,
      "grad_norm": 1.7592240571975708,
      "learning_rate": 0.00011244497799119649,
      "loss": 0.2431,
      "step": 2190
    },
    {
      "epoch": 4.382,
      "grad_norm": 1.6382962465286255,
      "learning_rate": 0.00011240496198479394,
      "loss": 0.2493,
      "step": 2191
    },
    {
      "epoch": 4.384,
      "grad_norm": 1.3610858917236328,
      "learning_rate": 0.00011236494597839136,
      "loss": 0.2331,
      "step": 2192
    },
    {
      "epoch": 4.386,
      "grad_norm": 1.3065946102142334,
      "learning_rate": 0.0001123249299719888,
      "loss": 0.2248,
      "step": 2193
    },
    {
      "epoch": 4.388,
      "grad_norm": 1.7722972631454468,
      "learning_rate": 0.00011228491396558625,
      "loss": 0.2591,
      "step": 2194
    },
    {
      "epoch": 4.39,
      "grad_norm": 1.3337208032608032,
      "learning_rate": 0.00011224489795918367,
      "loss": 0.2552,
      "step": 2195
    },
    {
      "epoch": 4.392,
      "grad_norm": 2.384047031402588,
      "learning_rate": 0.00011220488195278111,
      "loss": 0.3423,
      "step": 2196
    },
    {
      "epoch": 4.394,
      "grad_norm": 1.1950043439865112,
      "learning_rate": 0.00011216486594637856,
      "loss": 0.214,
      "step": 2197
    },
    {
      "epoch": 4.396,
      "grad_norm": 1.3633856773376465,
      "learning_rate": 0.00011212484993997601,
      "loss": 0.2067,
      "step": 2198
    },
    {
      "epoch": 4.398,
      "grad_norm": 1.5724846124649048,
      "learning_rate": 0.00011208483393357343,
      "loss": 0.257,
      "step": 2199
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.485039234161377,
      "learning_rate": 0.00011204481792717087,
      "loss": 0.2969,
      "step": 2200
    },
    {
      "epoch": 4.402,
      "grad_norm": 1.8500914573669434,
      "learning_rate": 0.00011200480192076832,
      "loss": 0.299,
      "step": 2201
    },
    {
      "epoch": 4.404,
      "grad_norm": 1.6063449382781982,
      "learning_rate": 0.00011196478591436575,
      "loss": 0.3654,
      "step": 2202
    },
    {
      "epoch": 4.406,
      "grad_norm": 1.2692031860351562,
      "learning_rate": 0.00011192476990796318,
      "loss": 0.2094,
      "step": 2203
    },
    {
      "epoch": 4.408,
      "grad_norm": 1.2807235717773438,
      "learning_rate": 0.00011188475390156063,
      "loss": 0.2475,
      "step": 2204
    },
    {
      "epoch": 4.41,
      "grad_norm": 1.4553425312042236,
      "learning_rate": 0.00011184473789515806,
      "loss": 0.2874,
      "step": 2205
    },
    {
      "epoch": 4.412,
      "grad_norm": 1.3547605276107788,
      "learning_rate": 0.00011180472188875551,
      "loss": 0.2171,
      "step": 2206
    },
    {
      "epoch": 4.414,
      "grad_norm": 1.4901002645492554,
      "learning_rate": 0.00011176470588235294,
      "loss": 0.3199,
      "step": 2207
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.9907261729240417,
      "learning_rate": 0.0001117246898759504,
      "loss": 0.2132,
      "step": 2208
    },
    {
      "epoch": 4.418,
      "grad_norm": 1.6079970598220825,
      "learning_rate": 0.00011168467386954782,
      "loss": 0.3081,
      "step": 2209
    },
    {
      "epoch": 4.42,
      "grad_norm": 1.328041434288025,
      "learning_rate": 0.00011164465786314527,
      "loss": 0.2526,
      "step": 2210
    },
    {
      "epoch": 4.422,
      "grad_norm": 1.7326587438583374,
      "learning_rate": 0.00011160464185674271,
      "loss": 0.3341,
      "step": 2211
    },
    {
      "epoch": 4.424,
      "grad_norm": 1.4903510808944702,
      "learning_rate": 0.00011156462585034013,
      "loss": 0.2696,
      "step": 2212
    },
    {
      "epoch": 4.426,
      "grad_norm": 1.141760230064392,
      "learning_rate": 0.00011152460984393758,
      "loss": 0.191,
      "step": 2213
    },
    {
      "epoch": 4.428,
      "grad_norm": 1.9185909032821655,
      "learning_rate": 0.00011148459383753502,
      "loss": 0.3098,
      "step": 2214
    },
    {
      "epoch": 4.43,
      "grad_norm": 1.2871078252792358,
      "learning_rate": 0.00011144457783113247,
      "loss": 0.3017,
      "step": 2215
    },
    {
      "epoch": 4.432,
      "grad_norm": 1.2965506315231323,
      "learning_rate": 0.00011140456182472989,
      "loss": 0.2808,
      "step": 2216
    },
    {
      "epoch": 4.434,
      "grad_norm": 1.0970525741577148,
      "learning_rate": 0.00011136454581832734,
      "loss": 0.2047,
      "step": 2217
    },
    {
      "epoch": 4.436,
      "grad_norm": 1.212965488433838,
      "learning_rate": 0.00011132452981192478,
      "loss": 0.2794,
      "step": 2218
    },
    {
      "epoch": 4.438,
      "grad_norm": 1.2595691680908203,
      "learning_rate": 0.0001112845138055222,
      "loss": 0.2059,
      "step": 2219
    },
    {
      "epoch": 4.44,
      "grad_norm": 1.003523349761963,
      "learning_rate": 0.00011124449779911965,
      "loss": 0.1737,
      "step": 2220
    },
    {
      "epoch": 4.442,
      "grad_norm": 1.192071557044983,
      "learning_rate": 0.00011120448179271709,
      "loss": 0.2501,
      "step": 2221
    },
    {
      "epoch": 4.444,
      "grad_norm": 1.1210956573486328,
      "learning_rate": 0.00011116446578631454,
      "loss": 0.242,
      "step": 2222
    },
    {
      "epoch": 4.446,
      "grad_norm": 1.1294373273849487,
      "learning_rate": 0.00011112444977991197,
      "loss": 0.2265,
      "step": 2223
    },
    {
      "epoch": 4.448,
      "grad_norm": 1.7978558540344238,
      "learning_rate": 0.00011108443377350942,
      "loss": 0.2538,
      "step": 2224
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.9884766936302185,
      "learning_rate": 0.00011104441776710685,
      "loss": 0.2357,
      "step": 2225
    },
    {
      "epoch": 4.452,
      "grad_norm": 1.1590402126312256,
      "learning_rate": 0.00011100440176070428,
      "loss": 0.2197,
      "step": 2226
    },
    {
      "epoch": 4.454,
      "grad_norm": 1.138434648513794,
      "learning_rate": 0.00011096438575430173,
      "loss": 0.2132,
      "step": 2227
    },
    {
      "epoch": 4.456,
      "grad_norm": 1.0987735986709595,
      "learning_rate": 0.00011092436974789917,
      "loss": 0.1706,
      "step": 2228
    },
    {
      "epoch": 4.458,
      "grad_norm": 1.1551122665405273,
      "learning_rate": 0.00011088435374149659,
      "loss": 0.2242,
      "step": 2229
    },
    {
      "epoch": 4.46,
      "grad_norm": 1.0840362310409546,
      "learning_rate": 0.00011084433773509404,
      "loss": 0.2078,
      "step": 2230
    },
    {
      "epoch": 4.462,
      "grad_norm": 1.7721474170684814,
      "learning_rate": 0.00011080432172869149,
      "loss": 0.2588,
      "step": 2231
    },
    {
      "epoch": 4.464,
      "grad_norm": 2.1468780040740967,
      "learning_rate": 0.00011076430572228893,
      "loss": 0.252,
      "step": 2232
    },
    {
      "epoch": 4.466,
      "grad_norm": 1.99006986618042,
      "learning_rate": 0.00011072428971588635,
      "loss": 0.2749,
      "step": 2233
    },
    {
      "epoch": 4.468,
      "grad_norm": 1.5596654415130615,
      "learning_rate": 0.0001106842737094838,
      "loss": 0.2954,
      "step": 2234
    },
    {
      "epoch": 4.47,
      "grad_norm": 1.4754225015640259,
      "learning_rate": 0.00011064425770308124,
      "loss": 0.2307,
      "step": 2235
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 1.1995598077774048,
      "learning_rate": 0.00011060424169667866,
      "loss": 0.1763,
      "step": 2236
    },
    {
      "epoch": 4.474,
      "grad_norm": 1.7576203346252441,
      "learning_rate": 0.00011056422569027611,
      "loss": 0.2584,
      "step": 2237
    },
    {
      "epoch": 4.476,
      "grad_norm": 1.454859733581543,
      "learning_rate": 0.00011052420968387356,
      "loss": 0.2289,
      "step": 2238
    },
    {
      "epoch": 4.478,
      "grad_norm": 1.192014455795288,
      "learning_rate": 0.000110484193677471,
      "loss": 0.2704,
      "step": 2239
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.751849889755249,
      "learning_rate": 0.00011044417767106842,
      "loss": 0.1977,
      "step": 2240
    },
    {
      "epoch": 4.482,
      "grad_norm": 1.3317654132843018,
      "learning_rate": 0.00011040416166466588,
      "loss": 0.207,
      "step": 2241
    },
    {
      "epoch": 4.484,
      "grad_norm": 1.5240321159362793,
      "learning_rate": 0.00011036414565826331,
      "loss": 0.2425,
      "step": 2242
    },
    {
      "epoch": 4.486,
      "grad_norm": 1.4540517330169678,
      "learning_rate": 0.00011032412965186074,
      "loss": 0.2581,
      "step": 2243
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 1.3115538358688354,
      "learning_rate": 0.00011028411364545819,
      "loss": 0.2288,
      "step": 2244
    },
    {
      "epoch": 4.49,
      "grad_norm": 1.328007698059082,
      "learning_rate": 0.00011024409763905564,
      "loss": 0.2239,
      "step": 2245
    },
    {
      "epoch": 4.492,
      "grad_norm": 1.4116666316986084,
      "learning_rate": 0.00011020408163265306,
      "loss": 0.1943,
      "step": 2246
    },
    {
      "epoch": 4.494,
      "grad_norm": 2.046070098876953,
      "learning_rate": 0.0001101640656262505,
      "loss": 0.3109,
      "step": 2247
    },
    {
      "epoch": 4.496,
      "grad_norm": 1.4613993167877197,
      "learning_rate": 0.00011012404961984795,
      "loss": 0.2157,
      "step": 2248
    },
    {
      "epoch": 4.498,
      "grad_norm": 1.4335389137268066,
      "learning_rate": 0.00011008403361344539,
      "loss": 0.2168,
      "step": 2249
    },
    {
      "epoch": 4.5,
      "grad_norm": 1.6589734554290771,
      "learning_rate": 0.00011004401760704281,
      "loss": 0.2412,
      "step": 2250
    },
    {
      "epoch": 4.502,
      "grad_norm": 1.7126808166503906,
      "learning_rate": 0.00011000400160064026,
      "loss": 0.2156,
      "step": 2251
    },
    {
      "epoch": 4.504,
      "grad_norm": 2.6912214756011963,
      "learning_rate": 0.00010996398559423771,
      "loss": 0.2116,
      "step": 2252
    },
    {
      "epoch": 4.506,
      "grad_norm": 1.2492517232894897,
      "learning_rate": 0.00010992396958783513,
      "loss": 0.2463,
      "step": 2253
    },
    {
      "epoch": 4.508,
      "grad_norm": 1.1634215116500854,
      "learning_rate": 0.00010988395358143257,
      "loss": 0.2307,
      "step": 2254
    },
    {
      "epoch": 4.51,
      "grad_norm": 1.2994952201843262,
      "learning_rate": 0.00010984393757503002,
      "loss": 0.2392,
      "step": 2255
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 1.9204151630401611,
      "learning_rate": 0.00010980392156862746,
      "loss": 0.2689,
      "step": 2256
    },
    {
      "epoch": 4.514,
      "grad_norm": 1.575927734375,
      "learning_rate": 0.00010976390556222488,
      "loss": 0.2078,
      "step": 2257
    },
    {
      "epoch": 4.516,
      "grad_norm": 1.5599322319030762,
      "learning_rate": 0.00010972388955582233,
      "loss": 0.1893,
      "step": 2258
    },
    {
      "epoch": 4.518,
      "grad_norm": 1.4451923370361328,
      "learning_rate": 0.00010968387354941978,
      "loss": 0.2267,
      "step": 2259
    },
    {
      "epoch": 4.52,
      "grad_norm": 1.274612545967102,
      "learning_rate": 0.00010964385754301721,
      "loss": 0.2218,
      "step": 2260
    },
    {
      "epoch": 4.522,
      "grad_norm": 1.2113581895828247,
      "learning_rate": 0.00010960384153661465,
      "loss": 0.212,
      "step": 2261
    },
    {
      "epoch": 4.524,
      "grad_norm": 1.7602540254592896,
      "learning_rate": 0.0001095638255302121,
      "loss": 0.2701,
      "step": 2262
    },
    {
      "epoch": 4.526,
      "grad_norm": 0.9528367519378662,
      "learning_rate": 0.00010952380952380953,
      "loss": 0.1682,
      "step": 2263
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 1.2033063173294067,
      "learning_rate": 0.00010948379351740696,
      "loss": 0.1926,
      "step": 2264
    },
    {
      "epoch": 4.53,
      "grad_norm": 1.4427870512008667,
      "learning_rate": 0.00010944377751100441,
      "loss": 0.2851,
      "step": 2265
    },
    {
      "epoch": 4.532,
      "grad_norm": 1.2168703079223633,
      "learning_rate": 0.00010940376150460186,
      "loss": 0.2111,
      "step": 2266
    },
    {
      "epoch": 4.534,
      "grad_norm": 1.1522880792617798,
      "learning_rate": 0.00010936374549819928,
      "loss": 0.1838,
      "step": 2267
    },
    {
      "epoch": 4.536,
      "grad_norm": 1.24254310131073,
      "learning_rate": 0.00010932372949179672,
      "loss": 0.2212,
      "step": 2268
    },
    {
      "epoch": 4.538,
      "grad_norm": 1.6740494966506958,
      "learning_rate": 0.00010928371348539417,
      "loss": 0.2798,
      "step": 2269
    },
    {
      "epoch": 4.54,
      "grad_norm": 1.3205041885375977,
      "learning_rate": 0.00010924369747899159,
      "loss": 0.2129,
      "step": 2270
    },
    {
      "epoch": 4.542,
      "grad_norm": 1.907870888710022,
      "learning_rate": 0.00010920368147258903,
      "loss": 0.3599,
      "step": 2271
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 1.253467321395874,
      "learning_rate": 0.00010916366546618648,
      "loss": 0.2306,
      "step": 2272
    },
    {
      "epoch": 4.546,
      "grad_norm": 1.8319817781448364,
      "learning_rate": 0.00010912364945978393,
      "loss": 0.2632,
      "step": 2273
    },
    {
      "epoch": 4.548,
      "grad_norm": 1.5315812826156616,
      "learning_rate": 0.00010908363345338135,
      "loss": 0.2661,
      "step": 2274
    },
    {
      "epoch": 4.55,
      "grad_norm": 1.2860499620437622,
      "learning_rate": 0.00010904361744697879,
      "loss": 0.221,
      "step": 2275
    },
    {
      "epoch": 4.552,
      "grad_norm": 1.2931820154190063,
      "learning_rate": 0.00010900360144057624,
      "loss": 0.2294,
      "step": 2276
    },
    {
      "epoch": 4.554,
      "grad_norm": 1.4476369619369507,
      "learning_rate": 0.00010896358543417367,
      "loss": 0.2151,
      "step": 2277
    },
    {
      "epoch": 4.556,
      "grad_norm": 1.0898563861846924,
      "learning_rate": 0.00010892356942777112,
      "loss": 0.1893,
      "step": 2278
    },
    {
      "epoch": 4.558,
      "grad_norm": 1.2110897302627563,
      "learning_rate": 0.00010888355342136855,
      "loss": 0.2375,
      "step": 2279
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 1.7489771842956543,
      "learning_rate": 0.000108843537414966,
      "loss": 0.296,
      "step": 2280
    },
    {
      "epoch": 4.562,
      "grad_norm": 1.572244644165039,
      "learning_rate": 0.00010880352140856343,
      "loss": 0.263,
      "step": 2281
    },
    {
      "epoch": 4.564,
      "grad_norm": 1.102055311203003,
      "learning_rate": 0.00010876350540216087,
      "loss": 0.2374,
      "step": 2282
    },
    {
      "epoch": 4.566,
      "grad_norm": 1.2728540897369385,
      "learning_rate": 0.00010872348939575832,
      "loss": 0.2051,
      "step": 2283
    },
    {
      "epoch": 4.568,
      "grad_norm": 1.3601487874984741,
      "learning_rate": 0.00010868347338935574,
      "loss": 0.2483,
      "step": 2284
    },
    {
      "epoch": 4.57,
      "grad_norm": 1.1228808164596558,
      "learning_rate": 0.00010864345738295319,
      "loss": 0.1939,
      "step": 2285
    },
    {
      "epoch": 4.572,
      "grad_norm": 1.2163676023483276,
      "learning_rate": 0.00010860344137655063,
      "loss": 0.2017,
      "step": 2286
    },
    {
      "epoch": 4.574,
      "grad_norm": 1.544173240661621,
      "learning_rate": 0.00010856342537014808,
      "loss": 0.315,
      "step": 2287
    },
    {
      "epoch": 4.576,
      "grad_norm": 1.6376115083694458,
      "learning_rate": 0.0001085234093637455,
      "loss": 0.2375,
      "step": 2288
    },
    {
      "epoch": 4.578,
      "grad_norm": 1.4579386711120605,
      "learning_rate": 0.00010848339335734294,
      "loss": 0.2207,
      "step": 2289
    },
    {
      "epoch": 4.58,
      "grad_norm": 1.2856508493423462,
      "learning_rate": 0.00010844337735094039,
      "loss": 0.2067,
      "step": 2290
    },
    {
      "epoch": 4.582,
      "grad_norm": 1.3480829000473022,
      "learning_rate": 0.00010840336134453781,
      "loss": 0.2174,
      "step": 2291
    },
    {
      "epoch": 4.584,
      "grad_norm": 1.212199330329895,
      "learning_rate": 0.00010836334533813526,
      "loss": 0.2241,
      "step": 2292
    },
    {
      "epoch": 4.586,
      "grad_norm": 1.2592633962631226,
      "learning_rate": 0.0001083233293317327,
      "loss": 0.1955,
      "step": 2293
    },
    {
      "epoch": 4.588,
      "grad_norm": 1.7927451133728027,
      "learning_rate": 0.00010828331332533012,
      "loss": 0.325,
      "step": 2294
    },
    {
      "epoch": 4.59,
      "grad_norm": 1.2480279207229614,
      "learning_rate": 0.00010824329731892758,
      "loss": 0.2345,
      "step": 2295
    },
    {
      "epoch": 4.592,
      "grad_norm": 1.3912625312805176,
      "learning_rate": 0.00010820328131252501,
      "loss": 0.2589,
      "step": 2296
    },
    {
      "epoch": 4.594,
      "grad_norm": 1.4515193700790405,
      "learning_rate": 0.00010816326530612246,
      "loss": 0.3108,
      "step": 2297
    },
    {
      "epoch": 4.596,
      "grad_norm": 1.1503510475158691,
      "learning_rate": 0.00010812324929971989,
      "loss": 0.2253,
      "step": 2298
    },
    {
      "epoch": 4.598,
      "grad_norm": 1.627600908279419,
      "learning_rate": 0.00010808323329331734,
      "loss": 0.2774,
      "step": 2299
    },
    {
      "epoch": 4.6,
      "grad_norm": 1.4233042001724243,
      "learning_rate": 0.00010804321728691477,
      "loss": 0.2447,
      "step": 2300
    },
    {
      "epoch": 4.602,
      "grad_norm": 1.110040307044983,
      "learning_rate": 0.0001080032012805122,
      "loss": 0.1918,
      "step": 2301
    },
    {
      "epoch": 4.604,
      "grad_norm": 1.4724072217941284,
      "learning_rate": 0.00010796318527410965,
      "loss": 0.269,
      "step": 2302
    },
    {
      "epoch": 4.606,
      "grad_norm": 1.1044337749481201,
      "learning_rate": 0.00010792316926770709,
      "loss": 0.2306,
      "step": 2303
    },
    {
      "epoch": 4.608,
      "grad_norm": 1.6554421186447144,
      "learning_rate": 0.00010788315326130454,
      "loss": 0.2835,
      "step": 2304
    },
    {
      "epoch": 4.61,
      "grad_norm": 1.270667552947998,
      "learning_rate": 0.00010784313725490196,
      "loss": 0.2411,
      "step": 2305
    },
    {
      "epoch": 4.612,
      "grad_norm": 0.9253114461898804,
      "learning_rate": 0.00010780312124849941,
      "loss": 0.1843,
      "step": 2306
    },
    {
      "epoch": 4.614,
      "grad_norm": 1.2781877517700195,
      "learning_rate": 0.00010776310524209685,
      "loss": 0.2844,
      "step": 2307
    },
    {
      "epoch": 4.616,
      "grad_norm": 1.2169121503829956,
      "learning_rate": 0.00010772308923569427,
      "loss": 0.242,
      "step": 2308
    },
    {
      "epoch": 4.618,
      "grad_norm": 1.671347737312317,
      "learning_rate": 0.00010768307322929172,
      "loss": 0.2133,
      "step": 2309
    },
    {
      "epoch": 4.62,
      "grad_norm": 1.7579882144927979,
      "learning_rate": 0.00010764305722288916,
      "loss": 0.2332,
      "step": 2310
    },
    {
      "epoch": 4.622,
      "grad_norm": 1.2717230319976807,
      "learning_rate": 0.00010760304121648661,
      "loss": 0.1983,
      "step": 2311
    },
    {
      "epoch": 4.624,
      "grad_norm": 2.0451972484588623,
      "learning_rate": 0.00010756302521008403,
      "loss": 0.2736,
      "step": 2312
    },
    {
      "epoch": 4.626,
      "grad_norm": 0.996079683303833,
      "learning_rate": 0.00010752300920368148,
      "loss": 0.2063,
      "step": 2313
    },
    {
      "epoch": 4.628,
      "grad_norm": 1.2658309936523438,
      "learning_rate": 0.00010748299319727892,
      "loss": 0.2459,
      "step": 2314
    },
    {
      "epoch": 4.63,
      "grad_norm": 1.6435003280639648,
      "learning_rate": 0.00010744297719087635,
      "loss": 0.2372,
      "step": 2315
    },
    {
      "epoch": 4.632,
      "grad_norm": 1.244848608970642,
      "learning_rate": 0.0001074029611844738,
      "loss": 0.2211,
      "step": 2316
    },
    {
      "epoch": 4.634,
      "grad_norm": 0.8519914746284485,
      "learning_rate": 0.00010736294517807123,
      "loss": 0.1513,
      "step": 2317
    },
    {
      "epoch": 4.636,
      "grad_norm": 1.4531298875808716,
      "learning_rate": 0.00010732292917166866,
      "loss": 0.2809,
      "step": 2318
    },
    {
      "epoch": 4.638,
      "grad_norm": 1.2378778457641602,
      "learning_rate": 0.00010728291316526611,
      "loss": 0.2237,
      "step": 2319
    },
    {
      "epoch": 4.64,
      "grad_norm": 1.4320780038833618,
      "learning_rate": 0.00010724289715886356,
      "loss": 0.2453,
      "step": 2320
    },
    {
      "epoch": 4.642,
      "grad_norm": 1.3264659643173218,
      "learning_rate": 0.000107202881152461,
      "loss": 0.2115,
      "step": 2321
    },
    {
      "epoch": 4.644,
      "grad_norm": 1.3401453495025635,
      "learning_rate": 0.00010716286514605842,
      "loss": 0.2084,
      "step": 2322
    },
    {
      "epoch": 4.646,
      "grad_norm": 1.2036304473876953,
      "learning_rate": 0.00010712284913965587,
      "loss": 0.2195,
      "step": 2323
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.9781928658485413,
      "learning_rate": 0.0001070828331332533,
      "loss": 0.1399,
      "step": 2324
    },
    {
      "epoch": 4.65,
      "grad_norm": 1.5882246494293213,
      "learning_rate": 0.00010704281712685073,
      "loss": 0.2594,
      "step": 2325
    },
    {
      "epoch": 4.652,
      "grad_norm": 1.4098546504974365,
      "learning_rate": 0.00010700280112044818,
      "loss": 0.2119,
      "step": 2326
    },
    {
      "epoch": 4.654,
      "grad_norm": 1.556830644607544,
      "learning_rate": 0.00010696278511404563,
      "loss": 0.2037,
      "step": 2327
    },
    {
      "epoch": 4.656,
      "grad_norm": 2.132658004760742,
      "learning_rate": 0.00010692276910764307,
      "loss": 0.2728,
      "step": 2328
    },
    {
      "epoch": 4.658,
      "grad_norm": 1.576109528541565,
      "learning_rate": 0.00010688275310124049,
      "loss": 0.2414,
      "step": 2329
    },
    {
      "epoch": 4.66,
      "grad_norm": 1.5384681224822998,
      "learning_rate": 0.00010684273709483794,
      "loss": 0.2018,
      "step": 2330
    },
    {
      "epoch": 4.662,
      "grad_norm": 1.2818857431411743,
      "learning_rate": 0.00010680272108843538,
      "loss": 0.2057,
      "step": 2331
    },
    {
      "epoch": 4.664,
      "grad_norm": 1.4807662963867188,
      "learning_rate": 0.0001067627050820328,
      "loss": 0.228,
      "step": 2332
    },
    {
      "epoch": 4.666,
      "grad_norm": 1.3021597862243652,
      "learning_rate": 0.00010672268907563025,
      "loss": 0.2142,
      "step": 2333
    },
    {
      "epoch": 4.668,
      "grad_norm": 1.7791599035263062,
      "learning_rate": 0.0001066826730692277,
      "loss": 0.3042,
      "step": 2334
    },
    {
      "epoch": 4.67,
      "grad_norm": 1.259008765220642,
      "learning_rate": 0.00010664265706282513,
      "loss": 0.2076,
      "step": 2335
    },
    {
      "epoch": 4.672,
      "grad_norm": 1.4026563167572021,
      "learning_rate": 0.00010660264105642257,
      "loss": 0.2604,
      "step": 2336
    },
    {
      "epoch": 4.674,
      "grad_norm": 1.4371716976165771,
      "learning_rate": 0.00010656262505002002,
      "loss": 0.1808,
      "step": 2337
    },
    {
      "epoch": 4.676,
      "grad_norm": 1.8335740566253662,
      "learning_rate": 0.00010652260904361747,
      "loss": 0.2949,
      "step": 2338
    },
    {
      "epoch": 4.678,
      "grad_norm": 1.8336946964263916,
      "learning_rate": 0.00010648259303721488,
      "loss": 0.2889,
      "step": 2339
    },
    {
      "epoch": 4.68,
      "grad_norm": 2.018103837966919,
      "learning_rate": 0.00010644257703081233,
      "loss": 0.2897,
      "step": 2340
    },
    {
      "epoch": 4.682,
      "grad_norm": 1.7269381284713745,
      "learning_rate": 0.00010640256102440978,
      "loss": 0.2674,
      "step": 2341
    },
    {
      "epoch": 4.684,
      "grad_norm": 1.7883391380310059,
      "learning_rate": 0.0001063625450180072,
      "loss": 0.2045,
      "step": 2342
    },
    {
      "epoch": 4.686,
      "grad_norm": 1.5902384519577026,
      "learning_rate": 0.00010632252901160464,
      "loss": 0.2902,
      "step": 2343
    },
    {
      "epoch": 4.688,
      "grad_norm": 1.370345115661621,
      "learning_rate": 0.00010628251300520209,
      "loss": 0.2166,
      "step": 2344
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 1.2350879907608032,
      "learning_rate": 0.00010624249699879954,
      "loss": 0.1758,
      "step": 2345
    },
    {
      "epoch": 4.692,
      "grad_norm": 1.2332981824874878,
      "learning_rate": 0.00010620248099239696,
      "loss": 0.2601,
      "step": 2346
    },
    {
      "epoch": 4.694,
      "grad_norm": 1.1123651266098022,
      "learning_rate": 0.0001061624649859944,
      "loss": 0.2056,
      "step": 2347
    },
    {
      "epoch": 4.696,
      "grad_norm": 1.0350748300552368,
      "learning_rate": 0.00010612244897959185,
      "loss": 0.2328,
      "step": 2348
    },
    {
      "epoch": 4.698,
      "grad_norm": 1.2861660718917847,
      "learning_rate": 0.00010608243297318928,
      "loss": 0.1796,
      "step": 2349
    },
    {
      "epoch": 4.7,
      "grad_norm": 1.6239442825317383,
      "learning_rate": 0.00010604241696678671,
      "loss": 0.2683,
      "step": 2350
    },
    {
      "epoch": 4.702,
      "grad_norm": 1.9610306024551392,
      "learning_rate": 0.00010600240096038416,
      "loss": 0.3566,
      "step": 2351
    },
    {
      "epoch": 4.704,
      "grad_norm": 1.1982282400131226,
      "learning_rate": 0.00010596238495398161,
      "loss": 0.2101,
      "step": 2352
    },
    {
      "epoch": 4.7059999999999995,
      "grad_norm": 1.2305278778076172,
      "learning_rate": 0.00010592236894757904,
      "loss": 0.1965,
      "step": 2353
    },
    {
      "epoch": 4.708,
      "grad_norm": 1.1206625699996948,
      "learning_rate": 0.00010588235294117647,
      "loss": 0.2105,
      "step": 2354
    },
    {
      "epoch": 4.71,
      "grad_norm": 2.537010431289673,
      "learning_rate": 0.00010584233693477393,
      "loss": 0.2934,
      "step": 2355
    },
    {
      "epoch": 4.712,
      "grad_norm": 1.2521538734436035,
      "learning_rate": 0.00010580232092837135,
      "loss": 0.2567,
      "step": 2356
    },
    {
      "epoch": 4.714,
      "grad_norm": 1.1080873012542725,
      "learning_rate": 0.00010576230492196879,
      "loss": 0.2579,
      "step": 2357
    },
    {
      "epoch": 4.716,
      "grad_norm": 1.4109517335891724,
      "learning_rate": 0.00010572228891556624,
      "loss": 0.2205,
      "step": 2358
    },
    {
      "epoch": 4.718,
      "grad_norm": 1.0845565795898438,
      "learning_rate": 0.00010568227290916366,
      "loss": 0.2403,
      "step": 2359
    },
    {
      "epoch": 4.72,
      "grad_norm": 1.1864274740219116,
      "learning_rate": 0.00010564225690276111,
      "loss": 0.2359,
      "step": 2360
    },
    {
      "epoch": 4.7219999999999995,
      "grad_norm": 1.0051486492156982,
      "learning_rate": 0.00010560224089635855,
      "loss": 0.2079,
      "step": 2361
    },
    {
      "epoch": 4.724,
      "grad_norm": 1.5918219089508057,
      "learning_rate": 0.000105562224889956,
      "loss": 0.2883,
      "step": 2362
    },
    {
      "epoch": 4.726,
      "grad_norm": 1.1349436044692993,
      "learning_rate": 0.00010552220888355342,
      "loss": 0.1619,
      "step": 2363
    },
    {
      "epoch": 4.728,
      "grad_norm": 1.6340266466140747,
      "learning_rate": 0.00010548219287715086,
      "loss": 0.2406,
      "step": 2364
    },
    {
      "epoch": 4.73,
      "grad_norm": 1.174712061882019,
      "learning_rate": 0.00010544217687074831,
      "loss": 0.2099,
      "step": 2365
    },
    {
      "epoch": 4.732,
      "grad_norm": 1.311676263809204,
      "learning_rate": 0.00010540216086434573,
      "loss": 0.219,
      "step": 2366
    },
    {
      "epoch": 4.734,
      "grad_norm": 1.0649336576461792,
      "learning_rate": 0.00010536214485794318,
      "loss": 0.1975,
      "step": 2367
    },
    {
      "epoch": 4.736,
      "grad_norm": 1.27157723903656,
      "learning_rate": 0.00010532212885154062,
      "loss": 0.2702,
      "step": 2368
    },
    {
      "epoch": 4.7379999999999995,
      "grad_norm": 1.345801830291748,
      "learning_rate": 0.00010528211284513807,
      "loss": 0.232,
      "step": 2369
    },
    {
      "epoch": 4.74,
      "grad_norm": 1.448462963104248,
      "learning_rate": 0.0001052420968387355,
      "loss": 0.2677,
      "step": 2370
    },
    {
      "epoch": 4.742,
      "grad_norm": 1.5136035680770874,
      "learning_rate": 0.00010520208083233293,
      "loss": 0.1841,
      "step": 2371
    },
    {
      "epoch": 4.744,
      "grad_norm": 1.5408782958984375,
      "learning_rate": 0.00010516206482593038,
      "loss": 0.2187,
      "step": 2372
    },
    {
      "epoch": 4.746,
      "grad_norm": 1.187625527381897,
      "learning_rate": 0.00010512204881952781,
      "loss": 0.2188,
      "step": 2373
    },
    {
      "epoch": 4.748,
      "grad_norm": 1.507684588432312,
      "learning_rate": 0.00010508203281312526,
      "loss": 0.2657,
      "step": 2374
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.8618869185447693,
      "learning_rate": 0.0001050420168067227,
      "loss": 0.151,
      "step": 2375
    },
    {
      "epoch": 4.752,
      "grad_norm": 1.5751041173934937,
      "learning_rate": 0.00010500200080032015,
      "loss": 0.2351,
      "step": 2376
    },
    {
      "epoch": 4.754,
      "grad_norm": 1.4519579410552979,
      "learning_rate": 0.00010496198479391757,
      "loss": 0.2523,
      "step": 2377
    },
    {
      "epoch": 4.756,
      "grad_norm": 1.1695917844772339,
      "learning_rate": 0.000104921968787515,
      "loss": 0.1817,
      "step": 2378
    },
    {
      "epoch": 4.758,
      "grad_norm": 1.4452475309371948,
      "learning_rate": 0.00010488195278111246,
      "loss": 0.2447,
      "step": 2379
    },
    {
      "epoch": 4.76,
      "grad_norm": 1.646225929260254,
      "learning_rate": 0.00010484193677470988,
      "loss": 0.2566,
      "step": 2380
    },
    {
      "epoch": 4.7620000000000005,
      "grad_norm": 1.2830556631088257,
      "learning_rate": 0.00010480192076830733,
      "loss": 0.212,
      "step": 2381
    },
    {
      "epoch": 4.764,
      "grad_norm": 1.8018678426742554,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.2421,
      "step": 2382
    },
    {
      "epoch": 4.766,
      "grad_norm": 1.206760287284851,
      "learning_rate": 0.00010472188875550219,
      "loss": 0.2033,
      "step": 2383
    },
    {
      "epoch": 4.768,
      "grad_norm": 1.4758384227752686,
      "learning_rate": 0.00010468187274909964,
      "loss": 0.2264,
      "step": 2384
    },
    {
      "epoch": 4.77,
      "grad_norm": 1.7135525941848755,
      "learning_rate": 0.00010464185674269708,
      "loss": 0.2131,
      "step": 2385
    },
    {
      "epoch": 4.772,
      "grad_norm": 1.8694435358047485,
      "learning_rate": 0.00010460184073629453,
      "loss": 0.2596,
      "step": 2386
    },
    {
      "epoch": 4.774,
      "grad_norm": 1.3790135383605957,
      "learning_rate": 0.00010456182472989195,
      "loss": 0.2009,
      "step": 2387
    },
    {
      "epoch": 4.776,
      "grad_norm": 1.4644230604171753,
      "learning_rate": 0.0001045218087234894,
      "loss": 0.2201,
      "step": 2388
    },
    {
      "epoch": 4.7780000000000005,
      "grad_norm": 1.6788501739501953,
      "learning_rate": 0.00010448179271708684,
      "loss": 0.2866,
      "step": 2389
    },
    {
      "epoch": 4.78,
      "grad_norm": 1.0488221645355225,
      "learning_rate": 0.00010444177671068427,
      "loss": 0.1999,
      "step": 2390
    },
    {
      "epoch": 4.782,
      "grad_norm": 2.9830644130706787,
      "learning_rate": 0.00010440176070428172,
      "loss": 0.251,
      "step": 2391
    },
    {
      "epoch": 4.784,
      "grad_norm": 1.3416857719421387,
      "learning_rate": 0.00010436174469787915,
      "loss": 0.2408,
      "step": 2392
    },
    {
      "epoch": 4.786,
      "grad_norm": 1.444880485534668,
      "learning_rate": 0.0001043217286914766,
      "loss": 0.2807,
      "step": 2393
    },
    {
      "epoch": 4.788,
      "grad_norm": 1.0805776119232178,
      "learning_rate": 0.00010428171268507403,
      "loss": 0.2336,
      "step": 2394
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.9546692371368408,
      "learning_rate": 0.00010424169667867148,
      "loss": 0.1907,
      "step": 2395
    },
    {
      "epoch": 4.792,
      "grad_norm": 1.3818682432174683,
      "learning_rate": 0.00010420168067226892,
      "loss": 0.2132,
      "step": 2396
    },
    {
      "epoch": 4.7940000000000005,
      "grad_norm": 1.188507080078125,
      "learning_rate": 0.00010416166466586634,
      "loss": 0.2084,
      "step": 2397
    },
    {
      "epoch": 4.796,
      "grad_norm": 1.7725085020065308,
      "learning_rate": 0.00010412164865946379,
      "loss": 0.2441,
      "step": 2398
    },
    {
      "epoch": 4.798,
      "grad_norm": 1.5870158672332764,
      "learning_rate": 0.00010408163265306123,
      "loss": 0.2587,
      "step": 2399
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.5237846374511719,
      "learning_rate": 0.00010404161664665868,
      "loss": 0.2404,
      "step": 2400
    },
    {
      "epoch": 4.802,
      "grad_norm": 1.2696053981781006,
      "learning_rate": 0.0001040016006402561,
      "loss": 0.2295,
      "step": 2401
    },
    {
      "epoch": 4.804,
      "grad_norm": 1.7823336124420166,
      "learning_rate": 0.00010396158463385355,
      "loss": 0.3247,
      "step": 2402
    },
    {
      "epoch": 4.806,
      "grad_norm": 1.8686494827270508,
      "learning_rate": 0.00010392156862745099,
      "loss": 0.3105,
      "step": 2403
    },
    {
      "epoch": 4.808,
      "grad_norm": 1.6652402877807617,
      "learning_rate": 0.00010388155262104841,
      "loss": 0.2573,
      "step": 2404
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 1.3991889953613281,
      "learning_rate": 0.00010384153661464586,
      "loss": 0.2492,
      "step": 2405
    },
    {
      "epoch": 4.812,
      "grad_norm": 1.6147828102111816,
      "learning_rate": 0.00010380152060824331,
      "loss": 0.2938,
      "step": 2406
    },
    {
      "epoch": 4.814,
      "grad_norm": 1.4666038751602173,
      "learning_rate": 0.00010376150460184072,
      "loss": 0.2589,
      "step": 2407
    },
    {
      "epoch": 4.816,
      "grad_norm": 1.78010892868042,
      "learning_rate": 0.00010372148859543818,
      "loss": 0.2736,
      "step": 2408
    },
    {
      "epoch": 4.818,
      "grad_norm": 1.5016705989837646,
      "learning_rate": 0.00010368147258903563,
      "loss": 0.2688,
      "step": 2409
    },
    {
      "epoch": 4.82,
      "grad_norm": 1.1864409446716309,
      "learning_rate": 0.00010364145658263306,
      "loss": 0.2068,
      "step": 2410
    },
    {
      "epoch": 4.822,
      "grad_norm": 1.0495457649230957,
      "learning_rate": 0.00010360144057623049,
      "loss": 0.192,
      "step": 2411
    },
    {
      "epoch": 4.824,
      "grad_norm": 1.342871069908142,
      "learning_rate": 0.00010356142456982794,
      "loss": 0.225,
      "step": 2412
    },
    {
      "epoch": 4.826,
      "grad_norm": 1.5558278560638428,
      "learning_rate": 0.00010352140856342539,
      "loss": 0.2056,
      "step": 2413
    },
    {
      "epoch": 4.828,
      "grad_norm": 1.683599829673767,
      "learning_rate": 0.00010348139255702281,
      "loss": 0.3153,
      "step": 2414
    },
    {
      "epoch": 4.83,
      "grad_norm": 1.1242750883102417,
      "learning_rate": 0.00010344137655062025,
      "loss": 0.1937,
      "step": 2415
    },
    {
      "epoch": 4.832,
      "grad_norm": 1.67144775390625,
      "learning_rate": 0.0001034013605442177,
      "loss": 0.3262,
      "step": 2416
    },
    {
      "epoch": 4.834,
      "grad_norm": 1.4405848979949951,
      "learning_rate": 0.00010336134453781514,
      "loss": 0.2103,
      "step": 2417
    },
    {
      "epoch": 4.836,
      "grad_norm": 1.0140374898910522,
      "learning_rate": 0.00010332132853141256,
      "loss": 0.2188,
      "step": 2418
    },
    {
      "epoch": 4.838,
      "grad_norm": 1.2663772106170654,
      "learning_rate": 0.00010328131252501001,
      "loss": 0.2044,
      "step": 2419
    },
    {
      "epoch": 4.84,
      "grad_norm": 1.8077218532562256,
      "learning_rate": 0.00010324129651860746,
      "loss": 0.2891,
      "step": 2420
    },
    {
      "epoch": 4.842,
      "grad_norm": 1.6621812582015991,
      "learning_rate": 0.00010320128051220488,
      "loss": 0.2395,
      "step": 2421
    },
    {
      "epoch": 4.844,
      "grad_norm": 1.3578060865402222,
      "learning_rate": 0.00010316126450580232,
      "loss": 0.3038,
      "step": 2422
    },
    {
      "epoch": 4.846,
      "grad_norm": 1.7146714925765991,
      "learning_rate": 0.00010312124849939977,
      "loss": 0.2283,
      "step": 2423
    },
    {
      "epoch": 4.848,
      "grad_norm": 1.5041958093643188,
      "learning_rate": 0.0001030812324929972,
      "loss": 0.2751,
      "step": 2424
    },
    {
      "epoch": 4.85,
      "grad_norm": 1.856713891029358,
      "learning_rate": 0.00010304121648659463,
      "loss": 0.2973,
      "step": 2425
    },
    {
      "epoch": 4.852,
      "grad_norm": 1.3892499208450317,
      "learning_rate": 0.00010300120048019208,
      "loss": 0.1817,
      "step": 2426
    },
    {
      "epoch": 4.854,
      "grad_norm": 1.1041287183761597,
      "learning_rate": 0.00010296118447378953,
      "loss": 0.2023,
      "step": 2427
    },
    {
      "epoch": 4.856,
      "grad_norm": 2.3856823444366455,
      "learning_rate": 0.00010292116846738696,
      "loss": 0.2323,
      "step": 2428
    },
    {
      "epoch": 4.858,
      "grad_norm": 1.5162701606750488,
      "learning_rate": 0.0001028811524609844,
      "loss": 0.2465,
      "step": 2429
    },
    {
      "epoch": 4.86,
      "grad_norm": 1.2250287532806396,
      "learning_rate": 0.00010284113645458185,
      "loss": 0.2108,
      "step": 2430
    },
    {
      "epoch": 4.862,
      "grad_norm": 1.1216087341308594,
      "learning_rate": 0.00010280112044817927,
      "loss": 0.2294,
      "step": 2431
    },
    {
      "epoch": 4.864,
      "grad_norm": 1.0572611093521118,
      "learning_rate": 0.00010276110444177671,
      "loss": 0.1889,
      "step": 2432
    },
    {
      "epoch": 4.866,
      "grad_norm": 1.3290594816207886,
      "learning_rate": 0.00010272108843537416,
      "loss": 0.2426,
      "step": 2433
    },
    {
      "epoch": 4.868,
      "grad_norm": 1.522912621498108,
      "learning_rate": 0.00010268107242897161,
      "loss": 0.3058,
      "step": 2434
    },
    {
      "epoch": 4.87,
      "grad_norm": 1.4754719734191895,
      "learning_rate": 0.00010264105642256903,
      "loss": 0.2653,
      "step": 2435
    },
    {
      "epoch": 4.872,
      "grad_norm": 1.7900909185409546,
      "learning_rate": 0.00010260104041616647,
      "loss": 0.2726,
      "step": 2436
    },
    {
      "epoch": 4.874,
      "grad_norm": 0.8677964210510254,
      "learning_rate": 0.00010256102440976392,
      "loss": 0.1605,
      "step": 2437
    },
    {
      "epoch": 4.876,
      "grad_norm": 1.7591906785964966,
      "learning_rate": 0.00010252100840336134,
      "loss": 0.2781,
      "step": 2438
    },
    {
      "epoch": 4.878,
      "grad_norm": 1.2170157432556152,
      "learning_rate": 0.00010248099239695878,
      "loss": 0.2107,
      "step": 2439
    },
    {
      "epoch": 4.88,
      "grad_norm": 1.5501165390014648,
      "learning_rate": 0.00010244097639055623,
      "loss": 0.2546,
      "step": 2440
    },
    {
      "epoch": 4.882,
      "grad_norm": 1.3604601621627808,
      "learning_rate": 0.00010240096038415368,
      "loss": 0.2306,
      "step": 2441
    },
    {
      "epoch": 4.884,
      "grad_norm": 1.2888880968093872,
      "learning_rate": 0.0001023609443777511,
      "loss": 0.2161,
      "step": 2442
    },
    {
      "epoch": 4.886,
      "grad_norm": 1.40798020362854,
      "learning_rate": 0.00010232092837134854,
      "loss": 0.2646,
      "step": 2443
    },
    {
      "epoch": 4.888,
      "grad_norm": 1.373769998550415,
      "learning_rate": 0.00010228091236494599,
      "loss": 0.2665,
      "step": 2444
    },
    {
      "epoch": 4.89,
      "grad_norm": 1.5070173740386963,
      "learning_rate": 0.00010224089635854342,
      "loss": 0.2516,
      "step": 2445
    },
    {
      "epoch": 4.892,
      "grad_norm": 1.739355444908142,
      "learning_rate": 0.00010220088035214085,
      "loss": 0.2681,
      "step": 2446
    },
    {
      "epoch": 4.894,
      "grad_norm": 1.152612328529358,
      "learning_rate": 0.0001021608643457383,
      "loss": 0.2131,
      "step": 2447
    },
    {
      "epoch": 4.896,
      "grad_norm": 1.7027536630630493,
      "learning_rate": 0.00010212084833933573,
      "loss": 0.2307,
      "step": 2448
    },
    {
      "epoch": 4.898,
      "grad_norm": 1.1327511072158813,
      "learning_rate": 0.00010208083233293318,
      "loss": 0.2094,
      "step": 2449
    },
    {
      "epoch": 4.9,
      "grad_norm": 1.4759976863861084,
      "learning_rate": 0.00010204081632653062,
      "loss": 0.3063,
      "step": 2450
    },
    {
      "epoch": 4.902,
      "grad_norm": 1.7398498058319092,
      "learning_rate": 0.00010200080032012807,
      "loss": 0.2463,
      "step": 2451
    },
    {
      "epoch": 4.904,
      "grad_norm": 1.1198681592941284,
      "learning_rate": 0.00010196078431372549,
      "loss": 0.1741,
      "step": 2452
    },
    {
      "epoch": 4.906,
      "grad_norm": 1.270111322402954,
      "learning_rate": 0.00010192076830732293,
      "loss": 0.2608,
      "step": 2453
    },
    {
      "epoch": 4.908,
      "grad_norm": 1.6045938730239868,
      "learning_rate": 0.00010188075230092038,
      "loss": 0.2882,
      "step": 2454
    },
    {
      "epoch": 4.91,
      "grad_norm": 1.5983177423477173,
      "learning_rate": 0.0001018407362945178,
      "loss": 0.2074,
      "step": 2455
    },
    {
      "epoch": 4.912,
      "grad_norm": 1.7203178405761719,
      "learning_rate": 0.00010180072028811525,
      "loss": 0.2061,
      "step": 2456
    },
    {
      "epoch": 4.914,
      "grad_norm": 1.2320153713226318,
      "learning_rate": 0.00010176070428171269,
      "loss": 0.2194,
      "step": 2457
    },
    {
      "epoch": 4.916,
      "grad_norm": 1.663728952407837,
      "learning_rate": 0.00010172068827531014,
      "loss": 0.2218,
      "step": 2458
    },
    {
      "epoch": 4.918,
      "grad_norm": 1.1882272958755493,
      "learning_rate": 0.00010168067226890756,
      "loss": 0.2202,
      "step": 2459
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.9210052490234375,
      "learning_rate": 0.000101640656262505,
      "loss": 0.2146,
      "step": 2460
    },
    {
      "epoch": 4.922,
      "grad_norm": 1.2405438423156738,
      "learning_rate": 0.00010160064025610245,
      "loss": 0.1859,
      "step": 2461
    },
    {
      "epoch": 4.924,
      "grad_norm": 2.1117281913757324,
      "learning_rate": 0.00010156062424969988,
      "loss": 0.3259,
      "step": 2462
    },
    {
      "epoch": 4.926,
      "grad_norm": 1.5352250337600708,
      "learning_rate": 0.00010152060824329733,
      "loss": 0.2337,
      "step": 2463
    },
    {
      "epoch": 4.928,
      "grad_norm": 1.7232359647750854,
      "learning_rate": 0.00010148059223689476,
      "loss": 0.2761,
      "step": 2464
    },
    {
      "epoch": 4.93,
      "grad_norm": 1.557720422744751,
      "learning_rate": 0.00010144057623049221,
      "loss": 0.2101,
      "step": 2465
    },
    {
      "epoch": 4.932,
      "grad_norm": 1.4983652830123901,
      "learning_rate": 0.00010140056022408964,
      "loss": 0.2025,
      "step": 2466
    },
    {
      "epoch": 4.934,
      "grad_norm": 1.1853511333465576,
      "learning_rate": 0.00010136054421768707,
      "loss": 0.2413,
      "step": 2467
    },
    {
      "epoch": 4.936,
      "grad_norm": 2.4293439388275146,
      "learning_rate": 0.00010132052821128453,
      "loss": 0.2977,
      "step": 2468
    },
    {
      "epoch": 4.938,
      "grad_norm": 0.932984471321106,
      "learning_rate": 0.00010128051220488195,
      "loss": 0.2022,
      "step": 2469
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 2.115896224975586,
      "learning_rate": 0.0001012404961984794,
      "loss": 0.299,
      "step": 2470
    },
    {
      "epoch": 4.942,
      "grad_norm": 1.6365355253219604,
      "learning_rate": 0.00010120048019207684,
      "loss": 0.2833,
      "step": 2471
    },
    {
      "epoch": 4.944,
      "grad_norm": 1.3459974527359009,
      "learning_rate": 0.00010116046418567426,
      "loss": 0.2897,
      "step": 2472
    },
    {
      "epoch": 4.946,
      "grad_norm": 1.4729305505752563,
      "learning_rate": 0.00010112044817927171,
      "loss": 0.2501,
      "step": 2473
    },
    {
      "epoch": 4.948,
      "grad_norm": 1.197666049003601,
      "learning_rate": 0.00010108043217286916,
      "loss": 0.2354,
      "step": 2474
    },
    {
      "epoch": 4.95,
      "grad_norm": 1.2780238389968872,
      "learning_rate": 0.0001010404161664666,
      "loss": 0.2007,
      "step": 2475
    },
    {
      "epoch": 4.952,
      "grad_norm": 1.8475799560546875,
      "learning_rate": 0.00010100040016006402,
      "loss": 0.258,
      "step": 2476
    },
    {
      "epoch": 4.954,
      "grad_norm": 1.227644920349121,
      "learning_rate": 0.00010096038415366147,
      "loss": 0.2739,
      "step": 2477
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 1.4624545574188232,
      "learning_rate": 0.00010092036814725891,
      "loss": 0.2875,
      "step": 2478
    },
    {
      "epoch": 4.958,
      "grad_norm": 1.4403103590011597,
      "learning_rate": 0.00010088035214085633,
      "loss": 0.212,
      "step": 2479
    },
    {
      "epoch": 4.96,
      "grad_norm": 1.2663484811782837,
      "learning_rate": 0.00010084033613445378,
      "loss": 0.2039,
      "step": 2480
    },
    {
      "epoch": 4.962,
      "grad_norm": 1.41867196559906,
      "learning_rate": 0.00010080032012805123,
      "loss": 0.24,
      "step": 2481
    },
    {
      "epoch": 4.964,
      "grad_norm": 1.410367727279663,
      "learning_rate": 0.00010076030412164867,
      "loss": 0.2206,
      "step": 2482
    },
    {
      "epoch": 4.966,
      "grad_norm": 1.4005626440048218,
      "learning_rate": 0.0001007202881152461,
      "loss": 0.2349,
      "step": 2483
    },
    {
      "epoch": 4.968,
      "grad_norm": 1.3522748947143555,
      "learning_rate": 0.00010068027210884355,
      "loss": 0.2624,
      "step": 2484
    },
    {
      "epoch": 4.97,
      "grad_norm": 1.050945520401001,
      "learning_rate": 0.00010064025610244098,
      "loss": 0.2167,
      "step": 2485
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 1.5231173038482666,
      "learning_rate": 0.00010060024009603841,
      "loss": 0.21,
      "step": 2486
    },
    {
      "epoch": 4.974,
      "grad_norm": 1.2608104944229126,
      "learning_rate": 0.00010056022408963586,
      "loss": 0.1934,
      "step": 2487
    },
    {
      "epoch": 4.976,
      "grad_norm": 1.4910202026367188,
      "learning_rate": 0.00010052020808323331,
      "loss": 0.3285,
      "step": 2488
    },
    {
      "epoch": 4.978,
      "grad_norm": 1.5339019298553467,
      "learning_rate": 0.00010048019207683075,
      "loss": 0.326,
      "step": 2489
    },
    {
      "epoch": 4.98,
      "grad_norm": 1.391248345375061,
      "learning_rate": 0.00010044017607042817,
      "loss": 0.287,
      "step": 2490
    },
    {
      "epoch": 4.982,
      "grad_norm": 1.4925493001937866,
      "learning_rate": 0.00010040016006402562,
      "loss": 0.2614,
      "step": 2491
    },
    {
      "epoch": 4.984,
      "grad_norm": 1.3534364700317383,
      "learning_rate": 0.00010036014405762306,
      "loss": 0.2366,
      "step": 2492
    },
    {
      "epoch": 4.986,
      "grad_norm": 2.411613941192627,
      "learning_rate": 0.00010032012805122048,
      "loss": 0.2728,
      "step": 2493
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 1.5121101140975952,
      "learning_rate": 0.00010028011204481793,
      "loss": 0.2316,
      "step": 2494
    },
    {
      "epoch": 4.99,
      "grad_norm": 1.6303141117095947,
      "learning_rate": 0.00010024009603841538,
      "loss": 0.2811,
      "step": 2495
    },
    {
      "epoch": 4.992,
      "grad_norm": 1.1084699630737305,
      "learning_rate": 0.0001002000800320128,
      "loss": 0.2024,
      "step": 2496
    },
    {
      "epoch": 4.994,
      "grad_norm": 1.3291670083999634,
      "learning_rate": 0.00010016006402561024,
      "loss": 0.2515,
      "step": 2497
    },
    {
      "epoch": 4.996,
      "grad_norm": 1.5055959224700928,
      "learning_rate": 0.0001001200480192077,
      "loss": 0.2977,
      "step": 2498
    },
    {
      "epoch": 4.998,
      "grad_norm": 1.116690993309021,
      "learning_rate": 0.00010008003201280513,
      "loss": 0.1964,
      "step": 2499
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.389381766319275,
      "learning_rate": 0.00010004001600640255,
      "loss": 0.2773,
      "step": 2500
    },
    {
      "epoch": 5.002,
      "grad_norm": 0.8422757983207703,
      "learning_rate": 0.0001,
      "loss": 0.1562,
      "step": 2501
    },
    {
      "epoch": 5.004,
      "grad_norm": 1.1898009777069092,
      "learning_rate": 9.995998399359744e-05,
      "loss": 0.2128,
      "step": 2502
    },
    {
      "epoch": 5.006,
      "grad_norm": 0.9358187913894653,
      "learning_rate": 9.991996798719488e-05,
      "loss": 0.1442,
      "step": 2503
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.8054648637771606,
      "learning_rate": 9.987995198079232e-05,
      "loss": 0.1509,
      "step": 2504
    },
    {
      "epoch": 5.01,
      "grad_norm": 1.04302978515625,
      "learning_rate": 9.983993597438977e-05,
      "loss": 0.1684,
      "step": 2505
    },
    {
      "epoch": 5.012,
      "grad_norm": 1.0430529117584229,
      "learning_rate": 9.97999199679872e-05,
      "loss": 0.1826,
      "step": 2506
    },
    {
      "epoch": 5.014,
      "grad_norm": 0.8160897493362427,
      "learning_rate": 9.975990396158463e-05,
      "loss": 0.1353,
      "step": 2507
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.9486539959907532,
      "learning_rate": 9.971988795518208e-05,
      "loss": 0.1423,
      "step": 2508
    },
    {
      "epoch": 5.018,
      "grad_norm": 1.2314175367355347,
      "learning_rate": 9.967987194877952e-05,
      "loss": 0.1542,
      "step": 2509
    },
    {
      "epoch": 5.02,
      "grad_norm": 1.663219690322876,
      "learning_rate": 9.963985594237695e-05,
      "loss": 0.1809,
      "step": 2510
    },
    {
      "epoch": 5.022,
      "grad_norm": 1.7284637689590454,
      "learning_rate": 9.959983993597439e-05,
      "loss": 0.1765,
      "step": 2511
    },
    {
      "epoch": 5.024,
      "grad_norm": 1.135128140449524,
      "learning_rate": 9.955982392957184e-05,
      "loss": 0.1693,
      "step": 2512
    },
    {
      "epoch": 5.026,
      "grad_norm": 1.1769981384277344,
      "learning_rate": 9.951980792316928e-05,
      "loss": 0.1405,
      "step": 2513
    },
    {
      "epoch": 5.028,
      "grad_norm": 1.5786235332489014,
      "learning_rate": 9.94797919167667e-05,
      "loss": 0.1851,
      "step": 2514
    },
    {
      "epoch": 5.03,
      "grad_norm": 1.1214531660079956,
      "learning_rate": 9.943977591036415e-05,
      "loss": 0.1658,
      "step": 2515
    },
    {
      "epoch": 5.032,
      "grad_norm": 1.3174556493759155,
      "learning_rate": 9.939975990396159e-05,
      "loss": 0.1748,
      "step": 2516
    },
    {
      "epoch": 5.034,
      "grad_norm": 0.6225986480712891,
      "learning_rate": 9.935974389755903e-05,
      "loss": 0.1208,
      "step": 2517
    },
    {
      "epoch": 5.036,
      "grad_norm": 2.366032361984253,
      "learning_rate": 9.931972789115646e-05,
      "loss": 0.2137,
      "step": 2518
    },
    {
      "epoch": 5.038,
      "grad_norm": 0.8700487613677979,
      "learning_rate": 9.92797118847539e-05,
      "loss": 0.1492,
      "step": 2519
    },
    {
      "epoch": 5.04,
      "grad_norm": 1.8791733980178833,
      "learning_rate": 9.923969587835135e-05,
      "loss": 0.1488,
      "step": 2520
    },
    {
      "epoch": 5.042,
      "grad_norm": 1.471121907234192,
      "learning_rate": 9.919967987194877e-05,
      "loss": 0.1659,
      "step": 2521
    },
    {
      "epoch": 5.044,
      "grad_norm": 0.9950549006462097,
      "learning_rate": 9.915966386554623e-05,
      "loss": 0.1607,
      "step": 2522
    },
    {
      "epoch": 5.046,
      "grad_norm": 0.7837868928909302,
      "learning_rate": 9.911964785914366e-05,
      "loss": 0.1351,
      "step": 2523
    },
    {
      "epoch": 5.048,
      "grad_norm": 1.0592432022094727,
      "learning_rate": 9.90796318527411e-05,
      "loss": 0.1583,
      "step": 2524
    },
    {
      "epoch": 5.05,
      "grad_norm": 1.2865194082260132,
      "learning_rate": 9.903961584633854e-05,
      "loss": 0.1439,
      "step": 2525
    },
    {
      "epoch": 5.052,
      "grad_norm": 1.4553806781768799,
      "learning_rate": 9.899959983993597e-05,
      "loss": 0.1468,
      "step": 2526
    },
    {
      "epoch": 5.054,
      "grad_norm": 0.9398150444030762,
      "learning_rate": 9.895958383353342e-05,
      "loss": 0.1542,
      "step": 2527
    },
    {
      "epoch": 5.056,
      "grad_norm": 1.495794415473938,
      "learning_rate": 9.891956782713085e-05,
      "loss": 0.1708,
      "step": 2528
    },
    {
      "epoch": 5.058,
      "grad_norm": 0.6124358177185059,
      "learning_rate": 9.88795518207283e-05,
      "loss": 0.13,
      "step": 2529
    },
    {
      "epoch": 5.06,
      "grad_norm": 1.1054514646530151,
      "learning_rate": 9.883953581432574e-05,
      "loss": 0.154,
      "step": 2530
    },
    {
      "epoch": 5.062,
      "grad_norm": 1.5312411785125732,
      "learning_rate": 9.879951980792317e-05,
      "loss": 0.1745,
      "step": 2531
    },
    {
      "epoch": 5.064,
      "grad_norm": 1.0351736545562744,
      "learning_rate": 9.875950380152061e-05,
      "loss": 0.1678,
      "step": 2532
    },
    {
      "epoch": 5.066,
      "grad_norm": 1.3556010723114014,
      "learning_rate": 9.871948779511805e-05,
      "loss": 0.1619,
      "step": 2533
    },
    {
      "epoch": 5.068,
      "grad_norm": 1.1367827653884888,
      "learning_rate": 9.86794717887155e-05,
      "loss": 0.1769,
      "step": 2534
    },
    {
      "epoch": 5.07,
      "grad_norm": 0.9994992017745972,
      "learning_rate": 9.863945578231294e-05,
      "loss": 0.1049,
      "step": 2535
    },
    {
      "epoch": 5.072,
      "grad_norm": 3.009859800338745,
      "learning_rate": 9.859943977591037e-05,
      "loss": 0.1471,
      "step": 2536
    },
    {
      "epoch": 5.074,
      "grad_norm": 1.112177848815918,
      "learning_rate": 9.855942376950781e-05,
      "loss": 0.1924,
      "step": 2537
    },
    {
      "epoch": 5.076,
      "grad_norm": 0.9278802871704102,
      "learning_rate": 9.851940776310525e-05,
      "loss": 0.12,
      "step": 2538
    },
    {
      "epoch": 5.078,
      "grad_norm": 1.0052788257598877,
      "learning_rate": 9.847939175670268e-05,
      "loss": 0.1571,
      "step": 2539
    },
    {
      "epoch": 5.08,
      "grad_norm": 1.5666357278823853,
      "learning_rate": 9.843937575030012e-05,
      "loss": 0.212,
      "step": 2540
    },
    {
      "epoch": 5.082,
      "grad_norm": 1.1476432085037231,
      "learning_rate": 9.839935974389757e-05,
      "loss": 0.1655,
      "step": 2541
    },
    {
      "epoch": 5.084,
      "grad_norm": 1.4007669687271118,
      "learning_rate": 9.835934373749501e-05,
      "loss": 0.1997,
      "step": 2542
    },
    {
      "epoch": 5.086,
      "grad_norm": 0.9545021057128906,
      "learning_rate": 9.831932773109243e-05,
      "loss": 0.1583,
      "step": 2543
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.9652093052864075,
      "learning_rate": 9.827931172468988e-05,
      "loss": 0.1384,
      "step": 2544
    },
    {
      "epoch": 5.09,
      "grad_norm": 1.471269965171814,
      "learning_rate": 9.823929571828732e-05,
      "loss": 0.205,
      "step": 2545
    },
    {
      "epoch": 5.092,
      "grad_norm": 1.6097636222839355,
      "learning_rate": 9.819927971188476e-05,
      "loss": 0.172,
      "step": 2546
    },
    {
      "epoch": 5.094,
      "grad_norm": 1.1725778579711914,
      "learning_rate": 9.81592637054822e-05,
      "loss": 0.1676,
      "step": 2547
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.819184422492981,
      "learning_rate": 9.811924769907965e-05,
      "loss": 0.1463,
      "step": 2548
    },
    {
      "epoch": 5.098,
      "grad_norm": 0.7222493290901184,
      "learning_rate": 9.807923169267708e-05,
      "loss": 0.1582,
      "step": 2549
    },
    {
      "epoch": 5.1,
      "grad_norm": 1.6401795148849487,
      "learning_rate": 9.80392156862745e-05,
      "loss": 0.1777,
      "step": 2550
    },
    {
      "epoch": 5.102,
      "grad_norm": 0.7674184441566467,
      "learning_rate": 9.799919967987196e-05,
      "loss": 0.158,
      "step": 2551
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.7847084403038025,
      "learning_rate": 9.79591836734694e-05,
      "loss": 0.16,
      "step": 2552
    },
    {
      "epoch": 5.106,
      "grad_norm": 1.08113694190979,
      "learning_rate": 9.791916766706683e-05,
      "loss": 0.1575,
      "step": 2553
    },
    {
      "epoch": 5.108,
      "grad_norm": 0.955981433391571,
      "learning_rate": 9.787915166066427e-05,
      "loss": 0.1514,
      "step": 2554
    },
    {
      "epoch": 5.11,
      "grad_norm": 1.6589124202728271,
      "learning_rate": 9.78391356542617e-05,
      "loss": 0.173,
      "step": 2555
    },
    {
      "epoch": 5.112,
      "grad_norm": 1.1468862295150757,
      "learning_rate": 9.779911964785916e-05,
      "loss": 0.1665,
      "step": 2556
    },
    {
      "epoch": 5.114,
      "grad_norm": 1.9268139600753784,
      "learning_rate": 9.775910364145658e-05,
      "loss": 0.1542,
      "step": 2557
    },
    {
      "epoch": 5.116,
      "grad_norm": 1.3147038221359253,
      "learning_rate": 9.771908763505403e-05,
      "loss": 0.1607,
      "step": 2558
    },
    {
      "epoch": 5.118,
      "grad_norm": 1.0207631587982178,
      "learning_rate": 9.767907162865147e-05,
      "loss": 0.1918,
      "step": 2559
    },
    {
      "epoch": 5.12,
      "grad_norm": 2.0290634632110596,
      "learning_rate": 9.76390556222489e-05,
      "loss": 0.2076,
      "step": 2560
    },
    {
      "epoch": 5.122,
      "grad_norm": 0.9861572980880737,
      "learning_rate": 9.759903961584634e-05,
      "loss": 0.1478,
      "step": 2561
    },
    {
      "epoch": 5.124,
      "grad_norm": 0.9267034530639648,
      "learning_rate": 9.755902360944378e-05,
      "loss": 0.1316,
      "step": 2562
    },
    {
      "epoch": 5.126,
      "grad_norm": 1.1439670324325562,
      "learning_rate": 9.751900760304123e-05,
      "loss": 0.171,
      "step": 2563
    },
    {
      "epoch": 5.128,
      "grad_norm": 1.2026416063308716,
      "learning_rate": 9.747899159663865e-05,
      "loss": 0.1388,
      "step": 2564
    },
    {
      "epoch": 5.13,
      "grad_norm": 1.2593632936477661,
      "learning_rate": 9.74389755902361e-05,
      "loss": 0.1541,
      "step": 2565
    },
    {
      "epoch": 5.132,
      "grad_norm": 0.9756919741630554,
      "learning_rate": 9.739895958383354e-05,
      "loss": 0.1803,
      "step": 2566
    },
    {
      "epoch": 5.134,
      "grad_norm": 1.3857982158660889,
      "learning_rate": 9.735894357743098e-05,
      "loss": 0.169,
      "step": 2567
    },
    {
      "epoch": 5.136,
      "grad_norm": 1.364406704902649,
      "learning_rate": 9.731892757102841e-05,
      "loss": 0.1665,
      "step": 2568
    },
    {
      "epoch": 5.138,
      "grad_norm": 1.1887754201889038,
      "learning_rate": 9.727891156462585e-05,
      "loss": 0.1385,
      "step": 2569
    },
    {
      "epoch": 5.14,
      "grad_norm": 1.0715179443359375,
      "learning_rate": 9.72388955582233e-05,
      "loss": 0.1685,
      "step": 2570
    },
    {
      "epoch": 5.142,
      "grad_norm": 1.6115728616714478,
      "learning_rate": 9.719887955182073e-05,
      "loss": 0.1603,
      "step": 2571
    },
    {
      "epoch": 5.144,
      "grad_norm": 1.245688796043396,
      "learning_rate": 9.715886354541818e-05,
      "loss": 0.1945,
      "step": 2572
    },
    {
      "epoch": 5.146,
      "grad_norm": 0.6593698263168335,
      "learning_rate": 9.711884753901561e-05,
      "loss": 0.1309,
      "step": 2573
    },
    {
      "epoch": 5.148,
      "grad_norm": 1.1309674978256226,
      "learning_rate": 9.707883153261305e-05,
      "loss": 0.1521,
      "step": 2574
    },
    {
      "epoch": 5.15,
      "grad_norm": 0.7102833390235901,
      "learning_rate": 9.703881552621049e-05,
      "loss": 0.1296,
      "step": 2575
    },
    {
      "epoch": 5.152,
      "grad_norm": 1.0930640697479248,
      "learning_rate": 9.699879951980793e-05,
      "loss": 0.1668,
      "step": 2576
    },
    {
      "epoch": 5.154,
      "grad_norm": 1.0872341394424438,
      "learning_rate": 9.695878351340538e-05,
      "loss": 0.1561,
      "step": 2577
    },
    {
      "epoch": 5.156,
      "grad_norm": 0.9928577542304993,
      "learning_rate": 9.69187675070028e-05,
      "loss": 0.1429,
      "step": 2578
    },
    {
      "epoch": 5.158,
      "grad_norm": 1.4905123710632324,
      "learning_rate": 9.687875150060024e-05,
      "loss": 0.1603,
      "step": 2579
    },
    {
      "epoch": 5.16,
      "grad_norm": 1.1557341814041138,
      "learning_rate": 9.683873549419769e-05,
      "loss": 0.164,
      "step": 2580
    },
    {
      "epoch": 5.162,
      "grad_norm": 1.0505280494689941,
      "learning_rate": 9.679871948779512e-05,
      "loss": 0.1552,
      "step": 2581
    },
    {
      "epoch": 5.164,
      "grad_norm": 1.060375452041626,
      "learning_rate": 9.675870348139256e-05,
      "loss": 0.1573,
      "step": 2582
    },
    {
      "epoch": 5.166,
      "grad_norm": 1.8030308485031128,
      "learning_rate": 9.671868747499e-05,
      "loss": 0.1322,
      "step": 2583
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.9659332633018494,
      "learning_rate": 9.667867146858745e-05,
      "loss": 0.1784,
      "step": 2584
    },
    {
      "epoch": 5.17,
      "grad_norm": 1.353161096572876,
      "learning_rate": 9.663865546218487e-05,
      "loss": 0.2139,
      "step": 2585
    },
    {
      "epoch": 5.172,
      "grad_norm": 0.6362184882164001,
      "learning_rate": 9.659863945578231e-05,
      "loss": 0.1575,
      "step": 2586
    },
    {
      "epoch": 5.174,
      "grad_norm": 1.1744493246078491,
      "learning_rate": 9.655862344937976e-05,
      "loss": 0.1963,
      "step": 2587
    },
    {
      "epoch": 5.176,
      "grad_norm": 1.197873830795288,
      "learning_rate": 9.65186074429772e-05,
      "loss": 0.1787,
      "step": 2588
    },
    {
      "epoch": 5.178,
      "grad_norm": 0.9101353883743286,
      "learning_rate": 9.647859143657464e-05,
      "loss": 0.158,
      "step": 2589
    },
    {
      "epoch": 5.18,
      "grad_norm": 1.127829670906067,
      "learning_rate": 9.643857543017207e-05,
      "loss": 0.1742,
      "step": 2590
    },
    {
      "epoch": 5.182,
      "grad_norm": 1.0204963684082031,
      "learning_rate": 9.639855942376951e-05,
      "loss": 0.1361,
      "step": 2591
    },
    {
      "epoch": 5.184,
      "grad_norm": 1.0957274436950684,
      "learning_rate": 9.635854341736695e-05,
      "loss": 0.176,
      "step": 2592
    },
    {
      "epoch": 5.186,
      "grad_norm": 0.8495940566062927,
      "learning_rate": 9.631852741096438e-05,
      "loss": 0.1386,
      "step": 2593
    },
    {
      "epoch": 5.188,
      "grad_norm": 1.05269193649292,
      "learning_rate": 9.627851140456183e-05,
      "loss": 0.1794,
      "step": 2594
    },
    {
      "epoch": 5.19,
      "grad_norm": 1.3054054975509644,
      "learning_rate": 9.623849539815927e-05,
      "loss": 0.1508,
      "step": 2595
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.8772323727607727,
      "learning_rate": 9.61984793917567e-05,
      "loss": 0.1786,
      "step": 2596
    },
    {
      "epoch": 5.194,
      "grad_norm": 0.6226502060890198,
      "learning_rate": 9.615846338535415e-05,
      "loss": 0.1122,
      "step": 2597
    },
    {
      "epoch": 5.196,
      "grad_norm": 1.1088050603866577,
      "learning_rate": 9.611844737895158e-05,
      "loss": 0.1817,
      "step": 2598
    },
    {
      "epoch": 5.198,
      "grad_norm": 1.22465181350708,
      "learning_rate": 9.607843137254903e-05,
      "loss": 0.2177,
      "step": 2599
    },
    {
      "epoch": 5.2,
      "grad_norm": 1.0930691957473755,
      "learning_rate": 9.603841536614646e-05,
      "loss": 0.1693,
      "step": 2600
    },
    {
      "epoch": 5.202,
      "grad_norm": 1.2846018075942993,
      "learning_rate": 9.599839935974391e-05,
      "loss": 0.1407,
      "step": 2601
    },
    {
      "epoch": 5.204,
      "grad_norm": 1.6379332542419434,
      "learning_rate": 9.595838335334135e-05,
      "loss": 0.1556,
      "step": 2602
    },
    {
      "epoch": 5.206,
      "grad_norm": 1.0694512128829956,
      "learning_rate": 9.591836734693878e-05,
      "loss": 0.1268,
      "step": 2603
    },
    {
      "epoch": 5.208,
      "grad_norm": 1.5140348672866821,
      "learning_rate": 9.587835134053622e-05,
      "loss": 0.2318,
      "step": 2604
    },
    {
      "epoch": 5.21,
      "grad_norm": 1.7004369497299194,
      "learning_rate": 9.583833533413366e-05,
      "loss": 0.1695,
      "step": 2605
    },
    {
      "epoch": 5.212,
      "grad_norm": 1.2534478902816772,
      "learning_rate": 9.579831932773111e-05,
      "loss": 0.1588,
      "step": 2606
    },
    {
      "epoch": 5.214,
      "grad_norm": 0.9317173361778259,
      "learning_rate": 9.575830332132853e-05,
      "loss": 0.1234,
      "step": 2607
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.8144178986549377,
      "learning_rate": 9.571828731492597e-05,
      "loss": 0.1765,
      "step": 2608
    },
    {
      "epoch": 5.218,
      "grad_norm": 0.9948593378067017,
      "learning_rate": 9.567827130852342e-05,
      "loss": 0.1504,
      "step": 2609
    },
    {
      "epoch": 5.22,
      "grad_norm": 1.2165818214416504,
      "learning_rate": 9.563825530212086e-05,
      "loss": 0.1525,
      "step": 2610
    },
    {
      "epoch": 5.222,
      "grad_norm": 0.932685375213623,
      "learning_rate": 9.559823929571829e-05,
      "loss": 0.1391,
      "step": 2611
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.7463222742080688,
      "learning_rate": 9.555822328931573e-05,
      "loss": 0.1415,
      "step": 2612
    },
    {
      "epoch": 5.226,
      "grad_norm": 2.1153616905212402,
      "learning_rate": 9.551820728291318e-05,
      "loss": 0.189,
      "step": 2613
    },
    {
      "epoch": 5.228,
      "grad_norm": 2.085841178894043,
      "learning_rate": 9.54781912765106e-05,
      "loss": 0.2456,
      "step": 2614
    },
    {
      "epoch": 5.23,
      "grad_norm": 1.4060899019241333,
      "learning_rate": 9.543817527010804e-05,
      "loss": 0.1845,
      "step": 2615
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.9000400900840759,
      "learning_rate": 9.539815926370549e-05,
      "loss": 0.1561,
      "step": 2616
    },
    {
      "epoch": 5.234,
      "grad_norm": 2.638140916824341,
      "learning_rate": 9.535814325730293e-05,
      "loss": 0.1454,
      "step": 2617
    },
    {
      "epoch": 5.236,
      "grad_norm": 1.3569594621658325,
      "learning_rate": 9.531812725090037e-05,
      "loss": 0.2008,
      "step": 2618
    },
    {
      "epoch": 5.2379999999999995,
      "grad_norm": 0.9948651790618896,
      "learning_rate": 9.52781112444978e-05,
      "loss": 0.1249,
      "step": 2619
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.7239668965339661,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.1443,
      "step": 2620
    },
    {
      "epoch": 5.242,
      "grad_norm": 1.0767021179199219,
      "learning_rate": 9.519807923169268e-05,
      "loss": 0.177,
      "step": 2621
    },
    {
      "epoch": 5.244,
      "grad_norm": 1.0687344074249268,
      "learning_rate": 9.515806322529011e-05,
      "loss": 0.1593,
      "step": 2622
    },
    {
      "epoch": 5.246,
      "grad_norm": 1.169082760810852,
      "learning_rate": 9.511804721888757e-05,
      "loss": 0.16,
      "step": 2623
    },
    {
      "epoch": 5.248,
      "grad_norm": 1.307000994682312,
      "learning_rate": 9.5078031212485e-05,
      "loss": 0.1983,
      "step": 2624
    },
    {
      "epoch": 5.25,
      "grad_norm": 1.3457553386688232,
      "learning_rate": 9.503801520608244e-05,
      "loss": 0.1448,
      "step": 2625
    },
    {
      "epoch": 5.252,
      "grad_norm": 0.4898655414581299,
      "learning_rate": 9.499799919967988e-05,
      "loss": 0.1003,
      "step": 2626
    },
    {
      "epoch": 5.254,
      "grad_norm": 1.3317893743515015,
      "learning_rate": 9.495798319327731e-05,
      "loss": 0.1598,
      "step": 2627
    },
    {
      "epoch": 5.256,
      "grad_norm": 1.0148428678512573,
      "learning_rate": 9.491796718687475e-05,
      "loss": 0.1519,
      "step": 2628
    },
    {
      "epoch": 5.258,
      "grad_norm": 0.6864725351333618,
      "learning_rate": 9.487795118047219e-05,
      "loss": 0.1276,
      "step": 2629
    },
    {
      "epoch": 5.26,
      "grad_norm": 1.1559131145477295,
      "learning_rate": 9.483793517406964e-05,
      "loss": 0.186,
      "step": 2630
    },
    {
      "epoch": 5.2620000000000005,
      "grad_norm": 1.8483822345733643,
      "learning_rate": 9.479791916766708e-05,
      "loss": 0.1674,
      "step": 2631
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.9295294284820557,
      "learning_rate": 9.47579031612645e-05,
      "loss": 0.1272,
      "step": 2632
    },
    {
      "epoch": 5.266,
      "grad_norm": 1.0094887018203735,
      "learning_rate": 9.471788715486195e-05,
      "loss": 0.1705,
      "step": 2633
    },
    {
      "epoch": 5.268,
      "grad_norm": 1.007576584815979,
      "learning_rate": 9.467787114845939e-05,
      "loss": 0.1806,
      "step": 2634
    },
    {
      "epoch": 5.27,
      "grad_norm": 2.472352981567383,
      "learning_rate": 9.463785514205682e-05,
      "loss": 0.2375,
      "step": 2635
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.8231088519096375,
      "learning_rate": 9.459783913565426e-05,
      "loss": 0.1334,
      "step": 2636
    },
    {
      "epoch": 5.274,
      "grad_norm": 1.0495009422302246,
      "learning_rate": 9.455782312925171e-05,
      "loss": 0.1398,
      "step": 2637
    },
    {
      "epoch": 5.276,
      "grad_norm": 0.9256539940834045,
      "learning_rate": 9.451780712284915e-05,
      "loss": 0.1475,
      "step": 2638
    },
    {
      "epoch": 5.2780000000000005,
      "grad_norm": 1.424208402633667,
      "learning_rate": 9.447779111644657e-05,
      "loss": 0.1639,
      "step": 2639
    },
    {
      "epoch": 5.28,
      "grad_norm": 1.0311347246170044,
      "learning_rate": 9.443777511004402e-05,
      "loss": 0.1509,
      "step": 2640
    },
    {
      "epoch": 5.282,
      "grad_norm": 1.6377995014190674,
      "learning_rate": 9.439775910364146e-05,
      "loss": 0.1472,
      "step": 2641
    },
    {
      "epoch": 5.284,
      "grad_norm": 0.9989171624183655,
      "learning_rate": 9.43577430972389e-05,
      "loss": 0.1196,
      "step": 2642
    },
    {
      "epoch": 5.286,
      "grad_norm": 1.1160777807235718,
      "learning_rate": 9.431772709083634e-05,
      "loss": 0.1573,
      "step": 2643
    },
    {
      "epoch": 5.288,
      "grad_norm": 1.579838514328003,
      "learning_rate": 9.427771108443377e-05,
      "loss": 0.1743,
      "step": 2644
    },
    {
      "epoch": 5.29,
      "grad_norm": 1.539046287536621,
      "learning_rate": 9.423769507803122e-05,
      "loss": 0.2631,
      "step": 2645
    },
    {
      "epoch": 5.292,
      "grad_norm": 1.3538168668746948,
      "learning_rate": 9.419767907162865e-05,
      "loss": 0.1936,
      "step": 2646
    },
    {
      "epoch": 5.294,
      "grad_norm": 1.605595588684082,
      "learning_rate": 9.41576630652261e-05,
      "loss": 0.1568,
      "step": 2647
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.9230272769927979,
      "learning_rate": 9.411764705882353e-05,
      "loss": 0.1354,
      "step": 2648
    },
    {
      "epoch": 5.298,
      "grad_norm": 0.8562687039375305,
      "learning_rate": 9.407763105242097e-05,
      "loss": 0.1146,
      "step": 2649
    },
    {
      "epoch": 5.3,
      "grad_norm": 1.0030922889709473,
      "learning_rate": 9.403761504601841e-05,
      "loss": 0.1301,
      "step": 2650
    },
    {
      "epoch": 5.302,
      "grad_norm": 1.1682512760162354,
      "learning_rate": 9.399759903961585e-05,
      "loss": 0.1522,
      "step": 2651
    },
    {
      "epoch": 5.304,
      "grad_norm": 1.0311672687530518,
      "learning_rate": 9.39575830332133e-05,
      "loss": 0.1437,
      "step": 2652
    },
    {
      "epoch": 5.306,
      "grad_norm": 1.3628135919570923,
      "learning_rate": 9.391756702681072e-05,
      "loss": 0.1833,
      "step": 2653
    },
    {
      "epoch": 5.308,
      "grad_norm": 1.0154154300689697,
      "learning_rate": 9.387755102040817e-05,
      "loss": 0.1607,
      "step": 2654
    },
    {
      "epoch": 5.31,
      "grad_norm": 0.8267223238945007,
      "learning_rate": 9.383753501400561e-05,
      "loss": 0.1288,
      "step": 2655
    },
    {
      "epoch": 5.312,
      "grad_norm": 1.0474239587783813,
      "learning_rate": 9.379751900760305e-05,
      "loss": 0.1543,
      "step": 2656
    },
    {
      "epoch": 5.314,
      "grad_norm": 1.0721426010131836,
      "learning_rate": 9.375750300120048e-05,
      "loss": 0.1429,
      "step": 2657
    },
    {
      "epoch": 5.316,
      "grad_norm": 0.9877137541770935,
      "learning_rate": 9.371748699479792e-05,
      "loss": 0.1444,
      "step": 2658
    },
    {
      "epoch": 5.318,
      "grad_norm": 0.9941511750221252,
      "learning_rate": 9.367747098839537e-05,
      "loss": 0.166,
      "step": 2659
    },
    {
      "epoch": 5.32,
      "grad_norm": 1.3377450704574585,
      "learning_rate": 9.36374549819928e-05,
      "loss": 0.1614,
      "step": 2660
    },
    {
      "epoch": 5.322,
      "grad_norm": 1.1995809078216553,
      "learning_rate": 9.359743897559024e-05,
      "loss": 0.1481,
      "step": 2661
    },
    {
      "epoch": 5.324,
      "grad_norm": 0.9261762499809265,
      "learning_rate": 9.355742296918768e-05,
      "loss": 0.1334,
      "step": 2662
    },
    {
      "epoch": 5.326,
      "grad_norm": 0.8315647840499878,
      "learning_rate": 9.351740696278512e-05,
      "loss": 0.1093,
      "step": 2663
    },
    {
      "epoch": 5.328,
      "grad_norm": 1.268512487411499,
      "learning_rate": 9.347739095638256e-05,
      "loss": 0.1611,
      "step": 2664
    },
    {
      "epoch": 5.33,
      "grad_norm": 1.1295340061187744,
      "learning_rate": 9.343737494997999e-05,
      "loss": 0.1395,
      "step": 2665
    },
    {
      "epoch": 5.332,
      "grad_norm": 1.611478328704834,
      "learning_rate": 9.339735894357744e-05,
      "loss": 0.1729,
      "step": 2666
    },
    {
      "epoch": 5.334,
      "grad_norm": 1.0311250686645508,
      "learning_rate": 9.335734293717488e-05,
      "loss": 0.1363,
      "step": 2667
    },
    {
      "epoch": 5.336,
      "grad_norm": 1.504193663597107,
      "learning_rate": 9.33173269307723e-05,
      "loss": 0.1777,
      "step": 2668
    },
    {
      "epoch": 5.338,
      "grad_norm": 1.594266414642334,
      "learning_rate": 9.327731092436976e-05,
      "loss": 0.2093,
      "step": 2669
    },
    {
      "epoch": 5.34,
      "grad_norm": 3.009249210357666,
      "learning_rate": 9.323729491796719e-05,
      "loss": 0.1869,
      "step": 2670
    },
    {
      "epoch": 5.342,
      "grad_norm": 1.216748833656311,
      "learning_rate": 9.319727891156463e-05,
      "loss": 0.1937,
      "step": 2671
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.6198570728302002,
      "learning_rate": 9.315726290516207e-05,
      "loss": 0.128,
      "step": 2672
    },
    {
      "epoch": 5.346,
      "grad_norm": 0.9938380718231201,
      "learning_rate": 9.311724689875952e-05,
      "loss": 0.1682,
      "step": 2673
    },
    {
      "epoch": 5.348,
      "grad_norm": 1.3596245050430298,
      "learning_rate": 9.307723089235695e-05,
      "loss": 0.1969,
      "step": 2674
    },
    {
      "epoch": 5.35,
      "grad_norm": 1.5104167461395264,
      "learning_rate": 9.303721488595438e-05,
      "loss": 0.1753,
      "step": 2675
    },
    {
      "epoch": 5.352,
      "grad_norm": 1.0129567384719849,
      "learning_rate": 9.299719887955183e-05,
      "loss": 0.1689,
      "step": 2676
    },
    {
      "epoch": 5.354,
      "grad_norm": 1.507903814315796,
      "learning_rate": 9.295718287314927e-05,
      "loss": 0.1897,
      "step": 2677
    },
    {
      "epoch": 5.356,
      "grad_norm": 0.9799872040748596,
      "learning_rate": 9.29171668667467e-05,
      "loss": 0.133,
      "step": 2678
    },
    {
      "epoch": 5.358,
      "grad_norm": 0.8838467001914978,
      "learning_rate": 9.287715086034414e-05,
      "loss": 0.1603,
      "step": 2679
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.6749301552772522,
      "learning_rate": 9.283713485394158e-05,
      "loss": 0.146,
      "step": 2680
    },
    {
      "epoch": 5.362,
      "grad_norm": 1.06364107131958,
      "learning_rate": 9.279711884753903e-05,
      "loss": 0.1336,
      "step": 2681
    },
    {
      "epoch": 5.364,
      "grad_norm": 1.6324002742767334,
      "learning_rate": 9.275710284113645e-05,
      "loss": 0.1974,
      "step": 2682
    },
    {
      "epoch": 5.366,
      "grad_norm": 1.1654415130615234,
      "learning_rate": 9.27170868347339e-05,
      "loss": 0.1946,
      "step": 2683
    },
    {
      "epoch": 5.368,
      "grad_norm": 1.161777138710022,
      "learning_rate": 9.267707082833134e-05,
      "loss": 0.1879,
      "step": 2684
    },
    {
      "epoch": 5.37,
      "grad_norm": 1.3356513977050781,
      "learning_rate": 9.263705482192878e-05,
      "loss": 0.1652,
      "step": 2685
    },
    {
      "epoch": 5.372,
      "grad_norm": 2.369960069656372,
      "learning_rate": 9.259703881552621e-05,
      "loss": 0.1809,
      "step": 2686
    },
    {
      "epoch": 5.374,
      "grad_norm": 1.3633458614349365,
      "learning_rate": 9.255702280912365e-05,
      "loss": 0.1428,
      "step": 2687
    },
    {
      "epoch": 5.376,
      "grad_norm": 1.2474697828292847,
      "learning_rate": 9.25170068027211e-05,
      "loss": 0.1679,
      "step": 2688
    },
    {
      "epoch": 5.378,
      "grad_norm": 0.743276834487915,
      "learning_rate": 9.247699079631853e-05,
      "loss": 0.142,
      "step": 2689
    },
    {
      "epoch": 5.38,
      "grad_norm": 2.9208343029022217,
      "learning_rate": 9.243697478991598e-05,
      "loss": 0.1934,
      "step": 2690
    },
    {
      "epoch": 5.382,
      "grad_norm": 1.2972403764724731,
      "learning_rate": 9.239695878351341e-05,
      "loss": 0.1725,
      "step": 2691
    },
    {
      "epoch": 5.384,
      "grad_norm": 1.310160517692566,
      "learning_rate": 9.235694277711085e-05,
      "loss": 0.1398,
      "step": 2692
    },
    {
      "epoch": 5.386,
      "grad_norm": 1.4523122310638428,
      "learning_rate": 9.231692677070829e-05,
      "loss": 0.1606,
      "step": 2693
    },
    {
      "epoch": 5.388,
      "grad_norm": 1.003290057182312,
      "learning_rate": 9.227691076430572e-05,
      "loss": 0.1444,
      "step": 2694
    },
    {
      "epoch": 5.39,
      "grad_norm": 1.1623278856277466,
      "learning_rate": 9.223689475790317e-05,
      "loss": 0.1717,
      "step": 2695
    },
    {
      "epoch": 5.392,
      "grad_norm": 1.0240734815597534,
      "learning_rate": 9.21968787515006e-05,
      "loss": 0.1679,
      "step": 2696
    },
    {
      "epoch": 5.394,
      "grad_norm": 0.8633225560188293,
      "learning_rate": 9.215686274509804e-05,
      "loss": 0.1655,
      "step": 2697
    },
    {
      "epoch": 5.396,
      "grad_norm": 1.2183550596237183,
      "learning_rate": 9.211684673869549e-05,
      "loss": 0.1862,
      "step": 2698
    },
    {
      "epoch": 5.398,
      "grad_norm": 1.5212702751159668,
      "learning_rate": 9.207683073229292e-05,
      "loss": 0.2334,
      "step": 2699
    },
    {
      "epoch": 5.4,
      "grad_norm": 1.606221079826355,
      "learning_rate": 9.203681472589036e-05,
      "loss": 0.199,
      "step": 2700
    },
    {
      "epoch": 5.402,
      "grad_norm": 2.233160972595215,
      "learning_rate": 9.19967987194878e-05,
      "loss": 0.1699,
      "step": 2701
    },
    {
      "epoch": 5.404,
      "grad_norm": 1.010122537612915,
      "learning_rate": 9.195678271308525e-05,
      "loss": 0.1633,
      "step": 2702
    },
    {
      "epoch": 5.406,
      "grad_norm": 1.2891570329666138,
      "learning_rate": 9.191676670668267e-05,
      "loss": 0.1464,
      "step": 2703
    },
    {
      "epoch": 5.408,
      "grad_norm": 1.0493782758712769,
      "learning_rate": 9.187675070028011e-05,
      "loss": 0.1988,
      "step": 2704
    },
    {
      "epoch": 5.41,
      "grad_norm": 0.6115804314613342,
      "learning_rate": 9.183673469387756e-05,
      "loss": 0.1435,
      "step": 2705
    },
    {
      "epoch": 5.412,
      "grad_norm": 0.9118788242340088,
      "learning_rate": 9.1796718687475e-05,
      "loss": 0.1438,
      "step": 2706
    },
    {
      "epoch": 5.414,
      "grad_norm": 0.8921048641204834,
      "learning_rate": 9.175670268107243e-05,
      "loss": 0.1248,
      "step": 2707
    },
    {
      "epoch": 5.416,
      "grad_norm": 1.2892524003982544,
      "learning_rate": 9.171668667466987e-05,
      "loss": 0.1875,
      "step": 2708
    },
    {
      "epoch": 5.418,
      "grad_norm": 1.3134599924087524,
      "learning_rate": 9.167667066826731e-05,
      "loss": 0.1633,
      "step": 2709
    },
    {
      "epoch": 5.42,
      "grad_norm": 1.5423003435134888,
      "learning_rate": 9.163665466186475e-05,
      "loss": 0.1635,
      "step": 2710
    },
    {
      "epoch": 5.422,
      "grad_norm": 0.678543746471405,
      "learning_rate": 9.159663865546218e-05,
      "loss": 0.1519,
      "step": 2711
    },
    {
      "epoch": 5.424,
      "grad_norm": 1.5808391571044922,
      "learning_rate": 9.155662264905963e-05,
      "loss": 0.1873,
      "step": 2712
    },
    {
      "epoch": 5.426,
      "grad_norm": 0.8083933591842651,
      "learning_rate": 9.151660664265707e-05,
      "loss": 0.1407,
      "step": 2713
    },
    {
      "epoch": 5.428,
      "grad_norm": 0.7480546236038208,
      "learning_rate": 9.147659063625451e-05,
      "loss": 0.145,
      "step": 2714
    },
    {
      "epoch": 5.43,
      "grad_norm": 0.965046226978302,
      "learning_rate": 9.143657462985194e-05,
      "loss": 0.1591,
      "step": 2715
    },
    {
      "epoch": 5.432,
      "grad_norm": 1.682979941368103,
      "learning_rate": 9.139655862344938e-05,
      "loss": 0.187,
      "step": 2716
    },
    {
      "epoch": 5.434,
      "grad_norm": 0.683515727519989,
      "learning_rate": 9.135654261704682e-05,
      "loss": 0.1505,
      "step": 2717
    },
    {
      "epoch": 5.436,
      "grad_norm": 0.8886851668357849,
      "learning_rate": 9.131652661064426e-05,
      "loss": 0.1462,
      "step": 2718
    },
    {
      "epoch": 5.438,
      "grad_norm": 1.0027544498443604,
      "learning_rate": 9.127651060424171e-05,
      "loss": 0.1466,
      "step": 2719
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.270583987236023,
      "learning_rate": 9.123649459783914e-05,
      "loss": 0.2147,
      "step": 2720
    },
    {
      "epoch": 5.442,
      "grad_norm": 0.8047515153884888,
      "learning_rate": 9.119647859143657e-05,
      "loss": 0.1493,
      "step": 2721
    },
    {
      "epoch": 5.444,
      "grad_norm": 1.1375571489334106,
      "learning_rate": 9.115646258503402e-05,
      "loss": 0.1684,
      "step": 2722
    },
    {
      "epoch": 5.446,
      "grad_norm": 1.049697995185852,
      "learning_rate": 9.111644657863146e-05,
      "loss": 0.1542,
      "step": 2723
    },
    {
      "epoch": 5.448,
      "grad_norm": 1.2019479274749756,
      "learning_rate": 9.107643057222889e-05,
      "loss": 0.1799,
      "step": 2724
    },
    {
      "epoch": 5.45,
      "grad_norm": 1.0175957679748535,
      "learning_rate": 9.103641456582633e-05,
      "loss": 0.1638,
      "step": 2725
    },
    {
      "epoch": 5.452,
      "grad_norm": 1.3486641645431519,
      "learning_rate": 9.099639855942378e-05,
      "loss": 0.1937,
      "step": 2726
    },
    {
      "epoch": 5.454,
      "grad_norm": 1.0663745403289795,
      "learning_rate": 9.095638255302122e-05,
      "loss": 0.173,
      "step": 2727
    },
    {
      "epoch": 5.456,
      "grad_norm": 1.4779648780822754,
      "learning_rate": 9.091636654661864e-05,
      "loss": 0.165,
      "step": 2728
    },
    {
      "epoch": 5.458,
      "grad_norm": 1.9480808973312378,
      "learning_rate": 9.087635054021609e-05,
      "loss": 0.2011,
      "step": 2729
    },
    {
      "epoch": 5.46,
      "grad_norm": 1.1040143966674805,
      "learning_rate": 9.083633453381353e-05,
      "loss": 0.1398,
      "step": 2730
    },
    {
      "epoch": 5.462,
      "grad_norm": 1.0328623056411743,
      "learning_rate": 9.079631852741098e-05,
      "loss": 0.1324,
      "step": 2731
    },
    {
      "epoch": 5.464,
      "grad_norm": 1.5310713052749634,
      "learning_rate": 9.07563025210084e-05,
      "loss": 0.2038,
      "step": 2732
    },
    {
      "epoch": 5.466,
      "grad_norm": 1.075818419456482,
      "learning_rate": 9.071628651460584e-05,
      "loss": 0.1637,
      "step": 2733
    },
    {
      "epoch": 5.468,
      "grad_norm": 1.4966117143630981,
      "learning_rate": 9.067627050820329e-05,
      "loss": 0.1932,
      "step": 2734
    },
    {
      "epoch": 5.47,
      "grad_norm": 0.94671231508255,
      "learning_rate": 9.063625450180073e-05,
      "loss": 0.1764,
      "step": 2735
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 1.1886049509048462,
      "learning_rate": 9.059623849539817e-05,
      "loss": 0.2374,
      "step": 2736
    },
    {
      "epoch": 5.474,
      "grad_norm": 1.420032262802124,
      "learning_rate": 9.05562224889956e-05,
      "loss": 0.1675,
      "step": 2737
    },
    {
      "epoch": 5.476,
      "grad_norm": 1.017491340637207,
      "learning_rate": 9.051620648259305e-05,
      "loss": 0.161,
      "step": 2738
    },
    {
      "epoch": 5.478,
      "grad_norm": 0.9183212518692017,
      "learning_rate": 9.047619047619048e-05,
      "loss": 0.1416,
      "step": 2739
    },
    {
      "epoch": 5.48,
      "grad_norm": 1.4469069242477417,
      "learning_rate": 9.043617446978791e-05,
      "loss": 0.148,
      "step": 2740
    },
    {
      "epoch": 5.482,
      "grad_norm": 0.9530432820320129,
      "learning_rate": 9.039615846338536e-05,
      "loss": 0.1501,
      "step": 2741
    },
    {
      "epoch": 5.484,
      "grad_norm": 1.8158379793167114,
      "learning_rate": 9.03561424569828e-05,
      "loss": 0.2806,
      "step": 2742
    },
    {
      "epoch": 5.486,
      "grad_norm": 1.1607074737548828,
      "learning_rate": 9.031612645058024e-05,
      "loss": 0.1653,
      "step": 2743
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 1.1346595287322998,
      "learning_rate": 9.027611044417768e-05,
      "loss": 0.1869,
      "step": 2744
    },
    {
      "epoch": 5.49,
      "grad_norm": 1.7107492685317993,
      "learning_rate": 9.023609443777511e-05,
      "loss": 0.1845,
      "step": 2745
    },
    {
      "epoch": 5.492,
      "grad_norm": 1.052942156791687,
      "learning_rate": 9.019607843137255e-05,
      "loss": 0.1609,
      "step": 2746
    },
    {
      "epoch": 5.494,
      "grad_norm": 1.2906266450881958,
      "learning_rate": 9.015606242496999e-05,
      "loss": 0.1556,
      "step": 2747
    },
    {
      "epoch": 5.496,
      "grad_norm": 1.302268385887146,
      "learning_rate": 9.011604641856744e-05,
      "loss": 0.1932,
      "step": 2748
    },
    {
      "epoch": 5.498,
      "grad_norm": 0.7175474762916565,
      "learning_rate": 9.007603041216488e-05,
      "loss": 0.1691,
      "step": 2749
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.7873258590698242,
      "learning_rate": 9.003601440576231e-05,
      "loss": 0.1686,
      "step": 2750
    },
    {
      "epoch": 5.502,
      "grad_norm": 0.9411194920539856,
      "learning_rate": 8.999599839935975e-05,
      "loss": 0.1309,
      "step": 2751
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.8494802713394165,
      "learning_rate": 8.995598239295719e-05,
      "loss": 0.1551,
      "step": 2752
    },
    {
      "epoch": 5.506,
      "grad_norm": 1.0122711658477783,
      "learning_rate": 8.991596638655462e-05,
      "loss": 0.1764,
      "step": 2753
    },
    {
      "epoch": 5.508,
      "grad_norm": 1.0114802122116089,
      "learning_rate": 8.987595038015206e-05,
      "loss": 0.1419,
      "step": 2754
    },
    {
      "epoch": 5.51,
      "grad_norm": 1.6478325128555298,
      "learning_rate": 8.983593437374951e-05,
      "loss": 0.1933,
      "step": 2755
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 1.0733364820480347,
      "learning_rate": 8.979591836734695e-05,
      "loss": 0.1445,
      "step": 2756
    },
    {
      "epoch": 5.514,
      "grad_norm": 0.9981982707977295,
      "learning_rate": 8.975590236094437e-05,
      "loss": 0.1749,
      "step": 2757
    },
    {
      "epoch": 5.516,
      "grad_norm": 0.9304218292236328,
      "learning_rate": 8.971588635454182e-05,
      "loss": 0.1182,
      "step": 2758
    },
    {
      "epoch": 5.518,
      "grad_norm": 1.7013976573944092,
      "learning_rate": 8.967587034813926e-05,
      "loss": 0.1663,
      "step": 2759
    },
    {
      "epoch": 5.52,
      "grad_norm": 1.5274337530136108,
      "learning_rate": 8.96358543417367e-05,
      "loss": 0.195,
      "step": 2760
    },
    {
      "epoch": 5.522,
      "grad_norm": 1.08582603931427,
      "learning_rate": 8.959583833533413e-05,
      "loss": 0.1592,
      "step": 2761
    },
    {
      "epoch": 5.524,
      "grad_norm": 0.9488778710365295,
      "learning_rate": 8.955582232893158e-05,
      "loss": 0.152,
      "step": 2762
    },
    {
      "epoch": 5.526,
      "grad_norm": 1.0515868663787842,
      "learning_rate": 8.951580632252902e-05,
      "loss": 0.1376,
      "step": 2763
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 1.3679248094558716,
      "learning_rate": 8.947579031612645e-05,
      "loss": 0.1882,
      "step": 2764
    },
    {
      "epoch": 5.53,
      "grad_norm": 1.3521078824996948,
      "learning_rate": 8.94357743097239e-05,
      "loss": 0.1594,
      "step": 2765
    },
    {
      "epoch": 5.532,
      "grad_norm": 1.0969393253326416,
      "learning_rate": 8.939575830332133e-05,
      "loss": 0.1323,
      "step": 2766
    },
    {
      "epoch": 5.534,
      "grad_norm": 1.5131367444992065,
      "learning_rate": 8.935574229691877e-05,
      "loss": 0.1842,
      "step": 2767
    },
    {
      "epoch": 5.536,
      "grad_norm": 2.1452605724334717,
      "learning_rate": 8.931572629051621e-05,
      "loss": 0.1968,
      "step": 2768
    },
    {
      "epoch": 5.538,
      "grad_norm": 1.4110251665115356,
      "learning_rate": 8.927571028411364e-05,
      "loss": 0.1717,
      "step": 2769
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.9516535997390747,
      "learning_rate": 8.92356942777111e-05,
      "loss": 0.1773,
      "step": 2770
    },
    {
      "epoch": 5.542,
      "grad_norm": 1.2806122303009033,
      "learning_rate": 8.919567827130852e-05,
      "loss": 0.166,
      "step": 2771
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 1.0910874605178833,
      "learning_rate": 8.915566226490597e-05,
      "loss": 0.1567,
      "step": 2772
    },
    {
      "epoch": 5.546,
      "grad_norm": 1.485207200050354,
      "learning_rate": 8.911564625850341e-05,
      "loss": 0.2001,
      "step": 2773
    },
    {
      "epoch": 5.548,
      "grad_norm": 0.8098435401916504,
      "learning_rate": 8.907563025210084e-05,
      "loss": 0.1519,
      "step": 2774
    },
    {
      "epoch": 5.55,
      "grad_norm": 1.0903819799423218,
      "learning_rate": 8.903561424569828e-05,
      "loss": 0.1608,
      "step": 2775
    },
    {
      "epoch": 5.552,
      "grad_norm": 2.206836223602295,
      "learning_rate": 8.899559823929572e-05,
      "loss": 0.1777,
      "step": 2776
    },
    {
      "epoch": 5.554,
      "grad_norm": 1.325911521911621,
      "learning_rate": 8.895558223289317e-05,
      "loss": 0.1853,
      "step": 2777
    },
    {
      "epoch": 5.556,
      "grad_norm": 1.016432762145996,
      "learning_rate": 8.891556622649059e-05,
      "loss": 0.158,
      "step": 2778
    },
    {
      "epoch": 5.558,
      "grad_norm": 1.0444210767745972,
      "learning_rate": 8.887555022008804e-05,
      "loss": 0.1441,
      "step": 2779
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 1.380905032157898,
      "learning_rate": 8.883553421368548e-05,
      "loss": 0.1951,
      "step": 2780
    },
    {
      "epoch": 5.562,
      "grad_norm": 0.7933446764945984,
      "learning_rate": 8.879551820728292e-05,
      "loss": 0.1321,
      "step": 2781
    },
    {
      "epoch": 5.564,
      "grad_norm": 1.0649676322937012,
      "learning_rate": 8.875550220088035e-05,
      "loss": 0.1263,
      "step": 2782
    },
    {
      "epoch": 5.566,
      "grad_norm": 1.2759513854980469,
      "learning_rate": 8.871548619447779e-05,
      "loss": 0.1638,
      "step": 2783
    },
    {
      "epoch": 5.568,
      "grad_norm": 1.1431505680084229,
      "learning_rate": 8.867547018807524e-05,
      "loss": 0.1699,
      "step": 2784
    },
    {
      "epoch": 5.57,
      "grad_norm": 1.2334423065185547,
      "learning_rate": 8.863545418167267e-05,
      "loss": 0.2344,
      "step": 2785
    },
    {
      "epoch": 5.572,
      "grad_norm": 1.4579087495803833,
      "learning_rate": 8.85954381752701e-05,
      "loss": 0.2025,
      "step": 2786
    },
    {
      "epoch": 5.574,
      "grad_norm": 0.8487436175346375,
      "learning_rate": 8.855542216886755e-05,
      "loss": 0.1564,
      "step": 2787
    },
    {
      "epoch": 5.576,
      "grad_norm": 1.397694706916809,
      "learning_rate": 8.851540616246499e-05,
      "loss": 0.1586,
      "step": 2788
    },
    {
      "epoch": 5.578,
      "grad_norm": 0.8569777011871338,
      "learning_rate": 8.847539015606243e-05,
      "loss": 0.1715,
      "step": 2789
    },
    {
      "epoch": 5.58,
      "grad_norm": 2.1573452949523926,
      "learning_rate": 8.843537414965987e-05,
      "loss": 0.2055,
      "step": 2790
    },
    {
      "epoch": 5.582,
      "grad_norm": 1.0538201332092285,
      "learning_rate": 8.839535814325732e-05,
      "loss": 0.1511,
      "step": 2791
    },
    {
      "epoch": 5.584,
      "grad_norm": 1.0492686033248901,
      "learning_rate": 8.835534213685474e-05,
      "loss": 0.1627,
      "step": 2792
    },
    {
      "epoch": 5.586,
      "grad_norm": 1.0229994058609009,
      "learning_rate": 8.831532613045218e-05,
      "loss": 0.1603,
      "step": 2793
    },
    {
      "epoch": 5.588,
      "grad_norm": 1.0260286331176758,
      "learning_rate": 8.827531012404963e-05,
      "loss": 0.1478,
      "step": 2794
    },
    {
      "epoch": 5.59,
      "grad_norm": 1.5812575817108154,
      "learning_rate": 8.823529411764706e-05,
      "loss": 0.158,
      "step": 2795
    },
    {
      "epoch": 5.592,
      "grad_norm": 1.0605510473251343,
      "learning_rate": 8.81952781112445e-05,
      "loss": 0.1495,
      "step": 2796
    },
    {
      "epoch": 5.594,
      "grad_norm": 1.309125542640686,
      "learning_rate": 8.815526210484194e-05,
      "loss": 0.1559,
      "step": 2797
    },
    {
      "epoch": 5.596,
      "grad_norm": 0.9763482809066772,
      "learning_rate": 8.811524609843938e-05,
      "loss": 0.164,
      "step": 2798
    },
    {
      "epoch": 5.598,
      "grad_norm": 0.9639540314674377,
      "learning_rate": 8.807523009203683e-05,
      "loss": 0.1563,
      "step": 2799
    },
    {
      "epoch": 5.6,
      "grad_norm": 1.4033781290054321,
      "learning_rate": 8.803521408563425e-05,
      "loss": 0.1757,
      "step": 2800
    },
    {
      "epoch": 5.602,
      "grad_norm": 1.6288472414016724,
      "learning_rate": 8.79951980792317e-05,
      "loss": 0.195,
      "step": 2801
    },
    {
      "epoch": 5.604,
      "grad_norm": 1.3824827671051025,
      "learning_rate": 8.795518207282914e-05,
      "loss": 0.1792,
      "step": 2802
    },
    {
      "epoch": 5.606,
      "grad_norm": 0.7470564246177673,
      "learning_rate": 8.791516606642658e-05,
      "loss": 0.143,
      "step": 2803
    },
    {
      "epoch": 5.608,
      "grad_norm": 1.2553268671035767,
      "learning_rate": 8.787515006002401e-05,
      "loss": 0.2355,
      "step": 2804
    },
    {
      "epoch": 5.61,
      "grad_norm": 1.2193012237548828,
      "learning_rate": 8.783513405362145e-05,
      "loss": 0.1361,
      "step": 2805
    },
    {
      "epoch": 5.612,
      "grad_norm": 1.6106443405151367,
      "learning_rate": 8.77951180472189e-05,
      "loss": 0.2038,
      "step": 2806
    },
    {
      "epoch": 5.614,
      "grad_norm": 1.0198829174041748,
      "learning_rate": 8.775510204081632e-05,
      "loss": 0.1711,
      "step": 2807
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.9975561499595642,
      "learning_rate": 8.771508603441377e-05,
      "loss": 0.1501,
      "step": 2808
    },
    {
      "epoch": 5.618,
      "grad_norm": 1.1151012182235718,
      "learning_rate": 8.767507002801121e-05,
      "loss": 0.1752,
      "step": 2809
    },
    {
      "epoch": 5.62,
      "grad_norm": 1.1382629871368408,
      "learning_rate": 8.763505402160865e-05,
      "loss": 0.147,
      "step": 2810
    },
    {
      "epoch": 5.622,
      "grad_norm": 1.3395345211029053,
      "learning_rate": 8.759503801520609e-05,
      "loss": 0.201,
      "step": 2811
    },
    {
      "epoch": 5.624,
      "grad_norm": 1.683608055114746,
      "learning_rate": 8.755502200880352e-05,
      "loss": 0.1945,
      "step": 2812
    },
    {
      "epoch": 5.626,
      "grad_norm": 1.1883211135864258,
      "learning_rate": 8.751500600240097e-05,
      "loss": 0.1737,
      "step": 2813
    },
    {
      "epoch": 5.628,
      "grad_norm": 0.7856638431549072,
      "learning_rate": 8.74749899959984e-05,
      "loss": 0.1231,
      "step": 2814
    },
    {
      "epoch": 5.63,
      "grad_norm": 0.9566314816474915,
      "learning_rate": 8.743497398959585e-05,
      "loss": 0.1578,
      "step": 2815
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.9058823585510254,
      "learning_rate": 8.739495798319329e-05,
      "loss": 0.1647,
      "step": 2816
    },
    {
      "epoch": 5.634,
      "grad_norm": 0.7787922024726868,
      "learning_rate": 8.735494197679072e-05,
      "loss": 0.149,
      "step": 2817
    },
    {
      "epoch": 5.636,
      "grad_norm": 1.037751317024231,
      "learning_rate": 8.731492597038816e-05,
      "loss": 0.1765,
      "step": 2818
    },
    {
      "epoch": 5.638,
      "grad_norm": 1.0433692932128906,
      "learning_rate": 8.72749099639856e-05,
      "loss": 0.1628,
      "step": 2819
    },
    {
      "epoch": 5.64,
      "grad_norm": 1.7793387174606323,
      "learning_rate": 8.723489395758305e-05,
      "loss": 0.205,
      "step": 2820
    },
    {
      "epoch": 5.642,
      "grad_norm": 1.2766352891921997,
      "learning_rate": 8.719487795118047e-05,
      "loss": 0.1558,
      "step": 2821
    },
    {
      "epoch": 5.644,
      "grad_norm": 1.0905832052230835,
      "learning_rate": 8.715486194477791e-05,
      "loss": 0.1571,
      "step": 2822
    },
    {
      "epoch": 5.646,
      "grad_norm": 0.9547313451766968,
      "learning_rate": 8.711484593837536e-05,
      "loss": 0.1276,
      "step": 2823
    },
    {
      "epoch": 5.648,
      "grad_norm": 1.1608250141143799,
      "learning_rate": 8.70748299319728e-05,
      "loss": 0.1952,
      "step": 2824
    },
    {
      "epoch": 5.65,
      "grad_norm": 1.0511682033538818,
      "learning_rate": 8.703481392557023e-05,
      "loss": 0.1597,
      "step": 2825
    },
    {
      "epoch": 5.652,
      "grad_norm": 0.8044440150260925,
      "learning_rate": 8.699479791916767e-05,
      "loss": 0.1549,
      "step": 2826
    },
    {
      "epoch": 5.654,
      "grad_norm": 1.4928693771362305,
      "learning_rate": 8.695478191276512e-05,
      "loss": 0.1745,
      "step": 2827
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.7658103704452515,
      "learning_rate": 8.691476590636254e-05,
      "loss": 0.1697,
      "step": 2828
    },
    {
      "epoch": 5.658,
      "grad_norm": 0.88742595911026,
      "learning_rate": 8.687474989995998e-05,
      "loss": 0.1727,
      "step": 2829
    },
    {
      "epoch": 5.66,
      "grad_norm": 0.946435272693634,
      "learning_rate": 8.683473389355743e-05,
      "loss": 0.1598,
      "step": 2830
    },
    {
      "epoch": 5.662,
      "grad_norm": 1.6622363328933716,
      "learning_rate": 8.679471788715487e-05,
      "loss": 0.1816,
      "step": 2831
    },
    {
      "epoch": 5.664,
      "grad_norm": 1.731953501701355,
      "learning_rate": 8.67547018807523e-05,
      "loss": 0.1688,
      "step": 2832
    },
    {
      "epoch": 5.666,
      "grad_norm": 1.1749235391616821,
      "learning_rate": 8.671468587434974e-05,
      "loss": 0.1903,
      "step": 2833
    },
    {
      "epoch": 5.668,
      "grad_norm": 1.6618598699569702,
      "learning_rate": 8.667466986794718e-05,
      "loss": 0.1878,
      "step": 2834
    },
    {
      "epoch": 5.67,
      "grad_norm": 1.3001548051834106,
      "learning_rate": 8.663465386154462e-05,
      "loss": 0.1728,
      "step": 2835
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.8443682789802551,
      "learning_rate": 8.659463785514205e-05,
      "loss": 0.2028,
      "step": 2836
    },
    {
      "epoch": 5.674,
      "grad_norm": 1.5564179420471191,
      "learning_rate": 8.65546218487395e-05,
      "loss": 0.174,
      "step": 2837
    },
    {
      "epoch": 5.676,
      "grad_norm": 1.5463000535964966,
      "learning_rate": 8.651460584233694e-05,
      "loss": 0.1512,
      "step": 2838
    },
    {
      "epoch": 5.678,
      "grad_norm": 1.6846290826797485,
      "learning_rate": 8.647458983593438e-05,
      "loss": 0.1713,
      "step": 2839
    },
    {
      "epoch": 5.68,
      "grad_norm": 1.7590718269348145,
      "learning_rate": 8.643457382953182e-05,
      "loss": 0.2246,
      "step": 2840
    },
    {
      "epoch": 5.682,
      "grad_norm": 0.8371474146842957,
      "learning_rate": 8.639455782312925e-05,
      "loss": 0.1558,
      "step": 2841
    },
    {
      "epoch": 5.684,
      "grad_norm": 1.1964936256408691,
      "learning_rate": 8.635454181672669e-05,
      "loss": 0.175,
      "step": 2842
    },
    {
      "epoch": 5.686,
      "grad_norm": 0.7789863348007202,
      "learning_rate": 8.631452581032413e-05,
      "loss": 0.1351,
      "step": 2843
    },
    {
      "epoch": 5.688,
      "grad_norm": 1.2817267179489136,
      "learning_rate": 8.627450980392158e-05,
      "loss": 0.1439,
      "step": 2844
    },
    {
      "epoch": 5.6899999999999995,
      "grad_norm": 1.7812358140945435,
      "learning_rate": 8.623449379751902e-05,
      "loss": 0.2206,
      "step": 2845
    },
    {
      "epoch": 5.692,
      "grad_norm": 1.105400800704956,
      "learning_rate": 8.619447779111644e-05,
      "loss": 0.1642,
      "step": 2846
    },
    {
      "epoch": 5.694,
      "grad_norm": 0.8845281004905701,
      "learning_rate": 8.615446178471389e-05,
      "loss": 0.1549,
      "step": 2847
    },
    {
      "epoch": 5.696,
      "grad_norm": 1.7302403450012207,
      "learning_rate": 8.611444577831133e-05,
      "loss": 0.1806,
      "step": 2848
    },
    {
      "epoch": 5.698,
      "grad_norm": 0.8710479140281677,
      "learning_rate": 8.607442977190876e-05,
      "loss": 0.1546,
      "step": 2849
    },
    {
      "epoch": 5.7,
      "grad_norm": 1.1733051538467407,
      "learning_rate": 8.60344137655062e-05,
      "loss": 0.1664,
      "step": 2850
    },
    {
      "epoch": 5.702,
      "grad_norm": 1.344902753829956,
      "learning_rate": 8.599439775910365e-05,
      "loss": 0.2102,
      "step": 2851
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.7782883644104004,
      "learning_rate": 8.595438175270109e-05,
      "loss": 0.1223,
      "step": 2852
    },
    {
      "epoch": 5.7059999999999995,
      "grad_norm": 1.3399826288223267,
      "learning_rate": 8.591436574629851e-05,
      "loss": 0.1757,
      "step": 2853
    },
    {
      "epoch": 5.708,
      "grad_norm": 0.9219127893447876,
      "learning_rate": 8.587434973989596e-05,
      "loss": 0.1401,
      "step": 2854
    },
    {
      "epoch": 5.71,
      "grad_norm": 0.8143344521522522,
      "learning_rate": 8.58343337334934e-05,
      "loss": 0.1433,
      "step": 2855
    },
    {
      "epoch": 5.712,
      "grad_norm": 1.36189603805542,
      "learning_rate": 8.579431772709084e-05,
      "loss": 0.1872,
      "step": 2856
    },
    {
      "epoch": 5.714,
      "grad_norm": 1.350937843322754,
      "learning_rate": 8.575430172068828e-05,
      "loss": 0.1459,
      "step": 2857
    },
    {
      "epoch": 5.716,
      "grad_norm": 1.1871527433395386,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.1642,
      "step": 2858
    },
    {
      "epoch": 5.718,
      "grad_norm": 0.8505879044532776,
      "learning_rate": 8.567426970788316e-05,
      "loss": 0.156,
      "step": 2859
    },
    {
      "epoch": 5.72,
      "grad_norm": 1.0386691093444824,
      "learning_rate": 8.563425370148059e-05,
      "loss": 0.1455,
      "step": 2860
    },
    {
      "epoch": 5.7219999999999995,
      "grad_norm": 1.2336541414260864,
      "learning_rate": 8.559423769507804e-05,
      "loss": 0.1857,
      "step": 2861
    },
    {
      "epoch": 5.724,
      "grad_norm": 1.3101611137390137,
      "learning_rate": 8.555422168867547e-05,
      "loss": 0.163,
      "step": 2862
    },
    {
      "epoch": 5.726,
      "grad_norm": 1.0849353075027466,
      "learning_rate": 8.551420568227293e-05,
      "loss": 0.129,
      "step": 2863
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.6580145359039307,
      "learning_rate": 8.547418967587035e-05,
      "loss": 0.1603,
      "step": 2864
    },
    {
      "epoch": 5.73,
      "grad_norm": 4.69569206237793,
      "learning_rate": 8.543417366946779e-05,
      "loss": 0.2685,
      "step": 2865
    },
    {
      "epoch": 5.732,
      "grad_norm": 1.0879943370819092,
      "learning_rate": 8.539415766306524e-05,
      "loss": 0.1676,
      "step": 2866
    },
    {
      "epoch": 5.734,
      "grad_norm": 0.8452640175819397,
      "learning_rate": 8.535414165666267e-05,
      "loss": 0.1385,
      "step": 2867
    },
    {
      "epoch": 5.736,
      "grad_norm": 1.2710061073303223,
      "learning_rate": 8.531412565026011e-05,
      "loss": 0.1689,
      "step": 2868
    },
    {
      "epoch": 5.7379999999999995,
      "grad_norm": 1.0679810047149658,
      "learning_rate": 8.527410964385755e-05,
      "loss": 0.1363,
      "step": 2869
    },
    {
      "epoch": 5.74,
      "grad_norm": 1.2024277448654175,
      "learning_rate": 8.523409363745499e-05,
      "loss": 0.1705,
      "step": 2870
    },
    {
      "epoch": 5.742,
      "grad_norm": 1.1347788572311401,
      "learning_rate": 8.519407763105242e-05,
      "loss": 0.1503,
      "step": 2871
    },
    {
      "epoch": 5.744,
      "grad_norm": 1.2009177207946777,
      "learning_rate": 8.515406162464986e-05,
      "loss": 0.152,
      "step": 2872
    },
    {
      "epoch": 5.746,
      "grad_norm": 1.3889691829681396,
      "learning_rate": 8.511404561824731e-05,
      "loss": 0.2089,
      "step": 2873
    },
    {
      "epoch": 5.748,
      "grad_norm": 1.1937358379364014,
      "learning_rate": 8.507402961184475e-05,
      "loss": 0.1626,
      "step": 2874
    },
    {
      "epoch": 5.75,
      "grad_norm": 1.0305843353271484,
      "learning_rate": 8.503401360544217e-05,
      "loss": 0.1715,
      "step": 2875
    },
    {
      "epoch": 5.752,
      "grad_norm": 1.1192471981048584,
      "learning_rate": 8.499399759903962e-05,
      "loss": 0.151,
      "step": 2876
    },
    {
      "epoch": 5.754,
      "grad_norm": 1.1006090641021729,
      "learning_rate": 8.495398159263706e-05,
      "loss": 0.1886,
      "step": 2877
    },
    {
      "epoch": 5.756,
      "grad_norm": 0.7850536108016968,
      "learning_rate": 8.49139655862345e-05,
      "loss": 0.1382,
      "step": 2878
    },
    {
      "epoch": 5.758,
      "grad_norm": 1.2594168186187744,
      "learning_rate": 8.487394957983193e-05,
      "loss": 0.1722,
      "step": 2879
    },
    {
      "epoch": 5.76,
      "grad_norm": 1.1139106750488281,
      "learning_rate": 8.483393357342938e-05,
      "loss": 0.1487,
      "step": 2880
    },
    {
      "epoch": 5.7620000000000005,
      "grad_norm": 1.1216740608215332,
      "learning_rate": 8.479391756702682e-05,
      "loss": 0.1526,
      "step": 2881
    },
    {
      "epoch": 5.764,
      "grad_norm": 1.6598830223083496,
      "learning_rate": 8.475390156062424e-05,
      "loss": 0.2021,
      "step": 2882
    },
    {
      "epoch": 5.766,
      "grad_norm": 1.5519577264785767,
      "learning_rate": 8.47138855542217e-05,
      "loss": 0.1914,
      "step": 2883
    },
    {
      "epoch": 5.768,
      "grad_norm": 1.740234375,
      "learning_rate": 8.467386954781913e-05,
      "loss": 0.1772,
      "step": 2884
    },
    {
      "epoch": 5.77,
      "grad_norm": 0.8949241638183594,
      "learning_rate": 8.463385354141657e-05,
      "loss": 0.1444,
      "step": 2885
    },
    {
      "epoch": 5.772,
      "grad_norm": 0.9771389365196228,
      "learning_rate": 8.4593837535014e-05,
      "loss": 0.167,
      "step": 2886
    },
    {
      "epoch": 5.774,
      "grad_norm": 1.458380937576294,
      "learning_rate": 8.455382152861144e-05,
      "loss": 0.1752,
      "step": 2887
    },
    {
      "epoch": 5.776,
      "grad_norm": 1.229880452156067,
      "learning_rate": 8.45138055222089e-05,
      "loss": 0.192,
      "step": 2888
    },
    {
      "epoch": 5.7780000000000005,
      "grad_norm": 1.6659319400787354,
      "learning_rate": 8.447378951580632e-05,
      "loss": 0.216,
      "step": 2889
    },
    {
      "epoch": 5.78,
      "grad_norm": 1.3891276121139526,
      "learning_rate": 8.443377350940377e-05,
      "loss": 0.1826,
      "step": 2890
    },
    {
      "epoch": 5.782,
      "grad_norm": 0.8790585994720459,
      "learning_rate": 8.43937575030012e-05,
      "loss": 0.159,
      "step": 2891
    },
    {
      "epoch": 5.784,
      "grad_norm": 1.2419273853302002,
      "learning_rate": 8.435374149659864e-05,
      "loss": 0.1856,
      "step": 2892
    },
    {
      "epoch": 5.786,
      "grad_norm": 0.9891931414604187,
      "learning_rate": 8.431372549019608e-05,
      "loss": 0.1517,
      "step": 2893
    },
    {
      "epoch": 5.788,
      "grad_norm": 1.1689677238464355,
      "learning_rate": 8.427370948379352e-05,
      "loss": 0.1572,
      "step": 2894
    },
    {
      "epoch": 5.79,
      "grad_norm": 0.6782709956169128,
      "learning_rate": 8.423369347739097e-05,
      "loss": 0.1559,
      "step": 2895
    },
    {
      "epoch": 5.792,
      "grad_norm": 1.1653650999069214,
      "learning_rate": 8.419367747098839e-05,
      "loss": 0.194,
      "step": 2896
    },
    {
      "epoch": 5.7940000000000005,
      "grad_norm": 0.8830909729003906,
      "learning_rate": 8.415366146458584e-05,
      "loss": 0.1467,
      "step": 2897
    },
    {
      "epoch": 5.796,
      "grad_norm": 1.0002282857894897,
      "learning_rate": 8.411364545818328e-05,
      "loss": 0.1649,
      "step": 2898
    },
    {
      "epoch": 5.798,
      "grad_norm": 1.0779255628585815,
      "learning_rate": 8.407362945178072e-05,
      "loss": 0.1572,
      "step": 2899
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.9394637942314148,
      "learning_rate": 8.403361344537815e-05,
      "loss": 0.1596,
      "step": 2900
    },
    {
      "epoch": 5.802,
      "grad_norm": 1.2583816051483154,
      "learning_rate": 8.399359743897559e-05,
      "loss": 0.1573,
      "step": 2901
    },
    {
      "epoch": 5.804,
      "grad_norm": 1.4752956628799438,
      "learning_rate": 8.395358143257304e-05,
      "loss": 0.2368,
      "step": 2902
    },
    {
      "epoch": 5.806,
      "grad_norm": 1.7088565826416016,
      "learning_rate": 8.391356542617046e-05,
      "loss": 0.2088,
      "step": 2903
    },
    {
      "epoch": 5.808,
      "grad_norm": 1.045958161354065,
      "learning_rate": 8.387354941976792e-05,
      "loss": 0.1818,
      "step": 2904
    },
    {
      "epoch": 5.8100000000000005,
      "grad_norm": 0.8727527856826782,
      "learning_rate": 8.383353341336535e-05,
      "loss": 0.136,
      "step": 2905
    },
    {
      "epoch": 5.812,
      "grad_norm": 1.3497202396392822,
      "learning_rate": 8.379351740696279e-05,
      "loss": 0.1808,
      "step": 2906
    },
    {
      "epoch": 5.814,
      "grad_norm": 1.18924081325531,
      "learning_rate": 8.375350140056023e-05,
      "loss": 0.169,
      "step": 2907
    },
    {
      "epoch": 5.816,
      "grad_norm": 1.2161598205566406,
      "learning_rate": 8.371348539415766e-05,
      "loss": 0.1535,
      "step": 2908
    },
    {
      "epoch": 5.818,
      "grad_norm": 1.525795340538025,
      "learning_rate": 8.367346938775511e-05,
      "loss": 0.1836,
      "step": 2909
    },
    {
      "epoch": 5.82,
      "grad_norm": 1.4255478382110596,
      "learning_rate": 8.363345338135254e-05,
      "loss": 0.2009,
      "step": 2910
    },
    {
      "epoch": 5.822,
      "grad_norm": 1.1245696544647217,
      "learning_rate": 8.359343737494998e-05,
      "loss": 0.1797,
      "step": 2911
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.8242235779762268,
      "learning_rate": 8.355342136854743e-05,
      "loss": 0.1896,
      "step": 2912
    },
    {
      "epoch": 5.826,
      "grad_norm": 1.3026642799377441,
      "learning_rate": 8.351340536214486e-05,
      "loss": 0.1717,
      "step": 2913
    },
    {
      "epoch": 5.828,
      "grad_norm": 0.8401470184326172,
      "learning_rate": 8.34733893557423e-05,
      "loss": 0.1518,
      "step": 2914
    },
    {
      "epoch": 5.83,
      "grad_norm": 0.8921643495559692,
      "learning_rate": 8.343337334933974e-05,
      "loss": 0.1711,
      "step": 2915
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.9796515703201294,
      "learning_rate": 8.339335734293719e-05,
      "loss": 0.1749,
      "step": 2916
    },
    {
      "epoch": 5.834,
      "grad_norm": 0.9484637975692749,
      "learning_rate": 8.335334133653461e-05,
      "loss": 0.1721,
      "step": 2917
    },
    {
      "epoch": 5.836,
      "grad_norm": 1.4013091325759888,
      "learning_rate": 8.331332533013205e-05,
      "loss": 0.1637,
      "step": 2918
    },
    {
      "epoch": 5.838,
      "grad_norm": 1.2542842626571655,
      "learning_rate": 8.32733093237295e-05,
      "loss": 0.1745,
      "step": 2919
    },
    {
      "epoch": 5.84,
      "grad_norm": 1.582288146018982,
      "learning_rate": 8.323329331732694e-05,
      "loss": 0.2123,
      "step": 2920
    },
    {
      "epoch": 5.842,
      "grad_norm": 1.150987982749939,
      "learning_rate": 8.319327731092437e-05,
      "loss": 0.1743,
      "step": 2921
    },
    {
      "epoch": 5.844,
      "grad_norm": 1.7363104820251465,
      "learning_rate": 8.315326130452181e-05,
      "loss": 0.1893,
      "step": 2922
    },
    {
      "epoch": 5.846,
      "grad_norm": 1.6689691543579102,
      "learning_rate": 8.311324529811925e-05,
      "loss": 0.1527,
      "step": 2923
    },
    {
      "epoch": 5.848,
      "grad_norm": 1.1121801137924194,
      "learning_rate": 8.307322929171669e-05,
      "loss": 0.1788,
      "step": 2924
    },
    {
      "epoch": 5.85,
      "grad_norm": 0.9122114181518555,
      "learning_rate": 8.303321328531412e-05,
      "loss": 0.1776,
      "step": 2925
    },
    {
      "epoch": 5.852,
      "grad_norm": 1.0033767223358154,
      "learning_rate": 8.299319727891157e-05,
      "loss": 0.1521,
      "step": 2926
    },
    {
      "epoch": 5.854,
      "grad_norm": 0.9897469282150269,
      "learning_rate": 8.295318127250901e-05,
      "loss": 0.1382,
      "step": 2927
    },
    {
      "epoch": 5.856,
      "grad_norm": 1.0104769468307495,
      "learning_rate": 8.291316526610645e-05,
      "loss": 0.1712,
      "step": 2928
    },
    {
      "epoch": 5.858,
      "grad_norm": 1.122496247291565,
      "learning_rate": 8.287314925970388e-05,
      "loss": 0.1514,
      "step": 2929
    },
    {
      "epoch": 5.86,
      "grad_norm": 1.081926941871643,
      "learning_rate": 8.283313325330132e-05,
      "loss": 0.1526,
      "step": 2930
    },
    {
      "epoch": 5.862,
      "grad_norm": 0.9374178647994995,
      "learning_rate": 8.279311724689877e-05,
      "loss": 0.1557,
      "step": 2931
    },
    {
      "epoch": 5.864,
      "grad_norm": 1.3345446586608887,
      "learning_rate": 8.27531012404962e-05,
      "loss": 0.152,
      "step": 2932
    },
    {
      "epoch": 5.866,
      "grad_norm": 1.9567303657531738,
      "learning_rate": 8.271308523409365e-05,
      "loss": 0.2093,
      "step": 2933
    },
    {
      "epoch": 5.868,
      "grad_norm": 1.5481352806091309,
      "learning_rate": 8.267306922769108e-05,
      "loss": 0.2366,
      "step": 2934
    },
    {
      "epoch": 5.87,
      "grad_norm": 1.2202730178833008,
      "learning_rate": 8.263305322128852e-05,
      "loss": 0.1876,
      "step": 2935
    },
    {
      "epoch": 5.872,
      "grad_norm": 1.4620860815048218,
      "learning_rate": 8.259303721488596e-05,
      "loss": 0.191,
      "step": 2936
    },
    {
      "epoch": 5.874,
      "grad_norm": 2.902360439300537,
      "learning_rate": 8.25530212084834e-05,
      "loss": 0.1866,
      "step": 2937
    },
    {
      "epoch": 5.876,
      "grad_norm": 0.9415687918663025,
      "learning_rate": 8.251300520208085e-05,
      "loss": 0.1546,
      "step": 2938
    },
    {
      "epoch": 5.878,
      "grad_norm": 0.9689143896102905,
      "learning_rate": 8.247298919567827e-05,
      "loss": 0.1757,
      "step": 2939
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.9163576364517212,
      "learning_rate": 8.243297318927572e-05,
      "loss": 0.1393,
      "step": 2940
    },
    {
      "epoch": 5.882,
      "grad_norm": 1.4104901552200317,
      "learning_rate": 8.239295718287316e-05,
      "loss": 0.2017,
      "step": 2941
    },
    {
      "epoch": 5.884,
      "grad_norm": 1.598914384841919,
      "learning_rate": 8.23529411764706e-05,
      "loss": 0.2011,
      "step": 2942
    },
    {
      "epoch": 5.886,
      "grad_norm": 1.4215319156646729,
      "learning_rate": 8.231292517006803e-05,
      "loss": 0.1704,
      "step": 2943
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.8954929709434509,
      "learning_rate": 8.227290916366547e-05,
      "loss": 0.1416,
      "step": 2944
    },
    {
      "epoch": 5.89,
      "grad_norm": 1.6637985706329346,
      "learning_rate": 8.223289315726292e-05,
      "loss": 0.214,
      "step": 2945
    },
    {
      "epoch": 5.892,
      "grad_norm": 1.4269944429397583,
      "learning_rate": 8.219287715086034e-05,
      "loss": 0.1925,
      "step": 2946
    },
    {
      "epoch": 5.894,
      "grad_norm": 1.2081047296524048,
      "learning_rate": 8.215286114445778e-05,
      "loss": 0.1455,
      "step": 2947
    },
    {
      "epoch": 5.896,
      "grad_norm": 1.737257719039917,
      "learning_rate": 8.211284513805523e-05,
      "loss": 0.2394,
      "step": 2948
    },
    {
      "epoch": 5.898,
      "grad_norm": 1.0242679119110107,
      "learning_rate": 8.207282913165267e-05,
      "loss": 0.1657,
      "step": 2949
    },
    {
      "epoch": 5.9,
      "grad_norm": 1.3025751113891602,
      "learning_rate": 8.20328131252501e-05,
      "loss": 0.1844,
      "step": 2950
    },
    {
      "epoch": 5.902,
      "grad_norm": 1.069628357887268,
      "learning_rate": 8.199279711884754e-05,
      "loss": 0.1803,
      "step": 2951
    },
    {
      "epoch": 5.904,
      "grad_norm": 1.036450982093811,
      "learning_rate": 8.195278111244499e-05,
      "loss": 0.1831,
      "step": 2952
    },
    {
      "epoch": 5.906,
      "grad_norm": 1.540692687034607,
      "learning_rate": 8.191276510604242e-05,
      "loss": 0.1908,
      "step": 2953
    },
    {
      "epoch": 5.908,
      "grad_norm": 1.0691578388214111,
      "learning_rate": 8.187274909963985e-05,
      "loss": 0.1581,
      "step": 2954
    },
    {
      "epoch": 5.91,
      "grad_norm": 1.363254189491272,
      "learning_rate": 8.18327330932373e-05,
      "loss": 0.188,
      "step": 2955
    },
    {
      "epoch": 5.912,
      "grad_norm": 1.5129581689834595,
      "learning_rate": 8.179271708683474e-05,
      "loss": 0.1873,
      "step": 2956
    },
    {
      "epoch": 5.914,
      "grad_norm": 0.9867419600486755,
      "learning_rate": 8.175270108043218e-05,
      "loss": 0.135,
      "step": 2957
    },
    {
      "epoch": 5.916,
      "grad_norm": 1.0641990900039673,
      "learning_rate": 8.171268507402962e-05,
      "loss": 0.1816,
      "step": 2958
    },
    {
      "epoch": 5.918,
      "grad_norm": 1.395266056060791,
      "learning_rate": 8.167266906762705e-05,
      "loss": 0.1837,
      "step": 2959
    },
    {
      "epoch": 5.92,
      "grad_norm": 1.0799280405044556,
      "learning_rate": 8.163265306122449e-05,
      "loss": 0.1686,
      "step": 2960
    },
    {
      "epoch": 5.922,
      "grad_norm": 1.0367381572723389,
      "learning_rate": 8.159263705482193e-05,
      "loss": 0.4672,
      "step": 2961
    },
    {
      "epoch": 5.924,
      "grad_norm": 0.6855748891830444,
      "learning_rate": 8.155262104841938e-05,
      "loss": 0.1523,
      "step": 2962
    },
    {
      "epoch": 5.926,
      "grad_norm": 1.1104480028152466,
      "learning_rate": 8.151260504201682e-05,
      "loss": 0.192,
      "step": 2963
    },
    {
      "epoch": 5.928,
      "grad_norm": 1.1290550231933594,
      "learning_rate": 8.147258903561424e-05,
      "loss": 0.1561,
      "step": 2964
    },
    {
      "epoch": 5.93,
      "grad_norm": 1.2795591354370117,
      "learning_rate": 8.143257302921169e-05,
      "loss": 0.1799,
      "step": 2965
    },
    {
      "epoch": 5.932,
      "grad_norm": 0.928871214389801,
      "learning_rate": 8.139255702280913e-05,
      "loss": 0.1517,
      "step": 2966
    },
    {
      "epoch": 5.934,
      "grad_norm": 0.7596627473831177,
      "learning_rate": 8.135254101640656e-05,
      "loss": 0.1964,
      "step": 2967
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.7203482985496521,
      "learning_rate": 8.1312525010004e-05,
      "loss": 0.1474,
      "step": 2968
    },
    {
      "epoch": 5.938,
      "grad_norm": 1.077916145324707,
      "learning_rate": 8.127250900360145e-05,
      "loss": 0.1315,
      "step": 2969
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 1.3856618404388428,
      "learning_rate": 8.123249299719889e-05,
      "loss": 0.1594,
      "step": 2970
    },
    {
      "epoch": 5.942,
      "grad_norm": 1.3608922958374023,
      "learning_rate": 8.119247699079631e-05,
      "loss": 0.1586,
      "step": 2971
    },
    {
      "epoch": 5.944,
      "grad_norm": 1.5279814004898071,
      "learning_rate": 8.115246098439376e-05,
      "loss": 0.1798,
      "step": 2972
    },
    {
      "epoch": 5.946,
      "grad_norm": 1.0823968648910522,
      "learning_rate": 8.11124449779912e-05,
      "loss": 0.1633,
      "step": 2973
    },
    {
      "epoch": 5.948,
      "grad_norm": 1.3321614265441895,
      "learning_rate": 8.107242897158864e-05,
      "loss": 0.1451,
      "step": 2974
    },
    {
      "epoch": 5.95,
      "grad_norm": 1.5230865478515625,
      "learning_rate": 8.103241296518607e-05,
      "loss": 0.1749,
      "step": 2975
    },
    {
      "epoch": 5.952,
      "grad_norm": 1.878421425819397,
      "learning_rate": 8.099239695878351e-05,
      "loss": 0.1779,
      "step": 2976
    },
    {
      "epoch": 5.954,
      "grad_norm": 2.754836082458496,
      "learning_rate": 8.095238095238096e-05,
      "loss": 0.217,
      "step": 2977
    },
    {
      "epoch": 5.9559999999999995,
      "grad_norm": 1.0857305526733398,
      "learning_rate": 8.091236494597839e-05,
      "loss": 0.1619,
      "step": 2978
    },
    {
      "epoch": 5.958,
      "grad_norm": 0.8634448647499084,
      "learning_rate": 8.087234893957584e-05,
      "loss": 0.13,
      "step": 2979
    },
    {
      "epoch": 5.96,
      "grad_norm": 1.2826446294784546,
      "learning_rate": 8.083233293317327e-05,
      "loss": 0.1737,
      "step": 2980
    },
    {
      "epoch": 5.962,
      "grad_norm": 1.6978745460510254,
      "learning_rate": 8.079231692677071e-05,
      "loss": 0.1525,
      "step": 2981
    },
    {
      "epoch": 5.964,
      "grad_norm": 1.7822468280792236,
      "learning_rate": 8.075230092036815e-05,
      "loss": 0.1542,
      "step": 2982
    },
    {
      "epoch": 5.966,
      "grad_norm": 1.1800520420074463,
      "learning_rate": 8.071228491396558e-05,
      "loss": 0.1276,
      "step": 2983
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.8881363272666931,
      "learning_rate": 8.067226890756304e-05,
      "loss": 0.1673,
      "step": 2984
    },
    {
      "epoch": 5.97,
      "grad_norm": 0.9076560139656067,
      "learning_rate": 8.063225290116046e-05,
      "loss": 0.1333,
      "step": 2985
    },
    {
      "epoch": 5.9719999999999995,
      "grad_norm": 1.2626240253448486,
      "learning_rate": 8.059223689475791e-05,
      "loss": 0.1935,
      "step": 2986
    },
    {
      "epoch": 5.974,
      "grad_norm": 1.048727035522461,
      "learning_rate": 8.055222088835535e-05,
      "loss": 0.1333,
      "step": 2987
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.9556761980056763,
      "learning_rate": 8.051220488195278e-05,
      "loss": 0.1502,
      "step": 2988
    },
    {
      "epoch": 5.978,
      "grad_norm": 0.9409509301185608,
      "learning_rate": 8.047218887555022e-05,
      "loss": 0.1917,
      "step": 2989
    },
    {
      "epoch": 5.98,
      "grad_norm": 1.6509170532226562,
      "learning_rate": 8.043217286914766e-05,
      "loss": 0.2257,
      "step": 2990
    },
    {
      "epoch": 5.982,
      "grad_norm": 1.1942613124847412,
      "learning_rate": 8.039215686274511e-05,
      "loss": 0.1816,
      "step": 2991
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.9072628021240234,
      "learning_rate": 8.035214085634253e-05,
      "loss": 0.1899,
      "step": 2992
    },
    {
      "epoch": 5.986,
      "grad_norm": 1.031197190284729,
      "learning_rate": 8.031212484993998e-05,
      "loss": 0.1478,
      "step": 2993
    },
    {
      "epoch": 5.9879999999999995,
      "grad_norm": 1.7858004570007324,
      "learning_rate": 8.027210884353742e-05,
      "loss": 0.1709,
      "step": 2994
    },
    {
      "epoch": 5.99,
      "grad_norm": 1.9389485120773315,
      "learning_rate": 8.023209283713486e-05,
      "loss": 0.1606,
      "step": 2995
    },
    {
      "epoch": 5.992,
      "grad_norm": 1.4509495496749878,
      "learning_rate": 8.01920768307323e-05,
      "loss": 0.1858,
      "step": 2996
    },
    {
      "epoch": 5.994,
      "grad_norm": 0.973017156124115,
      "learning_rate": 8.015206082432973e-05,
      "loss": 0.1457,
      "step": 2997
    },
    {
      "epoch": 5.996,
      "grad_norm": 1.1364715099334717,
      "learning_rate": 8.011204481792718e-05,
      "loss": 0.1655,
      "step": 2998
    },
    {
      "epoch": 5.998,
      "grad_norm": 1.1833596229553223,
      "learning_rate": 8.007202881152462e-05,
      "loss": 0.1586,
      "step": 2999
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.5573302507400513,
      "learning_rate": 8.003201280512204e-05,
      "loss": 0.2028,
      "step": 3000
    },
    {
      "epoch": 6.002,
      "grad_norm": 0.8204983472824097,
      "learning_rate": 7.99919967987195e-05,
      "loss": 0.1332,
      "step": 3001
    },
    {
      "epoch": 6.004,
      "grad_norm": 0.43739598989486694,
      "learning_rate": 7.995198079231693e-05,
      "loss": 0.101,
      "step": 3002
    },
    {
      "epoch": 6.006,
      "grad_norm": 1.074625015258789,
      "learning_rate": 7.991196478591437e-05,
      "loss": 0.142,
      "step": 3003
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.8535321950912476,
      "learning_rate": 7.98719487795118e-05,
      "loss": 0.1199,
      "step": 3004
    },
    {
      "epoch": 6.01,
      "grad_norm": 0.9397767782211304,
      "learning_rate": 7.983193277310926e-05,
      "loss": 0.1757,
      "step": 3005
    },
    {
      "epoch": 6.012,
      "grad_norm": 0.6525891423225403,
      "learning_rate": 7.979191676670669e-05,
      "loss": 0.1339,
      "step": 3006
    },
    {
      "epoch": 6.014,
      "grad_norm": 1.1525871753692627,
      "learning_rate": 7.975190076030412e-05,
      "loss": 0.1163,
      "step": 3007
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.8868073225021362,
      "learning_rate": 7.971188475390157e-05,
      "loss": 0.1353,
      "step": 3008
    },
    {
      "epoch": 6.018,
      "grad_norm": 0.8630704879760742,
      "learning_rate": 7.9671868747499e-05,
      "loss": 0.1445,
      "step": 3009
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.5009502172470093,
      "learning_rate": 7.963185274109644e-05,
      "loss": 0.1561,
      "step": 3010
    },
    {
      "epoch": 6.022,
      "grad_norm": 1.259163737297058,
      "learning_rate": 7.959183673469388e-05,
      "loss": 0.1625,
      "step": 3011
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.3880799114704132,
      "learning_rate": 7.955182072829132e-05,
      "loss": 0.0874,
      "step": 3012
    },
    {
      "epoch": 6.026,
      "grad_norm": 0.73320472240448,
      "learning_rate": 7.951180472188877e-05,
      "loss": 0.1261,
      "step": 3013
    },
    {
      "epoch": 6.028,
      "grad_norm": 0.7500177025794983,
      "learning_rate": 7.947178871548619e-05,
      "loss": 0.1568,
      "step": 3014
    },
    {
      "epoch": 6.03,
      "grad_norm": 1.0712765455245972,
      "learning_rate": 7.943177270908364e-05,
      "loss": 0.1162,
      "step": 3015
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.712950587272644,
      "learning_rate": 7.939175670268108e-05,
      "loss": 0.1042,
      "step": 3016
    },
    {
      "epoch": 6.034,
      "grad_norm": 0.5925893187522888,
      "learning_rate": 7.935174069627852e-05,
      "loss": 0.1453,
      "step": 3017
    },
    {
      "epoch": 6.036,
      "grad_norm": 0.8239900469779968,
      "learning_rate": 7.931172468987595e-05,
      "loss": 0.1355,
      "step": 3018
    },
    {
      "epoch": 6.038,
      "grad_norm": 0.8240922689437866,
      "learning_rate": 7.927170868347339e-05,
      "loss": 0.1001,
      "step": 3019
    },
    {
      "epoch": 6.04,
      "grad_norm": 1.3107761144638062,
      "learning_rate": 7.923169267707084e-05,
      "loss": 0.1138,
      "step": 3020
    },
    {
      "epoch": 6.042,
      "grad_norm": 0.8161555528640747,
      "learning_rate": 7.919167667066826e-05,
      "loss": 0.1475,
      "step": 3021
    },
    {
      "epoch": 6.044,
      "grad_norm": 1.560002088546753,
      "learning_rate": 7.915166066426571e-05,
      "loss": 0.1456,
      "step": 3022
    },
    {
      "epoch": 6.046,
      "grad_norm": 0.8312576413154602,
      "learning_rate": 7.911164465786315e-05,
      "loss": 0.1217,
      "step": 3023
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.627323567867279,
      "learning_rate": 7.907162865146059e-05,
      "loss": 0.1363,
      "step": 3024
    },
    {
      "epoch": 6.05,
      "grad_norm": 0.65445876121521,
      "learning_rate": 7.903161264505803e-05,
      "loss": 0.124,
      "step": 3025
    },
    {
      "epoch": 6.052,
      "grad_norm": 0.5327045321464539,
      "learning_rate": 7.899159663865546e-05,
      "loss": 0.1315,
      "step": 3026
    },
    {
      "epoch": 6.054,
      "grad_norm": 1.0441972017288208,
      "learning_rate": 7.895158063225291e-05,
      "loss": 0.0953,
      "step": 3027
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.9775735139846802,
      "learning_rate": 7.891156462585034e-05,
      "loss": 0.125,
      "step": 3028
    },
    {
      "epoch": 6.058,
      "grad_norm": 0.8386448621749878,
      "learning_rate": 7.887154861944779e-05,
      "loss": 0.1338,
      "step": 3029
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.4889671504497528,
      "learning_rate": 7.883153261304523e-05,
      "loss": 0.1326,
      "step": 3030
    },
    {
      "epoch": 6.062,
      "grad_norm": 0.6807737350463867,
      "learning_rate": 7.879151660664266e-05,
      "loss": 0.1082,
      "step": 3031
    },
    {
      "epoch": 6.064,
      "grad_norm": 1.3008463382720947,
      "learning_rate": 7.87515006002401e-05,
      "loss": 0.1665,
      "step": 3032
    },
    {
      "epoch": 6.066,
      "grad_norm": 0.6954196095466614,
      "learning_rate": 7.871148459383754e-05,
      "loss": 0.1156,
      "step": 3033
    },
    {
      "epoch": 6.068,
      "grad_norm": 0.7393184304237366,
      "learning_rate": 7.867146858743499e-05,
      "loss": 0.1506,
      "step": 3034
    },
    {
      "epoch": 6.07,
      "grad_norm": 0.7091798782348633,
      "learning_rate": 7.863145258103241e-05,
      "loss": 0.1135,
      "step": 3035
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.586449384689331,
      "learning_rate": 7.859143657462985e-05,
      "loss": 0.1151,
      "step": 3036
    },
    {
      "epoch": 6.074,
      "grad_norm": 1.0477769374847412,
      "learning_rate": 7.85514205682273e-05,
      "loss": 0.1403,
      "step": 3037
    },
    {
      "epoch": 6.076,
      "grad_norm": 0.984343409538269,
      "learning_rate": 7.851140456182474e-05,
      "loss": 0.1577,
      "step": 3038
    },
    {
      "epoch": 6.078,
      "grad_norm": 0.8579038381576538,
      "learning_rate": 7.847138855542217e-05,
      "loss": 0.1205,
      "step": 3039
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.7887811660766602,
      "learning_rate": 7.843137254901961e-05,
      "loss": 0.1199,
      "step": 3040
    },
    {
      "epoch": 6.082,
      "grad_norm": 0.8807797431945801,
      "learning_rate": 7.839135654261706e-05,
      "loss": 0.1218,
      "step": 3041
    },
    {
      "epoch": 6.084,
      "grad_norm": 0.7498553991317749,
      "learning_rate": 7.835134053621448e-05,
      "loss": 0.135,
      "step": 3042
    },
    {
      "epoch": 6.086,
      "grad_norm": 0.9916102886199951,
      "learning_rate": 7.831132452981192e-05,
      "loss": 0.1486,
      "step": 3043
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.7953810691833496,
      "learning_rate": 7.827130852340937e-05,
      "loss": 0.1446,
      "step": 3044
    },
    {
      "epoch": 6.09,
      "grad_norm": 0.680220365524292,
      "learning_rate": 7.823129251700681e-05,
      "loss": 0.1285,
      "step": 3045
    },
    {
      "epoch": 6.092,
      "grad_norm": 0.6844316720962524,
      "learning_rate": 7.819127651060425e-05,
      "loss": 0.1278,
      "step": 3046
    },
    {
      "epoch": 6.094,
      "grad_norm": 0.7929331064224243,
      "learning_rate": 7.815126050420168e-05,
      "loss": 0.1203,
      "step": 3047
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.872602641582489,
      "learning_rate": 7.811124449779912e-05,
      "loss": 0.1199,
      "step": 3048
    },
    {
      "epoch": 6.098,
      "grad_norm": 0.9689401984214783,
      "learning_rate": 7.807122849139656e-05,
      "loss": 0.1381,
      "step": 3049
    },
    {
      "epoch": 6.1,
      "grad_norm": 1.3750196695327759,
      "learning_rate": 7.8031212484994e-05,
      "loss": 0.153,
      "step": 3050
    },
    {
      "epoch": 6.102,
      "grad_norm": 0.8859835267066956,
      "learning_rate": 7.799119647859145e-05,
      "loss": 0.1387,
      "step": 3051
    },
    {
      "epoch": 6.104,
      "grad_norm": 1.3831087350845337,
      "learning_rate": 7.795118047218888e-05,
      "loss": 0.1479,
      "step": 3052
    },
    {
      "epoch": 6.106,
      "grad_norm": 0.5936214327812195,
      "learning_rate": 7.791116446578632e-05,
      "loss": 0.1413,
      "step": 3053
    },
    {
      "epoch": 6.108,
      "grad_norm": 1.0831290483474731,
      "learning_rate": 7.787114845938376e-05,
      "loss": 0.1248,
      "step": 3054
    },
    {
      "epoch": 6.11,
      "grad_norm": 2.3110134601593018,
      "learning_rate": 7.78311324529812e-05,
      "loss": 0.166,
      "step": 3055
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.5705965757369995,
      "learning_rate": 7.779111644657863e-05,
      "loss": 0.1249,
      "step": 3056
    },
    {
      "epoch": 6.114,
      "grad_norm": 0.4480627477169037,
      "learning_rate": 7.775110044017607e-05,
      "loss": 0.1117,
      "step": 3057
    },
    {
      "epoch": 6.116,
      "grad_norm": 1.2528163194656372,
      "learning_rate": 7.771108443377352e-05,
      "loss": 0.1854,
      "step": 3058
    },
    {
      "epoch": 6.118,
      "grad_norm": 1.0447044372558594,
      "learning_rate": 7.767106842737096e-05,
      "loss": 0.1498,
      "step": 3059
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.4975796937942505,
      "learning_rate": 7.763105242096838e-05,
      "loss": 0.1051,
      "step": 3060
    },
    {
      "epoch": 6.122,
      "grad_norm": 0.5676985383033752,
      "learning_rate": 7.759103641456583e-05,
      "loss": 0.1479,
      "step": 3061
    },
    {
      "epoch": 6.124,
      "grad_norm": 0.5963708758354187,
      "learning_rate": 7.755102040816327e-05,
      "loss": 0.1271,
      "step": 3062
    },
    {
      "epoch": 6.126,
      "grad_norm": 0.8636741042137146,
      "learning_rate": 7.751100440176072e-05,
      "loss": 0.1311,
      "step": 3063
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.7197796106338501,
      "learning_rate": 7.747098839535814e-05,
      "loss": 0.1238,
      "step": 3064
    },
    {
      "epoch": 6.13,
      "grad_norm": 1.0011062622070312,
      "learning_rate": 7.743097238895558e-05,
      "loss": 0.1042,
      "step": 3065
    },
    {
      "epoch": 6.132,
      "grad_norm": 1.159440517425537,
      "learning_rate": 7.739095638255303e-05,
      "loss": 0.1416,
      "step": 3066
    },
    {
      "epoch": 6.134,
      "grad_norm": 1.538211703300476,
      "learning_rate": 7.735094037615047e-05,
      "loss": 0.1505,
      "step": 3067
    },
    {
      "epoch": 6.136,
      "grad_norm": 1.6408870220184326,
      "learning_rate": 7.73109243697479e-05,
      "loss": 0.1801,
      "step": 3068
    },
    {
      "epoch": 6.138,
      "grad_norm": 1.6474415063858032,
      "learning_rate": 7.727090836334534e-05,
      "loss": 0.1527,
      "step": 3069
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.8462596535682678,
      "learning_rate": 7.723089235694279e-05,
      "loss": 0.1112,
      "step": 3070
    },
    {
      "epoch": 6.142,
      "grad_norm": 1.2039545774459839,
      "learning_rate": 7.719087635054022e-05,
      "loss": 0.1522,
      "step": 3071
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.9344115853309631,
      "learning_rate": 7.715086034413765e-05,
      "loss": 0.1602,
      "step": 3072
    },
    {
      "epoch": 6.146,
      "grad_norm": 1.0638233423233032,
      "learning_rate": 7.71108443377351e-05,
      "loss": 0.1284,
      "step": 3073
    },
    {
      "epoch": 6.148,
      "grad_norm": 0.8136996626853943,
      "learning_rate": 7.707082833133254e-05,
      "loss": 0.1134,
      "step": 3074
    },
    {
      "epoch": 6.15,
      "grad_norm": 1.158663272857666,
      "learning_rate": 7.703081232492998e-05,
      "loss": 0.1277,
      "step": 3075
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.5785687565803528,
      "learning_rate": 7.699079631852741e-05,
      "loss": 0.1047,
      "step": 3076
    },
    {
      "epoch": 6.154,
      "grad_norm": 1.2041133642196655,
      "learning_rate": 7.695078031212485e-05,
      "loss": 0.137,
      "step": 3077
    },
    {
      "epoch": 6.156,
      "grad_norm": 0.5131932497024536,
      "learning_rate": 7.691076430572229e-05,
      "loss": 0.1089,
      "step": 3078
    },
    {
      "epoch": 6.158,
      "grad_norm": 0.7189245223999023,
      "learning_rate": 7.687074829931973e-05,
      "loss": 0.1234,
      "step": 3079
    },
    {
      "epoch": 6.16,
      "grad_norm": 1.3872801065444946,
      "learning_rate": 7.683073229291718e-05,
      "loss": 0.1374,
      "step": 3080
    },
    {
      "epoch": 6.162,
      "grad_norm": 0.6543912887573242,
      "learning_rate": 7.679071628651461e-05,
      "loss": 0.1139,
      "step": 3081
    },
    {
      "epoch": 6.164,
      "grad_norm": 0.6107168793678284,
      "learning_rate": 7.675070028011205e-05,
      "loss": 0.1081,
      "step": 3082
    },
    {
      "epoch": 6.166,
      "grad_norm": 1.010632038116455,
      "learning_rate": 7.671068427370949e-05,
      "loss": 0.1402,
      "step": 3083
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.5229726433753967,
      "learning_rate": 7.667066826730693e-05,
      "loss": 0.1115,
      "step": 3084
    },
    {
      "epoch": 6.17,
      "grad_norm": 0.79521644115448,
      "learning_rate": 7.663065226090436e-05,
      "loss": 0.1432,
      "step": 3085
    },
    {
      "epoch": 6.172,
      "grad_norm": 0.5678814053535461,
      "learning_rate": 7.65906362545018e-05,
      "loss": 0.0953,
      "step": 3086
    },
    {
      "epoch": 6.174,
      "grad_norm": 0.5802972912788391,
      "learning_rate": 7.655062024809925e-05,
      "loss": 0.1397,
      "step": 3087
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.7439606189727783,
      "learning_rate": 7.651060424169669e-05,
      "loss": 0.1276,
      "step": 3088
    },
    {
      "epoch": 6.178,
      "grad_norm": 0.6126213669776917,
      "learning_rate": 7.647058823529411e-05,
      "loss": 0.1187,
      "step": 3089
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.591168999671936,
      "learning_rate": 7.643057222889156e-05,
      "loss": 0.0897,
      "step": 3090
    },
    {
      "epoch": 6.182,
      "grad_norm": 0.527462899684906,
      "learning_rate": 7.6390556222489e-05,
      "loss": 0.105,
      "step": 3091
    },
    {
      "epoch": 6.184,
      "grad_norm": 1.1413928270339966,
      "learning_rate": 7.635054021608644e-05,
      "loss": 0.1376,
      "step": 3092
    },
    {
      "epoch": 6.186,
      "grad_norm": 0.6212630271911621,
      "learning_rate": 7.631052420968387e-05,
      "loss": 0.1172,
      "step": 3093
    },
    {
      "epoch": 6.188,
      "grad_norm": 1.2793320417404175,
      "learning_rate": 7.627050820328132e-05,
      "loss": 0.1391,
      "step": 3094
    },
    {
      "epoch": 6.19,
      "grad_norm": 1.0842688083648682,
      "learning_rate": 7.623049219687876e-05,
      "loss": 0.1234,
      "step": 3095
    },
    {
      "epoch": 6.192,
      "grad_norm": 1.143674612045288,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.1662,
      "step": 3096
    },
    {
      "epoch": 6.194,
      "grad_norm": 0.5915873050689697,
      "learning_rate": 7.615046018407364e-05,
      "loss": 0.1001,
      "step": 3097
    },
    {
      "epoch": 6.196,
      "grad_norm": 0.6787918210029602,
      "learning_rate": 7.611044417767107e-05,
      "loss": 0.1031,
      "step": 3098
    },
    {
      "epoch": 6.198,
      "grad_norm": 0.6372261643409729,
      "learning_rate": 7.607042817126851e-05,
      "loss": 0.125,
      "step": 3099
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.6368250846862793,
      "learning_rate": 7.603041216486595e-05,
      "loss": 0.1437,
      "step": 3100
    },
    {
      "epoch": 6.202,
      "grad_norm": 1.4614107608795166,
      "learning_rate": 7.599039615846338e-05,
      "loss": 0.1356,
      "step": 3101
    },
    {
      "epoch": 6.204,
      "grad_norm": 0.7487146258354187,
      "learning_rate": 7.595038015206083e-05,
      "loss": 0.1205,
      "step": 3102
    },
    {
      "epoch": 6.206,
      "grad_norm": 2.231590509414673,
      "learning_rate": 7.591036414565826e-05,
      "loss": 0.128,
      "step": 3103
    },
    {
      "epoch": 6.208,
      "grad_norm": 1.576084017753601,
      "learning_rate": 7.587034813925571e-05,
      "loss": 0.1364,
      "step": 3104
    },
    {
      "epoch": 6.21,
      "grad_norm": 0.8170769214630127,
      "learning_rate": 7.583033213285315e-05,
      "loss": 0.1021,
      "step": 3105
    },
    {
      "epoch": 6.212,
      "grad_norm": 0.6652771830558777,
      "learning_rate": 7.579031612645058e-05,
      "loss": 0.127,
      "step": 3106
    },
    {
      "epoch": 6.214,
      "grad_norm": 1.2066208124160767,
      "learning_rate": 7.575030012004802e-05,
      "loss": 0.1627,
      "step": 3107
    },
    {
      "epoch": 6.216,
      "grad_norm": 1.2838537693023682,
      "learning_rate": 7.571028411364546e-05,
      "loss": 0.1443,
      "step": 3108
    },
    {
      "epoch": 6.218,
      "grad_norm": 0.6862964034080505,
      "learning_rate": 7.567026810724291e-05,
      "loss": 0.1022,
      "step": 3109
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.5932841897010803,
      "learning_rate": 7.563025210084033e-05,
      "loss": 0.1198,
      "step": 3110
    },
    {
      "epoch": 6.222,
      "grad_norm": 0.6249816417694092,
      "learning_rate": 7.559023609443778e-05,
      "loss": 0.1504,
      "step": 3111
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.9503481388092041,
      "learning_rate": 7.555022008803522e-05,
      "loss": 0.1279,
      "step": 3112
    },
    {
      "epoch": 6.226,
      "grad_norm": 0.7826101183891296,
      "learning_rate": 7.551020408163266e-05,
      "loss": 0.1302,
      "step": 3113
    },
    {
      "epoch": 6.228,
      "grad_norm": 1.0233041048049927,
      "learning_rate": 7.54701880752301e-05,
      "loss": 0.1628,
      "step": 3114
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.812263548374176,
      "learning_rate": 7.543017206882753e-05,
      "loss": 0.1262,
      "step": 3115
    },
    {
      "epoch": 6.232,
      "grad_norm": 1.0608980655670166,
      "learning_rate": 7.539015606242498e-05,
      "loss": 0.1614,
      "step": 3116
    },
    {
      "epoch": 6.234,
      "grad_norm": 0.7060598731040955,
      "learning_rate": 7.53501400560224e-05,
      "loss": 0.1351,
      "step": 3117
    },
    {
      "epoch": 6.236,
      "grad_norm": 0.9349401593208313,
      "learning_rate": 7.531012404961986e-05,
      "loss": 0.144,
      "step": 3118
    },
    {
      "epoch": 6.2379999999999995,
      "grad_norm": 1.0448851585388184,
      "learning_rate": 7.527010804321729e-05,
      "loss": 0.1155,
      "step": 3119
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.7147437930107117,
      "learning_rate": 7.523009203681473e-05,
      "loss": 0.1268,
      "step": 3120
    },
    {
      "epoch": 6.242,
      "grad_norm": 0.9153996706008911,
      "learning_rate": 7.519007603041217e-05,
      "loss": 0.1105,
      "step": 3121
    },
    {
      "epoch": 6.244,
      "grad_norm": 0.7232232689857483,
      "learning_rate": 7.51500600240096e-05,
      "loss": 0.105,
      "step": 3122
    },
    {
      "epoch": 6.246,
      "grad_norm": 0.5535188317298889,
      "learning_rate": 7.511004401760705e-05,
      "loss": 0.1231,
      "step": 3123
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.5785990357398987,
      "learning_rate": 7.507002801120448e-05,
      "loss": 0.1082,
      "step": 3124
    },
    {
      "epoch": 6.25,
      "grad_norm": 0.8510013222694397,
      "learning_rate": 7.503001200480192e-05,
      "loss": 0.1371,
      "step": 3125
    },
    {
      "epoch": 6.252,
      "grad_norm": 0.602052628993988,
      "learning_rate": 7.498999599839937e-05,
      "loss": 0.0988,
      "step": 3126
    },
    {
      "epoch": 6.254,
      "grad_norm": 1.4924157857894897,
      "learning_rate": 7.49499799919968e-05,
      "loss": 0.2143,
      "step": 3127
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.5658670663833618,
      "learning_rate": 7.490996398559424e-05,
      "loss": 0.105,
      "step": 3128
    },
    {
      "epoch": 6.258,
      "grad_norm": 1.2120561599731445,
      "learning_rate": 7.486994797919168e-05,
      "loss": 0.1233,
      "step": 3129
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.6777826547622681,
      "learning_rate": 7.482993197278913e-05,
      "loss": 0.1545,
      "step": 3130
    },
    {
      "epoch": 6.2620000000000005,
      "grad_norm": 0.8346223831176758,
      "learning_rate": 7.478991596638657e-05,
      "loss": 0.1456,
      "step": 3131
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.6672936081886292,
      "learning_rate": 7.474989995998399e-05,
      "loss": 0.1192,
      "step": 3132
    },
    {
      "epoch": 6.266,
      "grad_norm": 0.735512375831604,
      "learning_rate": 7.470988395358144e-05,
      "loss": 0.0998,
      "step": 3133
    },
    {
      "epoch": 6.268,
      "grad_norm": 1.036478042602539,
      "learning_rate": 7.466986794717888e-05,
      "loss": 0.1186,
      "step": 3134
    },
    {
      "epoch": 6.27,
      "grad_norm": 0.6823474168777466,
      "learning_rate": 7.462985194077631e-05,
      "loss": 0.1372,
      "step": 3135
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.8340380191802979,
      "learning_rate": 7.458983593437375e-05,
      "loss": 0.1333,
      "step": 3136
    },
    {
      "epoch": 6.274,
      "grad_norm": 1.0899161100387573,
      "learning_rate": 7.454981992797119e-05,
      "loss": 0.1488,
      "step": 3137
    },
    {
      "epoch": 6.276,
      "grad_norm": 0.788507342338562,
      "learning_rate": 7.450980392156864e-05,
      "loss": 0.1173,
      "step": 3138
    },
    {
      "epoch": 6.2780000000000005,
      "grad_norm": 0.7945618629455566,
      "learning_rate": 7.446978791516606e-05,
      "loss": 0.1176,
      "step": 3139
    },
    {
      "epoch": 6.28,
      "grad_norm": 1.0418506860733032,
      "learning_rate": 7.442977190876351e-05,
      "loss": 0.1716,
      "step": 3140
    },
    {
      "epoch": 6.282,
      "grad_norm": 0.7609134912490845,
      "learning_rate": 7.438975590236095e-05,
      "loss": 0.1359,
      "step": 3141
    },
    {
      "epoch": 6.284,
      "grad_norm": 0.6814773678779602,
      "learning_rate": 7.434973989595839e-05,
      "loss": 0.1527,
      "step": 3142
    },
    {
      "epoch": 6.286,
      "grad_norm": 0.6552720069885254,
      "learning_rate": 7.430972388955582e-05,
      "loss": 0.1273,
      "step": 3143
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.7943016886711121,
      "learning_rate": 7.426970788315326e-05,
      "loss": 0.1282,
      "step": 3144
    },
    {
      "epoch": 6.29,
      "grad_norm": 0.773908257484436,
      "learning_rate": 7.422969187675071e-05,
      "loss": 0.1325,
      "step": 3145
    },
    {
      "epoch": 6.292,
      "grad_norm": 1.9739317893981934,
      "learning_rate": 7.418967587034814e-05,
      "loss": 0.171,
      "step": 3146
    },
    {
      "epoch": 6.294,
      "grad_norm": 1.186557650566101,
      "learning_rate": 7.414965986394559e-05,
      "loss": 0.1144,
      "step": 3147
    },
    {
      "epoch": 6.296,
      "grad_norm": 1.4657111167907715,
      "learning_rate": 7.410964385754302e-05,
      "loss": 0.1297,
      "step": 3148
    },
    {
      "epoch": 6.298,
      "grad_norm": 0.5945335030555725,
      "learning_rate": 7.406962785114046e-05,
      "loss": 0.0949,
      "step": 3149
    },
    {
      "epoch": 6.3,
      "grad_norm": 1.226356029510498,
      "learning_rate": 7.40296118447379e-05,
      "loss": 0.1389,
      "step": 3150
    },
    {
      "epoch": 6.302,
      "grad_norm": 0.8069003224372864,
      "learning_rate": 7.398959583833534e-05,
      "loss": 0.1252,
      "step": 3151
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.6806781888008118,
      "learning_rate": 7.394957983193279e-05,
      "loss": 0.1187,
      "step": 3152
    },
    {
      "epoch": 6.306,
      "grad_norm": 0.6836003661155701,
      "learning_rate": 7.390956382553021e-05,
      "loss": 0.1306,
      "step": 3153
    },
    {
      "epoch": 6.308,
      "grad_norm": 0.9039300680160522,
      "learning_rate": 7.386954781912765e-05,
      "loss": 0.1343,
      "step": 3154
    },
    {
      "epoch": 6.31,
      "grad_norm": 0.8454613089561462,
      "learning_rate": 7.38295318127251e-05,
      "loss": 0.1216,
      "step": 3155
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.46296048164367676,
      "learning_rate": 7.378951580632253e-05,
      "loss": 0.1352,
      "step": 3156
    },
    {
      "epoch": 6.314,
      "grad_norm": 0.5259920358657837,
      "learning_rate": 7.374949979991997e-05,
      "loss": 0.1374,
      "step": 3157
    },
    {
      "epoch": 6.316,
      "grad_norm": 0.9044138789176941,
      "learning_rate": 7.370948379351741e-05,
      "loss": 0.134,
      "step": 3158
    },
    {
      "epoch": 6.318,
      "grad_norm": 1.2317233085632324,
      "learning_rate": 7.366946778711486e-05,
      "loss": 0.1493,
      "step": 3159
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.994924783706665,
      "learning_rate": 7.362945178071228e-05,
      "loss": 0.1325,
      "step": 3160
    },
    {
      "epoch": 6.322,
      "grad_norm": 0.6969470977783203,
      "learning_rate": 7.358943577430972e-05,
      "loss": 0.1266,
      "step": 3161
    },
    {
      "epoch": 6.324,
      "grad_norm": 0.8012080192565918,
      "learning_rate": 7.354941976790717e-05,
      "loss": 0.0997,
      "step": 3162
    },
    {
      "epoch": 6.326,
      "grad_norm": 1.1502307653427124,
      "learning_rate": 7.350940376150461e-05,
      "loss": 0.1458,
      "step": 3163
    },
    {
      "epoch": 6.328,
      "grad_norm": 1.1900907754898071,
      "learning_rate": 7.346938775510205e-05,
      "loss": 0.1186,
      "step": 3164
    },
    {
      "epoch": 6.33,
      "grad_norm": 0.9056556224822998,
      "learning_rate": 7.342937174869948e-05,
      "loss": 0.1313,
      "step": 3165
    },
    {
      "epoch": 6.332,
      "grad_norm": 0.7929139137268066,
      "learning_rate": 7.338935574229692e-05,
      "loss": 0.1318,
      "step": 3166
    },
    {
      "epoch": 6.334,
      "grad_norm": 0.7886803150177002,
      "learning_rate": 7.334933973589436e-05,
      "loss": 0.1172,
      "step": 3167
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.9164085388183594,
      "learning_rate": 7.33093237294918e-05,
      "loss": 0.1464,
      "step": 3168
    },
    {
      "epoch": 6.338,
      "grad_norm": 1.2505582571029663,
      "learning_rate": 7.326930772308924e-05,
      "loss": 0.1307,
      "step": 3169
    },
    {
      "epoch": 6.34,
      "grad_norm": 1.4374099969863892,
      "learning_rate": 7.322929171668668e-05,
      "loss": 0.1629,
      "step": 3170
    },
    {
      "epoch": 6.342,
      "grad_norm": 0.9894323348999023,
      "learning_rate": 7.318927571028412e-05,
      "loss": 0.1315,
      "step": 3171
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.5185578465461731,
      "learning_rate": 7.314925970388156e-05,
      "loss": 0.155,
      "step": 3172
    },
    {
      "epoch": 6.346,
      "grad_norm": 0.6708161234855652,
      "learning_rate": 7.310924369747899e-05,
      "loss": 0.1258,
      "step": 3173
    },
    {
      "epoch": 6.348,
      "grad_norm": 0.4163958728313446,
      "learning_rate": 7.306922769107643e-05,
      "loss": 0.0894,
      "step": 3174
    },
    {
      "epoch": 6.35,
      "grad_norm": 0.7348213195800781,
      "learning_rate": 7.302921168467387e-05,
      "loss": 0.1311,
      "step": 3175
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.6262266635894775,
      "learning_rate": 7.298919567827132e-05,
      "loss": 0.099,
      "step": 3176
    },
    {
      "epoch": 6.354,
      "grad_norm": 0.8963378667831421,
      "learning_rate": 7.294917967186876e-05,
      "loss": 0.1464,
      "step": 3177
    },
    {
      "epoch": 6.356,
      "grad_norm": 1.0171375274658203,
      "learning_rate": 7.290916366546618e-05,
      "loss": 0.1266,
      "step": 3178
    },
    {
      "epoch": 6.358,
      "grad_norm": 0.6424863338470459,
      "learning_rate": 7.286914765906363e-05,
      "loss": 0.1196,
      "step": 3179
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.7130900025367737,
      "learning_rate": 7.282913165266107e-05,
      "loss": 0.1439,
      "step": 3180
    },
    {
      "epoch": 6.362,
      "grad_norm": 0.881267786026001,
      "learning_rate": 7.27891156462585e-05,
      "loss": 0.1239,
      "step": 3181
    },
    {
      "epoch": 6.364,
      "grad_norm": 0.5396923422813416,
      "learning_rate": 7.274909963985594e-05,
      "loss": 0.1173,
      "step": 3182
    },
    {
      "epoch": 6.366,
      "grad_norm": 0.8761635422706604,
      "learning_rate": 7.270908363345339e-05,
      "loss": 0.1312,
      "step": 3183
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.6377121210098267,
      "learning_rate": 7.266906762705083e-05,
      "loss": 0.1232,
      "step": 3184
    },
    {
      "epoch": 6.37,
      "grad_norm": 1.454074501991272,
      "learning_rate": 7.262905162064825e-05,
      "loss": 0.143,
      "step": 3185
    },
    {
      "epoch": 6.372,
      "grad_norm": 1.387455701828003,
      "learning_rate": 7.25890356142457e-05,
      "loss": 0.1206,
      "step": 3186
    },
    {
      "epoch": 6.374,
      "grad_norm": 0.7099047899246216,
      "learning_rate": 7.254901960784314e-05,
      "loss": 0.1369,
      "step": 3187
    },
    {
      "epoch": 6.376,
      "grad_norm": 1.4395326375961304,
      "learning_rate": 7.250900360144058e-05,
      "loss": 0.1334,
      "step": 3188
    },
    {
      "epoch": 6.378,
      "grad_norm": 1.257127046585083,
      "learning_rate": 7.246898759503801e-05,
      "loss": 0.1215,
      "step": 3189
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.5249350666999817,
      "learning_rate": 7.242897158863545e-05,
      "loss": 0.1496,
      "step": 3190
    },
    {
      "epoch": 6.382,
      "grad_norm": 1.0823789834976196,
      "learning_rate": 7.23889555822329e-05,
      "loss": 0.1602,
      "step": 3191
    },
    {
      "epoch": 6.384,
      "grad_norm": 1.652918815612793,
      "learning_rate": 7.234893957583033e-05,
      "loss": 0.1654,
      "step": 3192
    },
    {
      "epoch": 6.386,
      "grad_norm": 1.0704244375228882,
      "learning_rate": 7.230892356942778e-05,
      "loss": 0.1261,
      "step": 3193
    },
    {
      "epoch": 6.388,
      "grad_norm": 1.1259788274765015,
      "learning_rate": 7.226890756302521e-05,
      "loss": 0.1421,
      "step": 3194
    },
    {
      "epoch": 6.39,
      "grad_norm": 0.8255473375320435,
      "learning_rate": 7.222889155662266e-05,
      "loss": 0.1002,
      "step": 3195
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.6852921843528748,
      "learning_rate": 7.218887555022009e-05,
      "loss": 0.1132,
      "step": 3196
    },
    {
      "epoch": 6.394,
      "grad_norm": 0.8049837946891785,
      "learning_rate": 7.214885954381752e-05,
      "loss": 0.1257,
      "step": 3197
    },
    {
      "epoch": 6.396,
      "grad_norm": 0.969046413898468,
      "learning_rate": 7.210884353741498e-05,
      "loss": 0.1288,
      "step": 3198
    },
    {
      "epoch": 6.398,
      "grad_norm": 0.884573221206665,
      "learning_rate": 7.206882753101241e-05,
      "loss": 0.1586,
      "step": 3199
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.8187596201896667,
      "learning_rate": 7.202881152460985e-05,
      "loss": 0.1194,
      "step": 3200
    },
    {
      "epoch": 6.402,
      "grad_norm": 0.9895249009132385,
      "learning_rate": 7.198879551820729e-05,
      "loss": 0.1389,
      "step": 3201
    },
    {
      "epoch": 6.404,
      "grad_norm": 1.2348312139511108,
      "learning_rate": 7.194877951180472e-05,
      "loss": 0.1005,
      "step": 3202
    },
    {
      "epoch": 6.406,
      "grad_norm": 0.8451693654060364,
      "learning_rate": 7.190876350540216e-05,
      "loss": 0.1277,
      "step": 3203
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.686384379863739,
      "learning_rate": 7.18687474989996e-05,
      "loss": 0.13,
      "step": 3204
    },
    {
      "epoch": 6.41,
      "grad_norm": 0.8807417750358582,
      "learning_rate": 7.182873149259705e-05,
      "loss": 0.1486,
      "step": 3205
    },
    {
      "epoch": 6.412,
      "grad_norm": 0.9688190221786499,
      "learning_rate": 7.178871548619449e-05,
      "loss": 0.1468,
      "step": 3206
    },
    {
      "epoch": 6.414,
      "grad_norm": 0.7743750214576721,
      "learning_rate": 7.174869947979192e-05,
      "loss": 0.1075,
      "step": 3207
    },
    {
      "epoch": 6.416,
      "grad_norm": 1.2890788316726685,
      "learning_rate": 7.170868347338936e-05,
      "loss": 0.1378,
      "step": 3208
    },
    {
      "epoch": 6.418,
      "grad_norm": 0.7183858752250671,
      "learning_rate": 7.16686674669868e-05,
      "loss": 0.1164,
      "step": 3209
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.5388736128807068,
      "learning_rate": 7.162865146058423e-05,
      "loss": 0.115,
      "step": 3210
    },
    {
      "epoch": 6.422,
      "grad_norm": 0.8978815078735352,
      "learning_rate": 7.158863545418167e-05,
      "loss": 0.1199,
      "step": 3211
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.6756554245948792,
      "learning_rate": 7.154861944777912e-05,
      "loss": 0.1562,
      "step": 3212
    },
    {
      "epoch": 6.426,
      "grad_norm": 0.8372210264205933,
      "learning_rate": 7.150860344137656e-05,
      "loss": 0.1256,
      "step": 3213
    },
    {
      "epoch": 6.428,
      "grad_norm": 0.8105727434158325,
      "learning_rate": 7.146858743497398e-05,
      "loss": 0.1379,
      "step": 3214
    },
    {
      "epoch": 6.43,
      "grad_norm": 1.159338116645813,
      "learning_rate": 7.142857142857143e-05,
      "loss": 0.1289,
      "step": 3215
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.45591074228286743,
      "learning_rate": 7.138855542216887e-05,
      "loss": 0.1215,
      "step": 3216
    },
    {
      "epoch": 6.434,
      "grad_norm": 1.0915477275848389,
      "learning_rate": 7.134853941576631e-05,
      "loss": 0.1102,
      "step": 3217
    },
    {
      "epoch": 6.436,
      "grad_norm": 0.8741487860679626,
      "learning_rate": 7.130852340936375e-05,
      "loss": 0.1283,
      "step": 3218
    },
    {
      "epoch": 6.438,
      "grad_norm": 1.1293448209762573,
      "learning_rate": 7.12685074029612e-05,
      "loss": 0.1265,
      "step": 3219
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.8463954329490662,
      "learning_rate": 7.122849139655863e-05,
      "loss": 0.1478,
      "step": 3220
    },
    {
      "epoch": 6.442,
      "grad_norm": 1.2034196853637695,
      "learning_rate": 7.118847539015606e-05,
      "loss": 0.1341,
      "step": 3221
    },
    {
      "epoch": 6.444,
      "grad_norm": 1.4658297300338745,
      "learning_rate": 7.114845938375351e-05,
      "loss": 0.1262,
      "step": 3222
    },
    {
      "epoch": 6.446,
      "grad_norm": 0.8772963881492615,
      "learning_rate": 7.110844337735094e-05,
      "loss": 0.1305,
      "step": 3223
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.7687184810638428,
      "learning_rate": 7.106842737094838e-05,
      "loss": 0.0977,
      "step": 3224
    },
    {
      "epoch": 6.45,
      "grad_norm": 0.45940396189689636,
      "learning_rate": 7.102841136454582e-05,
      "loss": 0.167,
      "step": 3225
    },
    {
      "epoch": 6.452,
      "grad_norm": 0.7732481956481934,
      "learning_rate": 7.098839535814326e-05,
      "loss": 0.1073,
      "step": 3226
    },
    {
      "epoch": 6.454,
      "grad_norm": 0.7865553498268127,
      "learning_rate": 7.09483793517407e-05,
      "loss": 0.1081,
      "step": 3227
    },
    {
      "epoch": 6.456,
      "grad_norm": 1.441825270652771,
      "learning_rate": 7.090836334533813e-05,
      "loss": 0.1023,
      "step": 3228
    },
    {
      "epoch": 6.458,
      "grad_norm": 0.8014804720878601,
      "learning_rate": 7.086834733893558e-05,
      "loss": 0.1347,
      "step": 3229
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.4828227758407593,
      "learning_rate": 7.082833133253302e-05,
      "loss": 0.1253,
      "step": 3230
    },
    {
      "epoch": 6.462,
      "grad_norm": 0.7061969041824341,
      "learning_rate": 7.078831532613046e-05,
      "loss": 0.133,
      "step": 3231
    },
    {
      "epoch": 6.464,
      "grad_norm": 1.2418075799942017,
      "learning_rate": 7.074829931972789e-05,
      "loss": 0.146,
      "step": 3232
    },
    {
      "epoch": 6.466,
      "grad_norm": 0.6745686531066895,
      "learning_rate": 7.070828331332533e-05,
      "loss": 0.1596,
      "step": 3233
    },
    {
      "epoch": 6.468,
      "grad_norm": 0.6519967913627625,
      "learning_rate": 7.066826730692278e-05,
      "loss": 0.1064,
      "step": 3234
    },
    {
      "epoch": 6.47,
      "grad_norm": 0.7793651223182678,
      "learning_rate": 7.06282513005202e-05,
      "loss": 0.1747,
      "step": 3235
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.8301289081573486,
      "learning_rate": 7.058823529411765e-05,
      "loss": 0.1307,
      "step": 3236
    },
    {
      "epoch": 6.474,
      "grad_norm": 0.7689080834388733,
      "learning_rate": 7.054821928771509e-05,
      "loss": 0.1266,
      "step": 3237
    },
    {
      "epoch": 6.476,
      "grad_norm": 1.32997465133667,
      "learning_rate": 7.050820328131253e-05,
      "loss": 0.1366,
      "step": 3238
    },
    {
      "epoch": 6.478,
      "grad_norm": 0.76767897605896,
      "learning_rate": 7.046818727490997e-05,
      "loss": 0.1191,
      "step": 3239
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.8058955073356628,
      "learning_rate": 7.04281712685074e-05,
      "loss": 0.0971,
      "step": 3240
    },
    {
      "epoch": 6.482,
      "grad_norm": 1.240853190422058,
      "learning_rate": 7.038815526210485e-05,
      "loss": 0.1208,
      "step": 3241
    },
    {
      "epoch": 6.484,
      "grad_norm": 0.9727857708930969,
      "learning_rate": 7.034813925570228e-05,
      "loss": 0.1416,
      "step": 3242
    },
    {
      "epoch": 6.486,
      "grad_norm": 0.9322090148925781,
      "learning_rate": 7.030812324929971e-05,
      "loss": 0.1821,
      "step": 3243
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.6068418025970459,
      "learning_rate": 7.026810724289717e-05,
      "loss": 0.1643,
      "step": 3244
    },
    {
      "epoch": 6.49,
      "grad_norm": 0.7367005944252014,
      "learning_rate": 7.02280912364946e-05,
      "loss": 0.1356,
      "step": 3245
    },
    {
      "epoch": 6.492,
      "grad_norm": 0.8745823502540588,
      "learning_rate": 7.018807523009204e-05,
      "loss": 0.1449,
      "step": 3246
    },
    {
      "epoch": 6.494,
      "grad_norm": 0.3835018277168274,
      "learning_rate": 7.014805922368948e-05,
      "loss": 0.1141,
      "step": 3247
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.8105372190475464,
      "learning_rate": 7.010804321728693e-05,
      "loss": 0.1342,
      "step": 3248
    },
    {
      "epoch": 6.498,
      "grad_norm": 0.7487125992774963,
      "learning_rate": 7.006802721088435e-05,
      "loss": 0.1244,
      "step": 3249
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.9079821109771729,
      "learning_rate": 7.002801120448179e-05,
      "loss": 0.1475,
      "step": 3250
    },
    {
      "epoch": 6.502,
      "grad_norm": 0.6080668568611145,
      "learning_rate": 6.998799519807924e-05,
      "loss": 0.1508,
      "step": 3251
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.7584841251373291,
      "learning_rate": 6.994797919167668e-05,
      "loss": 0.1106,
      "step": 3252
    },
    {
      "epoch": 6.506,
      "grad_norm": 0.8678178191184998,
      "learning_rate": 6.990796318527411e-05,
      "loss": 0.1156,
      "step": 3253
    },
    {
      "epoch": 6.508,
      "grad_norm": 0.9400903582572937,
      "learning_rate": 6.986794717887155e-05,
      "loss": 0.1378,
      "step": 3254
    },
    {
      "epoch": 6.51,
      "grad_norm": 1.4128700494766235,
      "learning_rate": 6.982793117246899e-05,
      "loss": 0.1528,
      "step": 3255
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.6642916798591614,
      "learning_rate": 6.978791516606642e-05,
      "loss": 0.1319,
      "step": 3256
    },
    {
      "epoch": 6.514,
      "grad_norm": 0.7136831283569336,
      "learning_rate": 6.974789915966386e-05,
      "loss": 0.1558,
      "step": 3257
    },
    {
      "epoch": 6.516,
      "grad_norm": 1.219443440437317,
      "learning_rate": 6.970788315326131e-05,
      "loss": 0.161,
      "step": 3258
    },
    {
      "epoch": 6.518,
      "grad_norm": 0.63144451379776,
      "learning_rate": 6.966786714685875e-05,
      "loss": 0.1212,
      "step": 3259
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.7947389483451843,
      "learning_rate": 6.962785114045619e-05,
      "loss": 0.1392,
      "step": 3260
    },
    {
      "epoch": 6.522,
      "grad_norm": 0.7000576853752136,
      "learning_rate": 6.958783513405362e-05,
      "loss": 0.1561,
      "step": 3261
    },
    {
      "epoch": 6.524,
      "grad_norm": 0.636735737323761,
      "learning_rate": 6.954781912765106e-05,
      "loss": 0.1023,
      "step": 3262
    },
    {
      "epoch": 6.526,
      "grad_norm": 0.7611345052719116,
      "learning_rate": 6.950780312124851e-05,
      "loss": 0.1306,
      "step": 3263
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.9445732831954956,
      "learning_rate": 6.946778711484593e-05,
      "loss": 0.1431,
      "step": 3264
    },
    {
      "epoch": 6.53,
      "grad_norm": 0.6743659377098083,
      "learning_rate": 6.942777110844339e-05,
      "loss": 0.1229,
      "step": 3265
    },
    {
      "epoch": 6.532,
      "grad_norm": 0.5303447246551514,
      "learning_rate": 6.938775510204082e-05,
      "loss": 0.1106,
      "step": 3266
    },
    {
      "epoch": 6.534,
      "grad_norm": 0.7147079706192017,
      "learning_rate": 6.934773909563826e-05,
      "loss": 0.1391,
      "step": 3267
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.6953262686729431,
      "learning_rate": 6.93077230892357e-05,
      "loss": 0.1249,
      "step": 3268
    },
    {
      "epoch": 6.538,
      "grad_norm": 1.3181079626083374,
      "learning_rate": 6.926770708283313e-05,
      "loss": 0.1219,
      "step": 3269
    },
    {
      "epoch": 6.54,
      "grad_norm": 0.44852274656295776,
      "learning_rate": 6.922769107643058e-05,
      "loss": 0.1085,
      "step": 3270
    },
    {
      "epoch": 6.542,
      "grad_norm": 0.9123613834381104,
      "learning_rate": 6.918767507002801e-05,
      "loss": 0.1196,
      "step": 3271
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 1.4051971435546875,
      "learning_rate": 6.914765906362546e-05,
      "loss": 0.1354,
      "step": 3272
    },
    {
      "epoch": 6.546,
      "grad_norm": 0.7123590707778931,
      "learning_rate": 6.91076430572229e-05,
      "loss": 0.1285,
      "step": 3273
    },
    {
      "epoch": 6.548,
      "grad_norm": 0.988937258720398,
      "learning_rate": 6.906762705082033e-05,
      "loss": 0.1375,
      "step": 3274
    },
    {
      "epoch": 6.55,
      "grad_norm": 1.034571886062622,
      "learning_rate": 6.902761104441777e-05,
      "loss": 0.1474,
      "step": 3275
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.8352829813957214,
      "learning_rate": 6.898759503801521e-05,
      "loss": 0.1383,
      "step": 3276
    },
    {
      "epoch": 6.554,
      "grad_norm": 0.9822867512702942,
      "learning_rate": 6.894757903161266e-05,
      "loss": 0.1407,
      "step": 3277
    },
    {
      "epoch": 6.556,
      "grad_norm": 0.7618607878684998,
      "learning_rate": 6.890756302521008e-05,
      "loss": 0.1371,
      "step": 3278
    },
    {
      "epoch": 6.558,
      "grad_norm": 0.9252336621284485,
      "learning_rate": 6.886754701880752e-05,
      "loss": 0.1407,
      "step": 3279
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.5888535976409912,
      "learning_rate": 6.882753101240497e-05,
      "loss": 0.1019,
      "step": 3280
    },
    {
      "epoch": 6.562,
      "grad_norm": 0.5292155742645264,
      "learning_rate": 6.878751500600241e-05,
      "loss": 0.1596,
      "step": 3281
    },
    {
      "epoch": 6.564,
      "grad_norm": 0.7735881805419922,
      "learning_rate": 6.874749899959984e-05,
      "loss": 0.104,
      "step": 3282
    },
    {
      "epoch": 6.566,
      "grad_norm": 0.8078415393829346,
      "learning_rate": 6.870748299319728e-05,
      "loss": 0.1608,
      "step": 3283
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.5667288899421692,
      "learning_rate": 6.866746698679473e-05,
      "loss": 0.1305,
      "step": 3284
    },
    {
      "epoch": 6.57,
      "grad_norm": 1.0761927366256714,
      "learning_rate": 6.862745098039216e-05,
      "loss": 0.1404,
      "step": 3285
    },
    {
      "epoch": 6.572,
      "grad_norm": 0.631811797618866,
      "learning_rate": 6.858743497398959e-05,
      "loss": 0.096,
      "step": 3286
    },
    {
      "epoch": 6.574,
      "grad_norm": 0.88933926820755,
      "learning_rate": 6.854741896758704e-05,
      "loss": 0.1454,
      "step": 3287
    },
    {
      "epoch": 6.576,
      "grad_norm": 1.0532039403915405,
      "learning_rate": 6.850740296118448e-05,
      "loss": 0.144,
      "step": 3288
    },
    {
      "epoch": 6.578,
      "grad_norm": 0.6773180365562439,
      "learning_rate": 6.846738695478192e-05,
      "loss": 0.1021,
      "step": 3289
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.6640743017196655,
      "learning_rate": 6.842737094837935e-05,
      "loss": 0.1307,
      "step": 3290
    },
    {
      "epoch": 6.582,
      "grad_norm": 0.5779561996459961,
      "learning_rate": 6.838735494197679e-05,
      "loss": 0.1373,
      "step": 3291
    },
    {
      "epoch": 6.584,
      "grad_norm": 1.2808616161346436,
      "learning_rate": 6.834733893557423e-05,
      "loss": 0.1362,
      "step": 3292
    },
    {
      "epoch": 6.586,
      "grad_norm": 1.432707667350769,
      "learning_rate": 6.830732292917167e-05,
      "loss": 0.1103,
      "step": 3293
    },
    {
      "epoch": 6.588,
      "grad_norm": 1.491857886314392,
      "learning_rate": 6.826730692276912e-05,
      "loss": 0.196,
      "step": 3294
    },
    {
      "epoch": 6.59,
      "grad_norm": 1.1068940162658691,
      "learning_rate": 6.822729091636655e-05,
      "loss": 0.1173,
      "step": 3295
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.6991425156593323,
      "learning_rate": 6.818727490996399e-05,
      "loss": 0.1226,
      "step": 3296
    },
    {
      "epoch": 6.594,
      "grad_norm": 0.6095620393753052,
      "learning_rate": 6.814725890356143e-05,
      "loss": 0.1599,
      "step": 3297
    },
    {
      "epoch": 6.596,
      "grad_norm": 1.0851091146469116,
      "learning_rate": 6.810724289715887e-05,
      "loss": 0.1361,
      "step": 3298
    },
    {
      "epoch": 6.598,
      "grad_norm": 1.2413784265518188,
      "learning_rate": 6.80672268907563e-05,
      "loss": 0.1502,
      "step": 3299
    },
    {
      "epoch": 6.6,
      "grad_norm": 1.0356981754302979,
      "learning_rate": 6.802721088435374e-05,
      "loss": 0.1396,
      "step": 3300
    },
    {
      "epoch": 6.602,
      "grad_norm": 0.7018018960952759,
      "learning_rate": 6.798719487795119e-05,
      "loss": 0.1061,
      "step": 3301
    },
    {
      "epoch": 6.604,
      "grad_norm": 0.6382538080215454,
      "learning_rate": 6.794717887154863e-05,
      "loss": 0.1005,
      "step": 3302
    },
    {
      "epoch": 6.606,
      "grad_norm": 0.639030933380127,
      "learning_rate": 6.790716286514605e-05,
      "loss": 0.1011,
      "step": 3303
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.7553163170814514,
      "learning_rate": 6.78671468587435e-05,
      "loss": 0.135,
      "step": 3304
    },
    {
      "epoch": 6.61,
      "grad_norm": 0.5216665267944336,
      "learning_rate": 6.782713085234094e-05,
      "loss": 0.109,
      "step": 3305
    },
    {
      "epoch": 6.612,
      "grad_norm": 0.7998887300491333,
      "learning_rate": 6.778711484593838e-05,
      "loss": 0.105,
      "step": 3306
    },
    {
      "epoch": 6.614,
      "grad_norm": 1.3274263143539429,
      "learning_rate": 6.774709883953581e-05,
      "loss": 0.1316,
      "step": 3307
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.9101221561431885,
      "learning_rate": 6.770708283313326e-05,
      "loss": 0.1344,
      "step": 3308
    },
    {
      "epoch": 6.618,
      "grad_norm": 1.0357228517532349,
      "learning_rate": 6.76670668267307e-05,
      "loss": 0.1353,
      "step": 3309
    },
    {
      "epoch": 6.62,
      "grad_norm": 1.1185603141784668,
      "learning_rate": 6.762705082032812e-05,
      "loss": 0.1567,
      "step": 3310
    },
    {
      "epoch": 6.622,
      "grad_norm": 0.9941492080688477,
      "learning_rate": 6.758703481392558e-05,
      "loss": 0.1335,
      "step": 3311
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.9876352548599243,
      "learning_rate": 6.754701880752301e-05,
      "loss": 0.1485,
      "step": 3312
    },
    {
      "epoch": 6.626,
      "grad_norm": 0.7791686058044434,
      "learning_rate": 6.750700280112045e-05,
      "loss": 0.1444,
      "step": 3313
    },
    {
      "epoch": 6.628,
      "grad_norm": 0.5780264735221863,
      "learning_rate": 6.746698679471789e-05,
      "loss": 0.0949,
      "step": 3314
    },
    {
      "epoch": 6.63,
      "grad_norm": 0.7018561959266663,
      "learning_rate": 6.742697078831532e-05,
      "loss": 0.1282,
      "step": 3315
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.6291650533676147,
      "learning_rate": 6.738695478191277e-05,
      "loss": 0.1149,
      "step": 3316
    },
    {
      "epoch": 6.634,
      "grad_norm": 1.2298567295074463,
      "learning_rate": 6.73469387755102e-05,
      "loss": 0.1316,
      "step": 3317
    },
    {
      "epoch": 6.636,
      "grad_norm": 1.0451068878173828,
      "learning_rate": 6.730692276910765e-05,
      "loss": 0.2024,
      "step": 3318
    },
    {
      "epoch": 6.638,
      "grad_norm": 0.5616908669471741,
      "learning_rate": 6.726690676270509e-05,
      "loss": 0.1622,
      "step": 3319
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.576880156993866,
      "learning_rate": 6.722689075630254e-05,
      "loss": 0.1232,
      "step": 3320
    },
    {
      "epoch": 6.642,
      "grad_norm": 0.6231569051742554,
      "learning_rate": 6.718687474989996e-05,
      "loss": 0.1009,
      "step": 3321
    },
    {
      "epoch": 6.644,
      "grad_norm": 0.4804987609386444,
      "learning_rate": 6.71468587434974e-05,
      "loss": 0.107,
      "step": 3322
    },
    {
      "epoch": 6.646,
      "grad_norm": 0.9984261989593506,
      "learning_rate": 6.710684273709485e-05,
      "loss": 0.1372,
      "step": 3323
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.7093673348426819,
      "learning_rate": 6.706682673069227e-05,
      "loss": 0.1467,
      "step": 3324
    },
    {
      "epoch": 6.65,
      "grad_norm": 1.0271083116531372,
      "learning_rate": 6.702681072428972e-05,
      "loss": 0.1353,
      "step": 3325
    },
    {
      "epoch": 6.652,
      "grad_norm": 0.678356409072876,
      "learning_rate": 6.698679471788716e-05,
      "loss": 0.161,
      "step": 3326
    },
    {
      "epoch": 6.654,
      "grad_norm": 0.6686260104179382,
      "learning_rate": 6.69467787114846e-05,
      "loss": 0.1468,
      "step": 3327
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.733651340007782,
      "learning_rate": 6.690676270508203e-05,
      "loss": 0.167,
      "step": 3328
    },
    {
      "epoch": 6.658,
      "grad_norm": 0.7779372930526733,
      "learning_rate": 6.686674669867947e-05,
      "loss": 0.1242,
      "step": 3329
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.7667503356933594,
      "learning_rate": 6.682673069227692e-05,
      "loss": 0.1203,
      "step": 3330
    },
    {
      "epoch": 6.662,
      "grad_norm": 0.718268871307373,
      "learning_rate": 6.678671468587436e-05,
      "loss": 0.1251,
      "step": 3331
    },
    {
      "epoch": 6.664,
      "grad_norm": 1.012117862701416,
      "learning_rate": 6.67466986794718e-05,
      "loss": 0.1404,
      "step": 3332
    },
    {
      "epoch": 6.666,
      "grad_norm": 0.8754786252975464,
      "learning_rate": 6.670668267306923e-05,
      "loss": 0.1305,
      "step": 3333
    },
    {
      "epoch": 6.668,
      "grad_norm": 0.8274535536766052,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.1526,
      "step": 3334
    },
    {
      "epoch": 6.67,
      "grad_norm": 1.1320185661315918,
      "learning_rate": 6.662665066026411e-05,
      "loss": 0.1308,
      "step": 3335
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.6552767157554626,
      "learning_rate": 6.658663465386154e-05,
      "loss": 0.1257,
      "step": 3336
    },
    {
      "epoch": 6.674,
      "grad_norm": 0.8262262940406799,
      "learning_rate": 6.6546618647459e-05,
      "loss": 0.1159,
      "step": 3337
    },
    {
      "epoch": 6.676,
      "grad_norm": 0.8787583708763123,
      "learning_rate": 6.650660264105643e-05,
      "loss": 0.1549,
      "step": 3338
    },
    {
      "epoch": 6.678,
      "grad_norm": 0.8245974779129028,
      "learning_rate": 6.646658663465386e-05,
      "loss": 0.1305,
      "step": 3339
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.5976067185401917,
      "learning_rate": 6.64265706282513e-05,
      "loss": 0.1268,
      "step": 3340
    },
    {
      "epoch": 6.682,
      "grad_norm": 0.7714530825614929,
      "learning_rate": 6.638655462184874e-05,
      "loss": 0.1751,
      "step": 3341
    },
    {
      "epoch": 6.684,
      "grad_norm": 0.6946458220481873,
      "learning_rate": 6.634653861544618e-05,
      "loss": 0.1083,
      "step": 3342
    },
    {
      "epoch": 6.686,
      "grad_norm": 0.6000208258628845,
      "learning_rate": 6.630652260904362e-05,
      "loss": 0.1327,
      "step": 3343
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.8091560006141663,
      "learning_rate": 6.626650660264105e-05,
      "loss": 0.1197,
      "step": 3344
    },
    {
      "epoch": 6.6899999999999995,
      "grad_norm": 0.7651771306991577,
      "learning_rate": 6.62264905962385e-05,
      "loss": 0.1297,
      "step": 3345
    },
    {
      "epoch": 6.692,
      "grad_norm": 0.4615030586719513,
      "learning_rate": 6.618647458983593e-05,
      "loss": 0.1308,
      "step": 3346
    },
    {
      "epoch": 6.694,
      "grad_norm": 0.8190518617630005,
      "learning_rate": 6.614645858343338e-05,
      "loss": 0.1328,
      "step": 3347
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.665259063243866,
      "learning_rate": 6.610644257703082e-05,
      "loss": 0.1098,
      "step": 3348
    },
    {
      "epoch": 6.698,
      "grad_norm": 0.5765682458877563,
      "learning_rate": 6.606642657062825e-05,
      "loss": 0.1332,
      "step": 3349
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.8651880621910095,
      "learning_rate": 6.602641056422569e-05,
      "loss": 0.1287,
      "step": 3350
    },
    {
      "epoch": 6.702,
      "grad_norm": 1.4517489671707153,
      "learning_rate": 6.598639455782313e-05,
      "loss": 0.1257,
      "step": 3351
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.7918010950088501,
      "learning_rate": 6.594637855142058e-05,
      "loss": 0.1046,
      "step": 3352
    },
    {
      "epoch": 6.7059999999999995,
      "grad_norm": 0.6222139596939087,
      "learning_rate": 6.5906362545018e-05,
      "loss": 0.1229,
      "step": 3353
    },
    {
      "epoch": 6.708,
      "grad_norm": 0.8168577551841736,
      "learning_rate": 6.586634653861545e-05,
      "loss": 0.1369,
      "step": 3354
    },
    {
      "epoch": 6.71,
      "grad_norm": 0.5809880495071411,
      "learning_rate": 6.582633053221289e-05,
      "loss": 0.123,
      "step": 3355
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.49320849776268005,
      "learning_rate": 6.578631452581033e-05,
      "loss": 0.1095,
      "step": 3356
    },
    {
      "epoch": 6.714,
      "grad_norm": 0.6997506022453308,
      "learning_rate": 6.574629851940776e-05,
      "loss": 0.0962,
      "step": 3357
    },
    {
      "epoch": 6.716,
      "grad_norm": 1.6743433475494385,
      "learning_rate": 6.57062825130052e-05,
      "loss": 0.4028,
      "step": 3358
    },
    {
      "epoch": 6.718,
      "grad_norm": 0.7732222080230713,
      "learning_rate": 6.566626650660265e-05,
      "loss": 0.1342,
      "step": 3359
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.970393180847168,
      "learning_rate": 6.562625050020008e-05,
      "loss": 0.146,
      "step": 3360
    },
    {
      "epoch": 6.7219999999999995,
      "grad_norm": 0.8482528924942017,
      "learning_rate": 6.558623449379753e-05,
      "loss": 0.1035,
      "step": 3361
    },
    {
      "epoch": 6.724,
      "grad_norm": 0.5271252989768982,
      "learning_rate": 6.554621848739496e-05,
      "loss": 0.1563,
      "step": 3362
    },
    {
      "epoch": 6.726,
      "grad_norm": 1.1072332859039307,
      "learning_rate": 6.55062024809924e-05,
      "loss": 0.1492,
      "step": 3363
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.5497668385505676,
      "learning_rate": 6.546618647458984e-05,
      "loss": 0.1199,
      "step": 3364
    },
    {
      "epoch": 6.73,
      "grad_norm": 0.7803453803062439,
      "learning_rate": 6.542617046818728e-05,
      "loss": 0.146,
      "step": 3365
    },
    {
      "epoch": 6.732,
      "grad_norm": 0.7093465328216553,
      "learning_rate": 6.538615446178473e-05,
      "loss": 0.1411,
      "step": 3366
    },
    {
      "epoch": 6.734,
      "grad_norm": 0.5585440397262573,
      "learning_rate": 6.534613845538215e-05,
      "loss": 0.1059,
      "step": 3367
    },
    {
      "epoch": 6.736,
      "grad_norm": 1.1246601343154907,
      "learning_rate": 6.530612244897959e-05,
      "loss": 0.1336,
      "step": 3368
    },
    {
      "epoch": 6.7379999999999995,
      "grad_norm": 0.6661098003387451,
      "learning_rate": 6.526610644257704e-05,
      "loss": 0.1274,
      "step": 3369
    },
    {
      "epoch": 6.74,
      "grad_norm": 0.563984215259552,
      "learning_rate": 6.522609043617447e-05,
      "loss": 0.1336,
      "step": 3370
    },
    {
      "epoch": 6.742,
      "grad_norm": 0.93184494972229,
      "learning_rate": 6.518607442977191e-05,
      "loss": 0.116,
      "step": 3371
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.9182960987091064,
      "learning_rate": 6.514605842336935e-05,
      "loss": 0.1328,
      "step": 3372
    },
    {
      "epoch": 6.746,
      "grad_norm": 1.1641278266906738,
      "learning_rate": 6.51060424169668e-05,
      "loss": 0.1519,
      "step": 3373
    },
    {
      "epoch": 6.748,
      "grad_norm": 0.9399570822715759,
      "learning_rate": 6.506602641056422e-05,
      "loss": 0.1063,
      "step": 3374
    },
    {
      "epoch": 6.75,
      "grad_norm": 0.7698327898979187,
      "learning_rate": 6.502601040416166e-05,
      "loss": 0.1044,
      "step": 3375
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.7872354388237,
      "learning_rate": 6.498599439775911e-05,
      "loss": 0.1577,
      "step": 3376
    },
    {
      "epoch": 6.754,
      "grad_norm": 0.890251874923706,
      "learning_rate": 6.494597839135655e-05,
      "loss": 0.1265,
      "step": 3377
    },
    {
      "epoch": 6.756,
      "grad_norm": 0.8782482743263245,
      "learning_rate": 6.490596238495399e-05,
      "loss": 0.1269,
      "step": 3378
    },
    {
      "epoch": 6.758,
      "grad_norm": 0.7558450102806091,
      "learning_rate": 6.486594637855142e-05,
      "loss": 0.1515,
      "step": 3379
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.9353833198547363,
      "learning_rate": 6.482593037214886e-05,
      "loss": 0.1421,
      "step": 3380
    },
    {
      "epoch": 6.7620000000000005,
      "grad_norm": 0.7901548147201538,
      "learning_rate": 6.47859143657463e-05,
      "loss": 0.1361,
      "step": 3381
    },
    {
      "epoch": 6.764,
      "grad_norm": 0.6014137268066406,
      "learning_rate": 6.474589835934373e-05,
      "loss": 0.1208,
      "step": 3382
    },
    {
      "epoch": 6.766,
      "grad_norm": 0.6229453682899475,
      "learning_rate": 6.470588235294118e-05,
      "loss": 0.1314,
      "step": 3383
    },
    {
      "epoch": 6.768,
      "grad_norm": 1.1945900917053223,
      "learning_rate": 6.466586634653862e-05,
      "loss": 0.1277,
      "step": 3384
    },
    {
      "epoch": 6.77,
      "grad_norm": 0.5816320776939392,
      "learning_rate": 6.462585034013606e-05,
      "loss": 0.1459,
      "step": 3385
    },
    {
      "epoch": 6.772,
      "grad_norm": 1.4878236055374146,
      "learning_rate": 6.45858343337335e-05,
      "loss": 0.1522,
      "step": 3386
    },
    {
      "epoch": 6.774,
      "grad_norm": 1.0380358695983887,
      "learning_rate": 6.454581832733093e-05,
      "loss": 0.1228,
      "step": 3387
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.5844343304634094,
      "learning_rate": 6.450580232092838e-05,
      "loss": 0.1229,
      "step": 3388
    },
    {
      "epoch": 6.7780000000000005,
      "grad_norm": 0.5556461215019226,
      "learning_rate": 6.446578631452581e-05,
      "loss": 0.1651,
      "step": 3389
    },
    {
      "epoch": 6.78,
      "grad_norm": 0.8592057824134827,
      "learning_rate": 6.442577030812326e-05,
      "loss": 0.1268,
      "step": 3390
    },
    {
      "epoch": 6.782,
      "grad_norm": 0.9223089814186096,
      "learning_rate": 6.43857543017207e-05,
      "loss": 0.1242,
      "step": 3391
    },
    {
      "epoch": 6.784,
      "grad_norm": 1.546980619430542,
      "learning_rate": 6.434573829531813e-05,
      "loss": 0.1581,
      "step": 3392
    },
    {
      "epoch": 6.786,
      "grad_norm": 1.0002082586288452,
      "learning_rate": 6.430572228891557e-05,
      "loss": 0.1381,
      "step": 3393
    },
    {
      "epoch": 6.788,
      "grad_norm": 0.9037971496582031,
      "learning_rate": 6.4265706282513e-05,
      "loss": 0.2139,
      "step": 3394
    },
    {
      "epoch": 6.79,
      "grad_norm": 0.8028813600540161,
      "learning_rate": 6.422569027611046e-05,
      "loss": 0.1459,
      "step": 3395
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.8839102387428284,
      "learning_rate": 6.418567426970788e-05,
      "loss": 0.1464,
      "step": 3396
    },
    {
      "epoch": 6.7940000000000005,
      "grad_norm": 0.44183778762817383,
      "learning_rate": 6.414565826330533e-05,
      "loss": 0.1001,
      "step": 3397
    },
    {
      "epoch": 6.796,
      "grad_norm": 0.8336344957351685,
      "learning_rate": 6.410564225690277e-05,
      "loss": 0.1697,
      "step": 3398
    },
    {
      "epoch": 6.798,
      "grad_norm": 0.5487492680549622,
      "learning_rate": 6.40656262505002e-05,
      "loss": 0.1488,
      "step": 3399
    },
    {
      "epoch": 6.8,
      "grad_norm": 1.0258066654205322,
      "learning_rate": 6.402561024409764e-05,
      "loss": 0.1206,
      "step": 3400
    },
    {
      "epoch": 6.802,
      "grad_norm": 0.7278807163238525,
      "learning_rate": 6.398559423769508e-05,
      "loss": 0.1454,
      "step": 3401
    },
    {
      "epoch": 6.804,
      "grad_norm": 0.7571994066238403,
      "learning_rate": 6.394557823129253e-05,
      "loss": 0.1244,
      "step": 3402
    },
    {
      "epoch": 6.806,
      "grad_norm": 0.7973416447639465,
      "learning_rate": 6.390556222488995e-05,
      "loss": 0.1233,
      "step": 3403
    },
    {
      "epoch": 6.808,
      "grad_norm": 1.1644152402877808,
      "learning_rate": 6.386554621848739e-05,
      "loss": 0.1165,
      "step": 3404
    },
    {
      "epoch": 6.8100000000000005,
      "grad_norm": 0.6751247048377991,
      "learning_rate": 6.382553021208484e-05,
      "loss": 0.1195,
      "step": 3405
    },
    {
      "epoch": 6.812,
      "grad_norm": 0.9443332552909851,
      "learning_rate": 6.378551420568228e-05,
      "loss": 0.1542,
      "step": 3406
    },
    {
      "epoch": 6.814,
      "grad_norm": 0.7135043740272522,
      "learning_rate": 6.374549819927972e-05,
      "loss": 0.1287,
      "step": 3407
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.8106732964515686,
      "learning_rate": 6.370548219287715e-05,
      "loss": 0.1426,
      "step": 3408
    },
    {
      "epoch": 6.818,
      "grad_norm": 2.0862748622894287,
      "learning_rate": 6.36654661864746e-05,
      "loss": 0.1681,
      "step": 3409
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.7227470278739929,
      "learning_rate": 6.362545018007203e-05,
      "loss": 0.1172,
      "step": 3410
    },
    {
      "epoch": 6.822,
      "grad_norm": 2.179100513458252,
      "learning_rate": 6.358543417366946e-05,
      "loss": 0.1616,
      "step": 3411
    },
    {
      "epoch": 6.824,
      "grad_norm": 2.0009567737579346,
      "learning_rate": 6.354541816726692e-05,
      "loss": 0.1645,
      "step": 3412
    },
    {
      "epoch": 6.826,
      "grad_norm": 0.8542464971542358,
      "learning_rate": 6.350540216086435e-05,
      "loss": 0.1391,
      "step": 3413
    },
    {
      "epoch": 6.828,
      "grad_norm": 1.5881669521331787,
      "learning_rate": 6.346538615446179e-05,
      "loss": 0.17,
      "step": 3414
    },
    {
      "epoch": 6.83,
      "grad_norm": 0.9209977984428406,
      "learning_rate": 6.342537014805923e-05,
      "loss": 0.1475,
      "step": 3415
    },
    {
      "epoch": 6.832,
      "grad_norm": 1.1985645294189453,
      "learning_rate": 6.338535414165666e-05,
      "loss": 0.1369,
      "step": 3416
    },
    {
      "epoch": 6.834,
      "grad_norm": 1.4663118124008179,
      "learning_rate": 6.33453381352541e-05,
      "loss": 0.1399,
      "step": 3417
    },
    {
      "epoch": 6.836,
      "grad_norm": 0.9020479321479797,
      "learning_rate": 6.330532212885154e-05,
      "loss": 0.1793,
      "step": 3418
    },
    {
      "epoch": 6.838,
      "grad_norm": 1.116051435470581,
      "learning_rate": 6.326530612244899e-05,
      "loss": 0.1242,
      "step": 3419
    },
    {
      "epoch": 6.84,
      "grad_norm": 1.0494328737258911,
      "learning_rate": 6.322529011604643e-05,
      "loss": 0.1679,
      "step": 3420
    },
    {
      "epoch": 6.842,
      "grad_norm": 1.1001125574111938,
      "learning_rate": 6.318527410964386e-05,
      "loss": 0.1703,
      "step": 3421
    },
    {
      "epoch": 6.844,
      "grad_norm": 0.9889678955078125,
      "learning_rate": 6.31452581032413e-05,
      "loss": 0.1536,
      "step": 3422
    },
    {
      "epoch": 6.846,
      "grad_norm": 1.0175503492355347,
      "learning_rate": 6.310524209683874e-05,
      "loss": 0.1351,
      "step": 3423
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.5943191647529602,
      "learning_rate": 6.306522609043617e-05,
      "loss": 0.1297,
      "step": 3424
    },
    {
      "epoch": 6.85,
      "grad_norm": 0.5635902285575867,
      "learning_rate": 6.302521008403361e-05,
      "loss": 0.1418,
      "step": 3425
    },
    {
      "epoch": 6.852,
      "grad_norm": 1.4410513639450073,
      "learning_rate": 6.298519407763106e-05,
      "loss": 0.1481,
      "step": 3426
    },
    {
      "epoch": 6.854,
      "grad_norm": 1.5797882080078125,
      "learning_rate": 6.29451780712285e-05,
      "loss": 0.1926,
      "step": 3427
    },
    {
      "epoch": 6.856,
      "grad_norm": 1.101812720298767,
      "learning_rate": 6.290516206482592e-05,
      "loss": 0.1619,
      "step": 3428
    },
    {
      "epoch": 6.858,
      "grad_norm": 0.9487269520759583,
      "learning_rate": 6.286514605842337e-05,
      "loss": 0.1729,
      "step": 3429
    },
    {
      "epoch": 6.86,
      "grad_norm": 1.3821511268615723,
      "learning_rate": 6.282513005202081e-05,
      "loss": 0.1271,
      "step": 3430
    },
    {
      "epoch": 6.862,
      "grad_norm": 0.844601571559906,
      "learning_rate": 6.278511404561825e-05,
      "loss": 0.1197,
      "step": 3431
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.7272143363952637,
      "learning_rate": 6.274509803921569e-05,
      "loss": 0.1313,
      "step": 3432
    },
    {
      "epoch": 6.866,
      "grad_norm": 1.347532868385315,
      "learning_rate": 6.270508203281312e-05,
      "loss": 0.1622,
      "step": 3433
    },
    {
      "epoch": 6.868,
      "grad_norm": 0.6510604023933411,
      "learning_rate": 6.266506602641057e-05,
      "loss": 0.1591,
      "step": 3434
    },
    {
      "epoch": 6.87,
      "grad_norm": 0.5395932793617249,
      "learning_rate": 6.2625050020008e-05,
      "loss": 0.1225,
      "step": 3435
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.6841591596603394,
      "learning_rate": 6.258503401360545e-05,
      "loss": 0.1226,
      "step": 3436
    },
    {
      "epoch": 6.874,
      "grad_norm": 0.8024395704269409,
      "learning_rate": 6.254501800720288e-05,
      "loss": 0.1448,
      "step": 3437
    },
    {
      "epoch": 6.876,
      "grad_norm": 0.8662636876106262,
      "learning_rate": 6.250500200080032e-05,
      "loss": 0.1063,
      "step": 3438
    },
    {
      "epoch": 6.878,
      "grad_norm": 0.560923159122467,
      "learning_rate": 6.246498599439776e-05,
      "loss": 0.1287,
      "step": 3439
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.9603505730628967,
      "learning_rate": 6.24249699879952e-05,
      "loss": 0.1435,
      "step": 3440
    },
    {
      "epoch": 6.882,
      "grad_norm": 0.6852787137031555,
      "learning_rate": 6.238495398159265e-05,
      "loss": 0.1287,
      "step": 3441
    },
    {
      "epoch": 6.884,
      "grad_norm": 0.9804421067237854,
      "learning_rate": 6.234493797519007e-05,
      "loss": 0.1326,
      "step": 3442
    },
    {
      "epoch": 6.886,
      "grad_norm": 0.8679050207138062,
      "learning_rate": 6.230492196878752e-05,
      "loss": 0.1417,
      "step": 3443
    },
    {
      "epoch": 6.888,
      "grad_norm": 1.3023000955581665,
      "learning_rate": 6.226490596238496e-05,
      "loss": 0.1388,
      "step": 3444
    },
    {
      "epoch": 6.89,
      "grad_norm": 0.828033983707428,
      "learning_rate": 6.22248899559824e-05,
      "loss": 0.1754,
      "step": 3445
    },
    {
      "epoch": 6.892,
      "grad_norm": 1.3764634132385254,
      "learning_rate": 6.218487394957983e-05,
      "loss": 0.1542,
      "step": 3446
    },
    {
      "epoch": 6.894,
      "grad_norm": 1.1246083974838257,
      "learning_rate": 6.214485794317727e-05,
      "loss": 0.163,
      "step": 3447
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.5763655304908752,
      "learning_rate": 6.210484193677472e-05,
      "loss": 0.1099,
      "step": 3448
    },
    {
      "epoch": 6.898,
      "grad_norm": 0.6480821371078491,
      "learning_rate": 6.206482593037214e-05,
      "loss": 0.1038,
      "step": 3449
    },
    {
      "epoch": 6.9,
      "grad_norm": 0.7368549704551697,
      "learning_rate": 6.20248099239696e-05,
      "loss": 0.1457,
      "step": 3450
    },
    {
      "epoch": 6.902,
      "grad_norm": 0.5641866326332092,
      "learning_rate": 6.198479391756703e-05,
      "loss": 0.1484,
      "step": 3451
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.8179787397384644,
      "learning_rate": 6.194477791116447e-05,
      "loss": 0.1405,
      "step": 3452
    },
    {
      "epoch": 6.906,
      "grad_norm": 0.5747947692871094,
      "learning_rate": 6.19047619047619e-05,
      "loss": 0.1636,
      "step": 3453
    },
    {
      "epoch": 6.908,
      "grad_norm": 1.0314851999282837,
      "learning_rate": 6.186474589835934e-05,
      "loss": 0.1564,
      "step": 3454
    },
    {
      "epoch": 6.91,
      "grad_norm": 0.7536827921867371,
      "learning_rate": 6.18247298919568e-05,
      "loss": 0.1473,
      "step": 3455
    },
    {
      "epoch": 6.912,
      "grad_norm": 1.042026400566101,
      "learning_rate": 6.178471388555423e-05,
      "loss": 0.1491,
      "step": 3456
    },
    {
      "epoch": 6.914,
      "grad_norm": 1.36310613155365,
      "learning_rate": 6.174469787915165e-05,
      "loss": 0.1655,
      "step": 3457
    },
    {
      "epoch": 6.916,
      "grad_norm": 0.5136659145355225,
      "learning_rate": 6.17046818727491e-05,
      "loss": 0.0959,
      "step": 3458
    },
    {
      "epoch": 6.918,
      "grad_norm": 0.8874126076698303,
      "learning_rate": 6.166466586634654e-05,
      "loss": 0.1336,
      "step": 3459
    },
    {
      "epoch": 6.92,
      "grad_norm": 1.2226096391677856,
      "learning_rate": 6.162464985994398e-05,
      "loss": 0.1406,
      "step": 3460
    },
    {
      "epoch": 6.922,
      "grad_norm": 1.0088390111923218,
      "learning_rate": 6.158463385354142e-05,
      "loss": 0.1479,
      "step": 3461
    },
    {
      "epoch": 6.924,
      "grad_norm": 0.9911482930183411,
      "learning_rate": 6.154461784713887e-05,
      "loss": 0.1675,
      "step": 3462
    },
    {
      "epoch": 6.926,
      "grad_norm": 1.182854175567627,
      "learning_rate": 6.15046018407363e-05,
      "loss": 0.1449,
      "step": 3463
    },
    {
      "epoch": 6.928,
      "grad_norm": 1.063704013824463,
      "learning_rate": 6.146458583433373e-05,
      "loss": 0.1481,
      "step": 3464
    },
    {
      "epoch": 6.93,
      "grad_norm": 0.9149339199066162,
      "learning_rate": 6.142456982793118e-05,
      "loss": 0.1706,
      "step": 3465
    },
    {
      "epoch": 6.932,
      "grad_norm": 0.560732364654541,
      "learning_rate": 6.138455382152862e-05,
      "loss": 0.0924,
      "step": 3466
    },
    {
      "epoch": 6.934,
      "grad_norm": 0.7234567403793335,
      "learning_rate": 6.134453781512605e-05,
      "loss": 0.1424,
      "step": 3467
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.7797069549560547,
      "learning_rate": 6.130452180872349e-05,
      "loss": 0.1174,
      "step": 3468
    },
    {
      "epoch": 6.938,
      "grad_norm": 0.8215020298957825,
      "learning_rate": 6.126450580232093e-05,
      "loss": 0.1323,
      "step": 3469
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 0.8553625345230103,
      "learning_rate": 6.122448979591838e-05,
      "loss": 0.1529,
      "step": 3470
    },
    {
      "epoch": 6.942,
      "grad_norm": 0.48791447281837463,
      "learning_rate": 6.11844737895158e-05,
      "loss": 0.1105,
      "step": 3471
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.8522265553474426,
      "learning_rate": 6.114445778311325e-05,
      "loss": 0.1289,
      "step": 3472
    },
    {
      "epoch": 6.946,
      "grad_norm": 1.0820906162261963,
      "learning_rate": 6.110444177671069e-05,
      "loss": 0.1358,
      "step": 3473
    },
    {
      "epoch": 6.948,
      "grad_norm": 0.7498036026954651,
      "learning_rate": 6.106442577030813e-05,
      "loss": 0.1757,
      "step": 3474
    },
    {
      "epoch": 6.95,
      "grad_norm": 1.0578007698059082,
      "learning_rate": 6.1024409763905563e-05,
      "loss": 0.1217,
      "step": 3475
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.5389926433563232,
      "learning_rate": 6.0984393757503e-05,
      "loss": 0.1375,
      "step": 3476
    },
    {
      "epoch": 6.954,
      "grad_norm": 1.305571436882019,
      "learning_rate": 6.0944377751100444e-05,
      "loss": 0.1884,
      "step": 3477
    },
    {
      "epoch": 6.9559999999999995,
      "grad_norm": 0.9186521172523499,
      "learning_rate": 6.090436174469788e-05,
      "loss": 0.1382,
      "step": 3478
    },
    {
      "epoch": 6.958,
      "grad_norm": 0.6556112766265869,
      "learning_rate": 6.0864345738295326e-05,
      "loss": 0.1085,
      "step": 3479
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.9330482482910156,
      "learning_rate": 6.082432973189276e-05,
      "loss": 0.1376,
      "step": 3480
    },
    {
      "epoch": 6.962,
      "grad_norm": 0.9837954044342041,
      "learning_rate": 6.078431372549019e-05,
      "loss": 0.1536,
      "step": 3481
    },
    {
      "epoch": 6.964,
      "grad_norm": 0.7553731203079224,
      "learning_rate": 6.074429771908764e-05,
      "loss": 0.143,
      "step": 3482
    },
    {
      "epoch": 6.966,
      "grad_norm": 0.7064575552940369,
      "learning_rate": 6.0704281712685074e-05,
      "loss": 0.1103,
      "step": 3483
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.9288350939750671,
      "learning_rate": 6.066426570628252e-05,
      "loss": 0.1062,
      "step": 3484
    },
    {
      "epoch": 6.97,
      "grad_norm": 0.5286182165145874,
      "learning_rate": 6.0624249699879955e-05,
      "loss": 0.132,
      "step": 3485
    },
    {
      "epoch": 6.9719999999999995,
      "grad_norm": 0.8041695952415466,
      "learning_rate": 6.05842336934774e-05,
      "loss": 0.1194,
      "step": 3486
    },
    {
      "epoch": 6.974,
      "grad_norm": 0.6971917152404785,
      "learning_rate": 6.0544217687074836e-05,
      "loss": 0.1323,
      "step": 3487
    },
    {
      "epoch": 6.976,
      "grad_norm": 1.2965495586395264,
      "learning_rate": 6.0504201680672267e-05,
      "loss": 0.1766,
      "step": 3488
    },
    {
      "epoch": 6.978,
      "grad_norm": 1.113431692123413,
      "learning_rate": 6.046418567426971e-05,
      "loss": 0.1271,
      "step": 3489
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.7310764193534851,
      "learning_rate": 6.042416966786715e-05,
      "loss": 0.1525,
      "step": 3490
    },
    {
      "epoch": 6.982,
      "grad_norm": 0.563079297542572,
      "learning_rate": 6.038415366146459e-05,
      "loss": 0.094,
      "step": 3491
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.9864107966423035,
      "learning_rate": 6.034413765506203e-05,
      "loss": 0.1473,
      "step": 3492
    },
    {
      "epoch": 6.986,
      "grad_norm": 0.9670032858848572,
      "learning_rate": 6.030412164865946e-05,
      "loss": 0.1052,
      "step": 3493
    },
    {
      "epoch": 6.9879999999999995,
      "grad_norm": 0.6136649250984192,
      "learning_rate": 6.026410564225691e-05,
      "loss": 0.1118,
      "step": 3494
    },
    {
      "epoch": 6.99,
      "grad_norm": 0.6848742961883545,
      "learning_rate": 6.022408963585434e-05,
      "loss": 0.1225,
      "step": 3495
    },
    {
      "epoch": 6.992,
      "grad_norm": 1.2614790201187134,
      "learning_rate": 6.0184073629451784e-05,
      "loss": 0.1557,
      "step": 3496
    },
    {
      "epoch": 6.994,
      "grad_norm": 1.1555198431015015,
      "learning_rate": 6.014405762304922e-05,
      "loss": 0.1511,
      "step": 3497
    },
    {
      "epoch": 6.996,
      "grad_norm": 1.073868989944458,
      "learning_rate": 6.0104041616646665e-05,
      "loss": 0.1332,
      "step": 3498
    },
    {
      "epoch": 6.998,
      "grad_norm": 0.9734396934509277,
      "learning_rate": 6.00640256102441e-05,
      "loss": 0.1249,
      "step": 3499
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.8009904623031616,
      "learning_rate": 6.002400960384153e-05,
      "loss": 0.1306,
      "step": 3500
    },
    {
      "epoch": 7.002,
      "grad_norm": 0.44887596368789673,
      "learning_rate": 5.998399359743898e-05,
      "loss": 0.1244,
      "step": 3501
    },
    {
      "epoch": 7.004,
      "grad_norm": 0.4648987650871277,
      "learning_rate": 5.9943977591036414e-05,
      "loss": 0.1174,
      "step": 3502
    },
    {
      "epoch": 7.006,
      "grad_norm": 0.5477139353752136,
      "learning_rate": 5.9903961584633864e-05,
      "loss": 0.1153,
      "step": 3503
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.31630581617355347,
      "learning_rate": 5.9863945578231295e-05,
      "loss": 0.0872,
      "step": 3504
    },
    {
      "epoch": 7.01,
      "grad_norm": 1.1147239208221436,
      "learning_rate": 5.982392957182873e-05,
      "loss": 0.126,
      "step": 3505
    },
    {
      "epoch": 7.012,
      "grad_norm": 0.6277046203613281,
      "learning_rate": 5.9783913565426176e-05,
      "loss": 0.0966,
      "step": 3506
    },
    {
      "epoch": 7.014,
      "grad_norm": 0.49130141735076904,
      "learning_rate": 5.974389755902361e-05,
      "loss": 0.086,
      "step": 3507
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.6670851707458496,
      "learning_rate": 5.970388155262106e-05,
      "loss": 0.1018,
      "step": 3508
    },
    {
      "epoch": 7.018,
      "grad_norm": 0.40871912240982056,
      "learning_rate": 5.966386554621849e-05,
      "loss": 0.102,
      "step": 3509
    },
    {
      "epoch": 7.02,
      "grad_norm": 0.3985995948314667,
      "learning_rate": 5.962384953981594e-05,
      "loss": 0.0974,
      "step": 3510
    },
    {
      "epoch": 7.022,
      "grad_norm": 0.4459487497806549,
      "learning_rate": 5.958383353341337e-05,
      "loss": 0.0942,
      "step": 3511
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.50679612159729,
      "learning_rate": 5.9543817527010805e-05,
      "loss": 0.1255,
      "step": 3512
    },
    {
      "epoch": 7.026,
      "grad_norm": 0.4260833263397217,
      "learning_rate": 5.950380152060825e-05,
      "loss": 0.1009,
      "step": 3513
    },
    {
      "epoch": 7.028,
      "grad_norm": 0.5024675726890564,
      "learning_rate": 5.9463785514205686e-05,
      "loss": 0.0924,
      "step": 3514
    },
    {
      "epoch": 7.03,
      "grad_norm": 0.4468156099319458,
      "learning_rate": 5.942376950780313e-05,
      "loss": 0.1151,
      "step": 3515
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.6264138221740723,
      "learning_rate": 5.938375350140056e-05,
      "loss": 0.1136,
      "step": 3516
    },
    {
      "epoch": 7.034,
      "grad_norm": 0.3723755180835724,
      "learning_rate": 5.9343737494998e-05,
      "loss": 0.0959,
      "step": 3517
    },
    {
      "epoch": 7.036,
      "grad_norm": 0.6919192671775818,
      "learning_rate": 5.930372148859544e-05,
      "loss": 0.1093,
      "step": 3518
    },
    {
      "epoch": 7.038,
      "grad_norm": 0.4454904794692993,
      "learning_rate": 5.926370548219288e-05,
      "loss": 0.113,
      "step": 3519
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.4318808615207672,
      "learning_rate": 5.922368947579032e-05,
      "loss": 0.1467,
      "step": 3520
    },
    {
      "epoch": 7.042,
      "grad_norm": 0.7293393015861511,
      "learning_rate": 5.918367346938776e-05,
      "loss": 0.1054,
      "step": 3521
    },
    {
      "epoch": 7.044,
      "grad_norm": 0.5212000012397766,
      "learning_rate": 5.914365746298519e-05,
      "loss": 0.1052,
      "step": 3522
    },
    {
      "epoch": 7.046,
      "grad_norm": 0.5240744948387146,
      "learning_rate": 5.9103641456582634e-05,
      "loss": 0.1038,
      "step": 3523
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.45535025000572205,
      "learning_rate": 5.906362545018007e-05,
      "loss": 0.0876,
      "step": 3524
    },
    {
      "epoch": 7.05,
      "grad_norm": 0.7382702827453613,
      "learning_rate": 5.9023609443777515e-05,
      "loss": 0.1211,
      "step": 3525
    },
    {
      "epoch": 7.052,
      "grad_norm": 0.5396327376365662,
      "learning_rate": 5.898359343737495e-05,
      "loss": 0.1148,
      "step": 3526
    },
    {
      "epoch": 7.054,
      "grad_norm": 0.9564219117164612,
      "learning_rate": 5.8943577430972396e-05,
      "loss": 0.1366,
      "step": 3527
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.5089857578277588,
      "learning_rate": 5.890356142456983e-05,
      "loss": 0.1155,
      "step": 3528
    },
    {
      "epoch": 7.058,
      "grad_norm": 0.45265015959739685,
      "learning_rate": 5.8863545418167264e-05,
      "loss": 0.1042,
      "step": 3529
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.4617026448249817,
      "learning_rate": 5.882352941176471e-05,
      "loss": 0.1234,
      "step": 3530
    },
    {
      "epoch": 7.062,
      "grad_norm": 0.5044220685958862,
      "learning_rate": 5.8783513405362145e-05,
      "loss": 0.1159,
      "step": 3531
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.5936897397041321,
      "learning_rate": 5.874349739895959e-05,
      "loss": 0.1179,
      "step": 3532
    },
    {
      "epoch": 7.066,
      "grad_norm": 0.55621737241745,
      "learning_rate": 5.8703481392557026e-05,
      "loss": 0.1221,
      "step": 3533
    },
    {
      "epoch": 7.068,
      "grad_norm": 0.4731248617172241,
      "learning_rate": 5.8663465386154456e-05,
      "loss": 0.1087,
      "step": 3534
    },
    {
      "epoch": 7.07,
      "grad_norm": 0.4452562928199768,
      "learning_rate": 5.862344937975191e-05,
      "loss": 0.0928,
      "step": 3535
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.4871334433555603,
      "learning_rate": 5.858343337334934e-05,
      "loss": 0.11,
      "step": 3536
    },
    {
      "epoch": 7.074,
      "grad_norm": 0.5067949295043945,
      "learning_rate": 5.854341736694679e-05,
      "loss": 0.1327,
      "step": 3537
    },
    {
      "epoch": 7.076,
      "grad_norm": 0.4288332462310791,
      "learning_rate": 5.850340136054422e-05,
      "loss": 0.1244,
      "step": 3538
    },
    {
      "epoch": 7.078,
      "grad_norm": 0.6926658749580383,
      "learning_rate": 5.846338535414166e-05,
      "loss": 0.1544,
      "step": 3539
    },
    {
      "epoch": 7.08,
      "grad_norm": 1.8891390562057495,
      "learning_rate": 5.84233693477391e-05,
      "loss": 0.1583,
      "step": 3540
    },
    {
      "epoch": 7.082,
      "grad_norm": 0.6712085604667664,
      "learning_rate": 5.8383353341336536e-05,
      "loss": 0.0954,
      "step": 3541
    },
    {
      "epoch": 7.084,
      "grad_norm": 0.48507124185562134,
      "learning_rate": 5.834333733493398e-05,
      "loss": 0.0947,
      "step": 3542
    },
    {
      "epoch": 7.086,
      "grad_norm": 0.4792875051498413,
      "learning_rate": 5.830332132853141e-05,
      "loss": 0.1057,
      "step": 3543
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.6366331577301025,
      "learning_rate": 5.826330532212886e-05,
      "loss": 0.1092,
      "step": 3544
    },
    {
      "epoch": 7.09,
      "grad_norm": 0.5590084791183472,
      "learning_rate": 5.822328931572629e-05,
      "loss": 0.1159,
      "step": 3545
    },
    {
      "epoch": 7.092,
      "grad_norm": 0.6769156455993652,
      "learning_rate": 5.818327330932373e-05,
      "loss": 0.1526,
      "step": 3546
    },
    {
      "epoch": 7.094,
      "grad_norm": 0.49994710087776184,
      "learning_rate": 5.814325730292117e-05,
      "loss": 0.0758,
      "step": 3547
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.38162368535995483,
      "learning_rate": 5.810324129651861e-05,
      "loss": 0.084,
      "step": 3548
    },
    {
      "epoch": 7.098,
      "grad_norm": 0.44649165868759155,
      "learning_rate": 5.8063225290116054e-05,
      "loss": 0.0836,
      "step": 3549
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.9076690673828125,
      "learning_rate": 5.8023209283713484e-05,
      "loss": 0.1161,
      "step": 3550
    },
    {
      "epoch": 7.102,
      "grad_norm": 0.8236676454544067,
      "learning_rate": 5.7983193277310935e-05,
      "loss": 0.099,
      "step": 3551
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.6358888149261475,
      "learning_rate": 5.7943177270908365e-05,
      "loss": 0.1249,
      "step": 3552
    },
    {
      "epoch": 7.106,
      "grad_norm": 0.6132856607437134,
      "learning_rate": 5.79031612645058e-05,
      "loss": 0.1039,
      "step": 3553
    },
    {
      "epoch": 7.108,
      "grad_norm": 0.5152263641357422,
      "learning_rate": 5.7863145258103246e-05,
      "loss": 0.1285,
      "step": 3554
    },
    {
      "epoch": 7.11,
      "grad_norm": 0.4518950283527374,
      "learning_rate": 5.782312925170068e-05,
      "loss": 0.0953,
      "step": 3555
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.623772919178009,
      "learning_rate": 5.778311324529813e-05,
      "loss": 0.1127,
      "step": 3556
    },
    {
      "epoch": 7.114,
      "grad_norm": 0.5040473937988281,
      "learning_rate": 5.774309723889556e-05,
      "loss": 0.0916,
      "step": 3557
    },
    {
      "epoch": 7.116,
      "grad_norm": 0.4701021909713745,
      "learning_rate": 5.7703081232492995e-05,
      "loss": 0.0982,
      "step": 3558
    },
    {
      "epoch": 7.118,
      "grad_norm": 0.3966226875782013,
      "learning_rate": 5.766306522609044e-05,
      "loss": 0.0914,
      "step": 3559
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.5062397122383118,
      "learning_rate": 5.7623049219687876e-05,
      "loss": 0.118,
      "step": 3560
    },
    {
      "epoch": 7.122,
      "grad_norm": 1.0299129486083984,
      "learning_rate": 5.758303321328532e-05,
      "loss": 0.1083,
      "step": 3561
    },
    {
      "epoch": 7.124,
      "grad_norm": 0.8198691606521606,
      "learning_rate": 5.754301720688276e-05,
      "loss": 0.2944,
      "step": 3562
    },
    {
      "epoch": 7.126,
      "grad_norm": 0.5712230801582336,
      "learning_rate": 5.75030012004802e-05,
      "loss": 0.1425,
      "step": 3563
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.5207117795944214,
      "learning_rate": 5.746298519407763e-05,
      "loss": 0.1047,
      "step": 3564
    },
    {
      "epoch": 7.13,
      "grad_norm": 0.5860347151756287,
      "learning_rate": 5.742296918767507e-05,
      "loss": 0.106,
      "step": 3565
    },
    {
      "epoch": 7.132,
      "grad_norm": 0.4610897898674011,
      "learning_rate": 5.738295318127251e-05,
      "loss": 0.0991,
      "step": 3566
    },
    {
      "epoch": 7.134,
      "grad_norm": 1.9043465852737427,
      "learning_rate": 5.734293717486995e-05,
      "loss": 0.1361,
      "step": 3567
    },
    {
      "epoch": 7.136,
      "grad_norm": 1.1154836416244507,
      "learning_rate": 5.730292116846739e-05,
      "loss": 0.1464,
      "step": 3568
    },
    {
      "epoch": 7.138,
      "grad_norm": 0.4652504324913025,
      "learning_rate": 5.726290516206483e-05,
      "loss": 0.1069,
      "step": 3569
    },
    {
      "epoch": 7.14,
      "grad_norm": 0.6221725344657898,
      "learning_rate": 5.722288915566226e-05,
      "loss": 0.0905,
      "step": 3570
    },
    {
      "epoch": 7.142,
      "grad_norm": 0.4537011682987213,
      "learning_rate": 5.718287314925971e-05,
      "loss": 0.1178,
      "step": 3571
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.8230007886886597,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.09,
      "step": 3572
    },
    {
      "epoch": 7.146,
      "grad_norm": 0.84135502576828,
      "learning_rate": 5.7102841136454586e-05,
      "loss": 0.1171,
      "step": 3573
    },
    {
      "epoch": 7.148,
      "grad_norm": 0.5982239842414856,
      "learning_rate": 5.706282513005202e-05,
      "loss": 0.1096,
      "step": 3574
    },
    {
      "epoch": 7.15,
      "grad_norm": 0.3887218236923218,
      "learning_rate": 5.702280912364947e-05,
      "loss": 0.1005,
      "step": 3575
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.6056670546531677,
      "learning_rate": 5.6982793117246904e-05,
      "loss": 0.1359,
      "step": 3576
    },
    {
      "epoch": 7.154,
      "grad_norm": 0.5196876525878906,
      "learning_rate": 5.6942777110844334e-05,
      "loss": 0.1054,
      "step": 3577
    },
    {
      "epoch": 7.156,
      "grad_norm": 0.5069559812545776,
      "learning_rate": 5.6902761104441785e-05,
      "loss": 0.1305,
      "step": 3578
    },
    {
      "epoch": 7.158,
      "grad_norm": 0.4669414162635803,
      "learning_rate": 5.6862745098039215e-05,
      "loss": 0.11,
      "step": 3579
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.5735223293304443,
      "learning_rate": 5.682272909163666e-05,
      "loss": 0.1442,
      "step": 3580
    },
    {
      "epoch": 7.162,
      "grad_norm": 1.3295034170150757,
      "learning_rate": 5.6782713085234096e-05,
      "loss": 0.1369,
      "step": 3581
    },
    {
      "epoch": 7.164,
      "grad_norm": 0.49698618054389954,
      "learning_rate": 5.6742697078831533e-05,
      "loss": 0.129,
      "step": 3582
    },
    {
      "epoch": 7.166,
      "grad_norm": 0.5034261345863342,
      "learning_rate": 5.670268107242898e-05,
      "loss": 0.1417,
      "step": 3583
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.5048953294754028,
      "learning_rate": 5.666266506602641e-05,
      "loss": 0.1263,
      "step": 3584
    },
    {
      "epoch": 7.17,
      "grad_norm": 0.5082709193229675,
      "learning_rate": 5.662264905962386e-05,
      "loss": 0.1227,
      "step": 3585
    },
    {
      "epoch": 7.172,
      "grad_norm": 0.617577850818634,
      "learning_rate": 5.658263305322129e-05,
      "loss": 0.0857,
      "step": 3586
    },
    {
      "epoch": 7.174,
      "grad_norm": 1.448951005935669,
      "learning_rate": 5.654261704681873e-05,
      "loss": 0.154,
      "step": 3587
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.43813034892082214,
      "learning_rate": 5.650260104041617e-05,
      "loss": 0.1124,
      "step": 3588
    },
    {
      "epoch": 7.178,
      "grad_norm": 1.1203060150146484,
      "learning_rate": 5.646258503401361e-05,
      "loss": 0.1221,
      "step": 3589
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.6084178686141968,
      "learning_rate": 5.642256902761105e-05,
      "loss": 0.1027,
      "step": 3590
    },
    {
      "epoch": 7.182,
      "grad_norm": 0.5275458097457886,
      "learning_rate": 5.638255302120848e-05,
      "loss": 0.1154,
      "step": 3591
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.7500290274620056,
      "learning_rate": 5.634253701480593e-05,
      "loss": 0.1314,
      "step": 3592
    },
    {
      "epoch": 7.186,
      "grad_norm": 0.6236048340797424,
      "learning_rate": 5.630252100840336e-05,
      "loss": 0.1128,
      "step": 3593
    },
    {
      "epoch": 7.188,
      "grad_norm": 0.47146376967430115,
      "learning_rate": 5.62625050020008e-05,
      "loss": 0.1294,
      "step": 3594
    },
    {
      "epoch": 7.19,
      "grad_norm": 0.4538862705230713,
      "learning_rate": 5.622248899559824e-05,
      "loss": 0.0859,
      "step": 3595
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.5388128757476807,
      "learning_rate": 5.618247298919568e-05,
      "loss": 0.1202,
      "step": 3596
    },
    {
      "epoch": 7.194,
      "grad_norm": 0.5222463011741638,
      "learning_rate": 5.6142456982793124e-05,
      "loss": 0.1271,
      "step": 3597
    },
    {
      "epoch": 7.196,
      "grad_norm": 0.5095696449279785,
      "learning_rate": 5.6102440976390555e-05,
      "loss": 0.1299,
      "step": 3598
    },
    {
      "epoch": 7.198,
      "grad_norm": 0.45321717858314514,
      "learning_rate": 5.6062424969988005e-05,
      "loss": 0.1149,
      "step": 3599
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.546890914440155,
      "learning_rate": 5.6022408963585436e-05,
      "loss": 0.1108,
      "step": 3600
    },
    {
      "epoch": 7.202,
      "grad_norm": 0.4885621964931488,
      "learning_rate": 5.598239295718287e-05,
      "loss": 0.1384,
      "step": 3601
    },
    {
      "epoch": 7.204,
      "grad_norm": 0.4676900804042816,
      "learning_rate": 5.594237695078032e-05,
      "loss": 0.096,
      "step": 3602
    },
    {
      "epoch": 7.206,
      "grad_norm": 0.6596384644508362,
      "learning_rate": 5.5902360944377754e-05,
      "loss": 0.1199,
      "step": 3603
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.45634326338768005,
      "learning_rate": 5.58623449379752e-05,
      "loss": 0.0848,
      "step": 3604
    },
    {
      "epoch": 7.21,
      "grad_norm": 0.7731597423553467,
      "learning_rate": 5.5822328931572635e-05,
      "loss": 0.0992,
      "step": 3605
    },
    {
      "epoch": 7.212,
      "grad_norm": 1.2659916877746582,
      "learning_rate": 5.5782312925170065e-05,
      "loss": 0.1082,
      "step": 3606
    },
    {
      "epoch": 7.214,
      "grad_norm": 0.594092071056366,
      "learning_rate": 5.574229691876751e-05,
      "loss": 0.11,
      "step": 3607
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.7316626310348511,
      "learning_rate": 5.5702280912364946e-05,
      "loss": 0.1042,
      "step": 3608
    },
    {
      "epoch": 7.218,
      "grad_norm": 0.7098239660263062,
      "learning_rate": 5.566226490596239e-05,
      "loss": 0.0925,
      "step": 3609
    },
    {
      "epoch": 7.22,
      "grad_norm": 1.1329655647277832,
      "learning_rate": 5.562224889955983e-05,
      "loss": 0.1571,
      "step": 3610
    },
    {
      "epoch": 7.222,
      "grad_norm": 0.9135532975196838,
      "learning_rate": 5.558223289315727e-05,
      "loss": 0.1147,
      "step": 3611
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.5294966697692871,
      "learning_rate": 5.554221688675471e-05,
      "loss": 0.1068,
      "step": 3612
    },
    {
      "epoch": 7.226,
      "grad_norm": 0.9886205792427063,
      "learning_rate": 5.550220088035214e-05,
      "loss": 0.1326,
      "step": 3613
    },
    {
      "epoch": 7.228,
      "grad_norm": 0.6117187142372131,
      "learning_rate": 5.546218487394958e-05,
      "loss": 0.0996,
      "step": 3614
    },
    {
      "epoch": 7.23,
      "grad_norm": 0.9010709524154663,
      "learning_rate": 5.542216886754702e-05,
      "loss": 0.121,
      "step": 3615
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.566451907157898,
      "learning_rate": 5.5382152861144464e-05,
      "loss": 0.1373,
      "step": 3616
    },
    {
      "epoch": 7.234,
      "grad_norm": 0.6184837222099304,
      "learning_rate": 5.53421368547419e-05,
      "loss": 0.1,
      "step": 3617
    },
    {
      "epoch": 7.236,
      "grad_norm": 0.41967305541038513,
      "learning_rate": 5.530212084833933e-05,
      "loss": 0.0925,
      "step": 3618
    },
    {
      "epoch": 7.2379999999999995,
      "grad_norm": 0.4200052320957184,
      "learning_rate": 5.526210484193678e-05,
      "loss": 0.1195,
      "step": 3619
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.9202626347541809,
      "learning_rate": 5.522208883553421e-05,
      "loss": 0.1327,
      "step": 3620
    },
    {
      "epoch": 7.242,
      "grad_norm": 0.6005855202674866,
      "learning_rate": 5.5182072829131656e-05,
      "loss": 0.1145,
      "step": 3621
    },
    {
      "epoch": 7.244,
      "grad_norm": 0.5192042589187622,
      "learning_rate": 5.514205682272909e-05,
      "loss": 0.1331,
      "step": 3622
    },
    {
      "epoch": 7.246,
      "grad_norm": 0.5826175808906555,
      "learning_rate": 5.510204081632653e-05,
      "loss": 0.1203,
      "step": 3623
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.7907027006149292,
      "learning_rate": 5.5062024809923974e-05,
      "loss": 0.0998,
      "step": 3624
    },
    {
      "epoch": 7.25,
      "grad_norm": 0.43841660022735596,
      "learning_rate": 5.5022008803521405e-05,
      "loss": 0.118,
      "step": 3625
    },
    {
      "epoch": 7.252,
      "grad_norm": 0.5068176984786987,
      "learning_rate": 5.4981992797118855e-05,
      "loss": 0.1256,
      "step": 3626
    },
    {
      "epoch": 7.254,
      "grad_norm": 0.7858663201332092,
      "learning_rate": 5.4941976790716286e-05,
      "loss": 0.1589,
      "step": 3627
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.8898142576217651,
      "learning_rate": 5.490196078431373e-05,
      "loss": 0.1109,
      "step": 3628
    },
    {
      "epoch": 7.258,
      "grad_norm": 0.5450770854949951,
      "learning_rate": 5.486194477791117e-05,
      "loss": 0.0953,
      "step": 3629
    },
    {
      "epoch": 7.26,
      "grad_norm": 1.084751009941101,
      "learning_rate": 5.4821928771508604e-05,
      "loss": 0.127,
      "step": 3630
    },
    {
      "epoch": 7.2620000000000005,
      "grad_norm": 0.5742809772491455,
      "learning_rate": 5.478191276510605e-05,
      "loss": 0.1551,
      "step": 3631
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.39820849895477295,
      "learning_rate": 5.474189675870348e-05,
      "loss": 0.1069,
      "step": 3632
    },
    {
      "epoch": 7.266,
      "grad_norm": 0.4956503212451935,
      "learning_rate": 5.470188075230093e-05,
      "loss": 0.1003,
      "step": 3633
    },
    {
      "epoch": 7.268,
      "grad_norm": 0.46051496267318726,
      "learning_rate": 5.466186474589836e-05,
      "loss": 0.1111,
      "step": 3634
    },
    {
      "epoch": 7.27,
      "grad_norm": 0.5604313611984253,
      "learning_rate": 5.4621848739495796e-05,
      "loss": 0.1199,
      "step": 3635
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.7325754761695862,
      "learning_rate": 5.458183273309324e-05,
      "loss": 0.1268,
      "step": 3636
    },
    {
      "epoch": 7.274,
      "grad_norm": 0.522061288356781,
      "learning_rate": 5.454181672669068e-05,
      "loss": 0.1299,
      "step": 3637
    },
    {
      "epoch": 7.276,
      "grad_norm": 0.4976620376110077,
      "learning_rate": 5.450180072028812e-05,
      "loss": 0.1035,
      "step": 3638
    },
    {
      "epoch": 7.2780000000000005,
      "grad_norm": 0.7060635685920715,
      "learning_rate": 5.446178471388556e-05,
      "loss": 0.1114,
      "step": 3639
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.4066906273365021,
      "learning_rate": 5.4421768707483e-05,
      "loss": 0.1023,
      "step": 3640
    },
    {
      "epoch": 7.282,
      "grad_norm": 0.5889694094657898,
      "learning_rate": 5.438175270108043e-05,
      "loss": 0.13,
      "step": 3641
    },
    {
      "epoch": 7.284,
      "grad_norm": 1.3056658506393433,
      "learning_rate": 5.434173669467787e-05,
      "loss": 0.123,
      "step": 3642
    },
    {
      "epoch": 7.286,
      "grad_norm": 0.7892214059829712,
      "learning_rate": 5.4301720688275314e-05,
      "loss": 0.1569,
      "step": 3643
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.6811879873275757,
      "learning_rate": 5.426170468187275e-05,
      "loss": 0.1383,
      "step": 3644
    },
    {
      "epoch": 7.29,
      "grad_norm": 0.6002840995788574,
      "learning_rate": 5.4221688675470195e-05,
      "loss": 0.1026,
      "step": 3645
    },
    {
      "epoch": 7.292,
      "grad_norm": 1.0312907695770264,
      "learning_rate": 5.418167266906763e-05,
      "loss": 0.1578,
      "step": 3646
    },
    {
      "epoch": 7.294,
      "grad_norm": 0.6078788638114929,
      "learning_rate": 5.414165666266506e-05,
      "loss": 0.0963,
      "step": 3647
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.682142436504364,
      "learning_rate": 5.4101640656262506e-05,
      "loss": 0.1193,
      "step": 3648
    },
    {
      "epoch": 7.298,
      "grad_norm": 0.539286732673645,
      "learning_rate": 5.4061624649859943e-05,
      "loss": 0.1229,
      "step": 3649
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.5046945810317993,
      "learning_rate": 5.402160864345739e-05,
      "loss": 0.1078,
      "step": 3650
    },
    {
      "epoch": 7.302,
      "grad_norm": 0.40703392028808594,
      "learning_rate": 5.3981592637054824e-05,
      "loss": 0.112,
      "step": 3651
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.4276512563228607,
      "learning_rate": 5.394157663065227e-05,
      "loss": 0.0902,
      "step": 3652
    },
    {
      "epoch": 7.306,
      "grad_norm": 0.5106431841850281,
      "learning_rate": 5.3901560624249706e-05,
      "loss": 0.143,
      "step": 3653
    },
    {
      "epoch": 7.308,
      "grad_norm": 0.5742871761322021,
      "learning_rate": 5.3861544617847136e-05,
      "loss": 0.1151,
      "step": 3654
    },
    {
      "epoch": 7.31,
      "grad_norm": 0.38171088695526123,
      "learning_rate": 5.382152861144458e-05,
      "loss": 0.0899,
      "step": 3655
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.6150491833686829,
      "learning_rate": 5.378151260504202e-05,
      "loss": 0.1005,
      "step": 3656
    },
    {
      "epoch": 7.314,
      "grad_norm": 0.5924871563911438,
      "learning_rate": 5.374149659863946e-05,
      "loss": 0.1262,
      "step": 3657
    },
    {
      "epoch": 7.316,
      "grad_norm": 1.3824816942214966,
      "learning_rate": 5.37014805922369e-05,
      "loss": 0.1131,
      "step": 3658
    },
    {
      "epoch": 7.318,
      "grad_norm": 0.5420221090316772,
      "learning_rate": 5.366146458583433e-05,
      "loss": 0.1272,
      "step": 3659
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.6802549362182617,
      "learning_rate": 5.362144857943178e-05,
      "loss": 0.1437,
      "step": 3660
    },
    {
      "epoch": 7.322,
      "grad_norm": 0.5171161890029907,
      "learning_rate": 5.358143257302921e-05,
      "loss": 0.0904,
      "step": 3661
    },
    {
      "epoch": 7.324,
      "grad_norm": 0.5470767617225647,
      "learning_rate": 5.354141656662665e-05,
      "loss": 0.1421,
      "step": 3662
    },
    {
      "epoch": 7.326,
      "grad_norm": 0.6060178875923157,
      "learning_rate": 5.350140056022409e-05,
      "loss": 0.1163,
      "step": 3663
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.6944575905799866,
      "learning_rate": 5.3461384553821534e-05,
      "loss": 0.0972,
      "step": 3664
    },
    {
      "epoch": 7.33,
      "grad_norm": 0.39960071444511414,
      "learning_rate": 5.342136854741897e-05,
      "loss": 0.1016,
      "step": 3665
    },
    {
      "epoch": 7.332,
      "grad_norm": 0.42463555932044983,
      "learning_rate": 5.33813525410164e-05,
      "loss": 0.0986,
      "step": 3666
    },
    {
      "epoch": 7.334,
      "grad_norm": 0.5230726599693298,
      "learning_rate": 5.334133653461385e-05,
      "loss": 0.1146,
      "step": 3667
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.5714201927185059,
      "learning_rate": 5.330132052821128e-05,
      "loss": 0.1071,
      "step": 3668
    },
    {
      "epoch": 7.338,
      "grad_norm": 2.3490898609161377,
      "learning_rate": 5.3261304521808734e-05,
      "loss": 0.144,
      "step": 3669
    },
    {
      "epoch": 7.34,
      "grad_norm": 0.44129014015197754,
      "learning_rate": 5.3221288515406164e-05,
      "loss": 0.0922,
      "step": 3670
    },
    {
      "epoch": 7.342,
      "grad_norm": 0.5484680533409119,
      "learning_rate": 5.31812725090036e-05,
      "loss": 0.1103,
      "step": 3671
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.5604072213172913,
      "learning_rate": 5.3141256502601045e-05,
      "loss": 0.1093,
      "step": 3672
    },
    {
      "epoch": 7.346,
      "grad_norm": 0.7663495540618896,
      "learning_rate": 5.310124049619848e-05,
      "loss": 0.109,
      "step": 3673
    },
    {
      "epoch": 7.348,
      "grad_norm": 0.43220868706703186,
      "learning_rate": 5.3061224489795926e-05,
      "loss": 0.0958,
      "step": 3674
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.4771854281425476,
      "learning_rate": 5.3021208483393356e-05,
      "loss": 0.1058,
      "step": 3675
    },
    {
      "epoch": 7.352,
      "grad_norm": 1.6599318981170654,
      "learning_rate": 5.298119247699081e-05,
      "loss": 0.1343,
      "step": 3676
    },
    {
      "epoch": 7.354,
      "grad_norm": 0.7864224910736084,
      "learning_rate": 5.294117647058824e-05,
      "loss": 0.1174,
      "step": 3677
    },
    {
      "epoch": 7.356,
      "grad_norm": 0.5022655129432678,
      "learning_rate": 5.2901160464185675e-05,
      "loss": 0.1044,
      "step": 3678
    },
    {
      "epoch": 7.358,
      "grad_norm": 0.5833438634872437,
      "learning_rate": 5.286114445778312e-05,
      "loss": 0.1465,
      "step": 3679
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.47822287678718567,
      "learning_rate": 5.2821128451380556e-05,
      "loss": 0.1259,
      "step": 3680
    },
    {
      "epoch": 7.362,
      "grad_norm": 0.5924666523933411,
      "learning_rate": 5.2781112444978e-05,
      "loss": 0.1325,
      "step": 3681
    },
    {
      "epoch": 7.364,
      "grad_norm": 0.6286964416503906,
      "learning_rate": 5.274109643857543e-05,
      "loss": 0.154,
      "step": 3682
    },
    {
      "epoch": 7.366,
      "grad_norm": 0.46674981713294983,
      "learning_rate": 5.270108043217287e-05,
      "loss": 0.0978,
      "step": 3683
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.4112626910209656,
      "learning_rate": 5.266106442577031e-05,
      "loss": 0.0759,
      "step": 3684
    },
    {
      "epoch": 7.37,
      "grad_norm": 0.5784164667129517,
      "learning_rate": 5.262104841936775e-05,
      "loss": 0.1185,
      "step": 3685
    },
    {
      "epoch": 7.372,
      "grad_norm": 1.2260754108428955,
      "learning_rate": 5.258103241296519e-05,
      "loss": 0.1112,
      "step": 3686
    },
    {
      "epoch": 7.374,
      "grad_norm": 4.421906471252441,
      "learning_rate": 5.254101640656263e-05,
      "loss": 0.1423,
      "step": 3687
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.442400723695755,
      "learning_rate": 5.250100040016007e-05,
      "loss": 0.1301,
      "step": 3688
    },
    {
      "epoch": 7.378,
      "grad_norm": 0.4884265661239624,
      "learning_rate": 5.24609843937575e-05,
      "loss": 0.1354,
      "step": 3689
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.569658637046814,
      "learning_rate": 5.242096838735494e-05,
      "loss": 0.1348,
      "step": 3690
    },
    {
      "epoch": 7.382,
      "grad_norm": 0.41355010867118835,
      "learning_rate": 5.2380952380952384e-05,
      "loss": 0.089,
      "step": 3691
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.6487164497375488,
      "learning_rate": 5.234093637454982e-05,
      "loss": 0.1028,
      "step": 3692
    },
    {
      "epoch": 7.386,
      "grad_norm": 0.46229201555252075,
      "learning_rate": 5.2300920368147265e-05,
      "loss": 0.0991,
      "step": 3693
    },
    {
      "epoch": 7.388,
      "grad_norm": 0.4433143138885498,
      "learning_rate": 5.22609043617447e-05,
      "loss": 0.0901,
      "step": 3694
    },
    {
      "epoch": 7.39,
      "grad_norm": 0.8486679196357727,
      "learning_rate": 5.222088835534213e-05,
      "loss": 0.1259,
      "step": 3695
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.5485329627990723,
      "learning_rate": 5.218087234893958e-05,
      "loss": 0.1369,
      "step": 3696
    },
    {
      "epoch": 7.394,
      "grad_norm": 0.3277324438095093,
      "learning_rate": 5.2140856342537014e-05,
      "loss": 0.1008,
      "step": 3697
    },
    {
      "epoch": 7.396,
      "grad_norm": 1.0269328355789185,
      "learning_rate": 5.210084033613446e-05,
      "loss": 0.1452,
      "step": 3698
    },
    {
      "epoch": 7.398,
      "grad_norm": 0.385394424200058,
      "learning_rate": 5.2060824329731895e-05,
      "loss": 0.1151,
      "step": 3699
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.5355331301689148,
      "learning_rate": 5.202080832332934e-05,
      "loss": 0.0864,
      "step": 3700
    },
    {
      "epoch": 7.402,
      "grad_norm": 0.43780285120010376,
      "learning_rate": 5.1980792316926776e-05,
      "loss": 0.1209,
      "step": 3701
    },
    {
      "epoch": 7.404,
      "grad_norm": 0.5995559096336365,
      "learning_rate": 5.1940776310524206e-05,
      "loss": 0.1345,
      "step": 3702
    },
    {
      "epoch": 7.406,
      "grad_norm": 0.35063064098358154,
      "learning_rate": 5.190076030412166e-05,
      "loss": 0.0898,
      "step": 3703
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.43807798624038696,
      "learning_rate": 5.186074429771909e-05,
      "loss": 0.0994,
      "step": 3704
    },
    {
      "epoch": 7.41,
      "grad_norm": 0.4633462131023407,
      "learning_rate": 5.182072829131653e-05,
      "loss": 0.1145,
      "step": 3705
    },
    {
      "epoch": 7.412,
      "grad_norm": 0.828615665435791,
      "learning_rate": 5.178071228491397e-05,
      "loss": 0.1212,
      "step": 3706
    },
    {
      "epoch": 7.414,
      "grad_norm": 0.7632105946540833,
      "learning_rate": 5.1740696278511406e-05,
      "loss": 0.1283,
      "step": 3707
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.8247567415237427,
      "learning_rate": 5.170068027210885e-05,
      "loss": 0.1367,
      "step": 3708
    },
    {
      "epoch": 7.418,
      "grad_norm": 0.34152790904045105,
      "learning_rate": 5.166066426570628e-05,
      "loss": 0.0815,
      "step": 3709
    },
    {
      "epoch": 7.42,
      "grad_norm": 1.2091333866119385,
      "learning_rate": 5.162064825930373e-05,
      "loss": 0.1484,
      "step": 3710
    },
    {
      "epoch": 7.422,
      "grad_norm": 0.61158287525177,
      "learning_rate": 5.158063225290116e-05,
      "loss": 0.0917,
      "step": 3711
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.49144041538238525,
      "learning_rate": 5.15406162464986e-05,
      "loss": 0.1187,
      "step": 3712
    },
    {
      "epoch": 7.426,
      "grad_norm": 0.503276526927948,
      "learning_rate": 5.150060024009604e-05,
      "loss": 0.1056,
      "step": 3713
    },
    {
      "epoch": 7.428,
      "grad_norm": 0.4755628705024719,
      "learning_rate": 5.146058423369348e-05,
      "loss": 0.0934,
      "step": 3714
    },
    {
      "epoch": 7.43,
      "grad_norm": 0.7737010717391968,
      "learning_rate": 5.142056822729092e-05,
      "loss": 0.11,
      "step": 3715
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.6223269701004028,
      "learning_rate": 5.1380552220888353e-05,
      "loss": 0.1664,
      "step": 3716
    },
    {
      "epoch": 7.434,
      "grad_norm": 1.5079584121704102,
      "learning_rate": 5.1340536214485804e-05,
      "loss": 0.108,
      "step": 3717
    },
    {
      "epoch": 7.436,
      "grad_norm": 0.5307230353355408,
      "learning_rate": 5.1300520208083235e-05,
      "loss": 0.1054,
      "step": 3718
    },
    {
      "epoch": 7.438,
      "grad_norm": 0.58797287940979,
      "learning_rate": 5.126050420168067e-05,
      "loss": 0.1201,
      "step": 3719
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.6753197312355042,
      "learning_rate": 5.1220488195278116e-05,
      "loss": 0.0911,
      "step": 3720
    },
    {
      "epoch": 7.442,
      "grad_norm": 0.5020191073417664,
      "learning_rate": 5.118047218887555e-05,
      "loss": 0.1112,
      "step": 3721
    },
    {
      "epoch": 7.444,
      "grad_norm": 0.37251755595207214,
      "learning_rate": 5.1140456182472997e-05,
      "loss": 0.1065,
      "step": 3722
    },
    {
      "epoch": 7.446,
      "grad_norm": 0.5684875249862671,
      "learning_rate": 5.110044017607043e-05,
      "loss": 0.1172,
      "step": 3723
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.36856621503829956,
      "learning_rate": 5.1060424169667864e-05,
      "loss": 0.0865,
      "step": 3724
    },
    {
      "epoch": 7.45,
      "grad_norm": 0.6476094722747803,
      "learning_rate": 5.102040816326531e-05,
      "loss": 0.1457,
      "step": 3725
    },
    {
      "epoch": 7.452,
      "grad_norm": 0.5822692513465881,
      "learning_rate": 5.0980392156862745e-05,
      "loss": 0.1091,
      "step": 3726
    },
    {
      "epoch": 7.454,
      "grad_norm": 0.3816443979740143,
      "learning_rate": 5.094037615046019e-05,
      "loss": 0.108,
      "step": 3727
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.5032454133033752,
      "learning_rate": 5.0900360144057626e-05,
      "loss": 0.1183,
      "step": 3728
    },
    {
      "epoch": 7.458,
      "grad_norm": 0.38522255420684814,
      "learning_rate": 5.086034413765507e-05,
      "loss": 0.1176,
      "step": 3729
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.5336347222328186,
      "learning_rate": 5.08203281312525e-05,
      "loss": 0.1056,
      "step": 3730
    },
    {
      "epoch": 7.462,
      "grad_norm": 0.6907907128334045,
      "learning_rate": 5.078031212484994e-05,
      "loss": 0.0904,
      "step": 3731
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.6026893854141235,
      "learning_rate": 5.074029611844738e-05,
      "loss": 0.167,
      "step": 3732
    },
    {
      "epoch": 7.466,
      "grad_norm": 0.6642746329307556,
      "learning_rate": 5.070028011204482e-05,
      "loss": 0.1288,
      "step": 3733
    },
    {
      "epoch": 7.468,
      "grad_norm": 0.7335132360458374,
      "learning_rate": 5.066026410564226e-05,
      "loss": 0.1058,
      "step": 3734
    },
    {
      "epoch": 7.47,
      "grad_norm": 0.6343529224395752,
      "learning_rate": 5.06202480992397e-05,
      "loss": 0.1189,
      "step": 3735
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.47407329082489014,
      "learning_rate": 5.058023209283713e-05,
      "loss": 0.1125,
      "step": 3736
    },
    {
      "epoch": 7.474,
      "grad_norm": 0.4023822844028473,
      "learning_rate": 5.054021608643458e-05,
      "loss": 0.1016,
      "step": 3737
    },
    {
      "epoch": 7.476,
      "grad_norm": 0.570338785648346,
      "learning_rate": 5.050020008003201e-05,
      "loss": 0.1306,
      "step": 3738
    },
    {
      "epoch": 7.478,
      "grad_norm": 1.4570283889770508,
      "learning_rate": 5.0460184073629455e-05,
      "loss": 0.1121,
      "step": 3739
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.4114087224006653,
      "learning_rate": 5.042016806722689e-05,
      "loss": 0.1127,
      "step": 3740
    },
    {
      "epoch": 7.482,
      "grad_norm": 0.47151920199394226,
      "learning_rate": 5.0380152060824336e-05,
      "loss": 0.1123,
      "step": 3741
    },
    {
      "epoch": 7.484,
      "grad_norm": 0.6824806928634644,
      "learning_rate": 5.034013605442177e-05,
      "loss": 0.1625,
      "step": 3742
    },
    {
      "epoch": 7.486,
      "grad_norm": 0.7117224931716919,
      "learning_rate": 5.0300120048019204e-05,
      "loss": 0.1331,
      "step": 3743
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.9692190289497375,
      "learning_rate": 5.0260104041616654e-05,
      "loss": 0.1108,
      "step": 3744
    },
    {
      "epoch": 7.49,
      "grad_norm": 0.4766383469104767,
      "learning_rate": 5.0220088035214085e-05,
      "loss": 0.1275,
      "step": 3745
    },
    {
      "epoch": 7.492,
      "grad_norm": 0.7133887410163879,
      "learning_rate": 5.018007202881153e-05,
      "loss": 0.1383,
      "step": 3746
    },
    {
      "epoch": 7.494,
      "grad_norm": 0.5828672647476196,
      "learning_rate": 5.0140056022408966e-05,
      "loss": 0.0877,
      "step": 3747
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.6569713950157166,
      "learning_rate": 5.01000400160064e-05,
      "loss": 0.1223,
      "step": 3748
    },
    {
      "epoch": 7.498,
      "grad_norm": 0.6168208718299866,
      "learning_rate": 5.006002400960385e-05,
      "loss": 0.119,
      "step": 3749
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.8251925706863403,
      "learning_rate": 5.002000800320128e-05,
      "loss": 0.1353,
      "step": 3750
    },
    {
      "epoch": 7.502,
      "grad_norm": 0.7086117267608643,
      "learning_rate": 4.997999199679872e-05,
      "loss": 0.1278,
      "step": 3751
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.6080039143562317,
      "learning_rate": 4.993997599039616e-05,
      "loss": 0.0872,
      "step": 3752
    },
    {
      "epoch": 7.506,
      "grad_norm": 1.6425811052322388,
      "learning_rate": 4.98999599839936e-05,
      "loss": 0.1669,
      "step": 3753
    },
    {
      "epoch": 7.508,
      "grad_norm": 0.736905038356781,
      "learning_rate": 4.985994397759104e-05,
      "loss": 0.1365,
      "step": 3754
    },
    {
      "epoch": 7.51,
      "grad_norm": 3.7806360721588135,
      "learning_rate": 4.9819927971188476e-05,
      "loss": 0.1318,
      "step": 3755
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 1.2422804832458496,
      "learning_rate": 4.977991196478592e-05,
      "loss": 0.1161,
      "step": 3756
    },
    {
      "epoch": 7.514,
      "grad_norm": 0.694101095199585,
      "learning_rate": 4.973989595838335e-05,
      "loss": 0.1165,
      "step": 3757
    },
    {
      "epoch": 7.516,
      "grad_norm": 0.6387872099876404,
      "learning_rate": 4.9699879951980794e-05,
      "loss": 0.1663,
      "step": 3758
    },
    {
      "epoch": 7.518,
      "grad_norm": 0.7406594753265381,
      "learning_rate": 4.965986394557823e-05,
      "loss": 0.1147,
      "step": 3759
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.5942455530166626,
      "learning_rate": 4.9619847939175676e-05,
      "loss": 0.1481,
      "step": 3760
    },
    {
      "epoch": 7.522,
      "grad_norm": 0.6230452656745911,
      "learning_rate": 4.957983193277311e-05,
      "loss": 0.0984,
      "step": 3761
    },
    {
      "epoch": 7.524,
      "grad_norm": 0.879647433757782,
      "learning_rate": 4.953981592637055e-05,
      "loss": 0.095,
      "step": 3762
    },
    {
      "epoch": 7.526,
      "grad_norm": 0.5610366463661194,
      "learning_rate": 4.949979991996799e-05,
      "loss": 0.1315,
      "step": 3763
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.7646702527999878,
      "learning_rate": 4.9459783913565424e-05,
      "loss": 0.1057,
      "step": 3764
    },
    {
      "epoch": 7.53,
      "grad_norm": 0.5584739446640015,
      "learning_rate": 4.941976790716287e-05,
      "loss": 0.1174,
      "step": 3765
    },
    {
      "epoch": 7.532,
      "grad_norm": 0.8577808737754822,
      "learning_rate": 4.9379751900760305e-05,
      "loss": 0.1157,
      "step": 3766
    },
    {
      "epoch": 7.534,
      "grad_norm": 0.6558098793029785,
      "learning_rate": 4.933973589435775e-05,
      "loss": 0.1216,
      "step": 3767
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.55487060546875,
      "learning_rate": 4.9299719887955186e-05,
      "loss": 0.1147,
      "step": 3768
    },
    {
      "epoch": 7.538,
      "grad_norm": 0.7738316655158997,
      "learning_rate": 4.925970388155262e-05,
      "loss": 0.1037,
      "step": 3769
    },
    {
      "epoch": 7.54,
      "grad_norm": 0.7873461246490479,
      "learning_rate": 4.921968787515006e-05,
      "loss": 0.1164,
      "step": 3770
    },
    {
      "epoch": 7.542,
      "grad_norm": 0.5528768301010132,
      "learning_rate": 4.9179671868747504e-05,
      "loss": 0.1017,
      "step": 3771
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.4463774263858795,
      "learning_rate": 4.913965586234494e-05,
      "loss": 0.123,
      "step": 3772
    },
    {
      "epoch": 7.546,
      "grad_norm": 0.569430410861969,
      "learning_rate": 4.909963985594238e-05,
      "loss": 0.1392,
      "step": 3773
    },
    {
      "epoch": 7.548,
      "grad_norm": 0.7061887383460999,
      "learning_rate": 4.905962384953982e-05,
      "loss": 0.1292,
      "step": 3774
    },
    {
      "epoch": 7.55,
      "grad_norm": 0.6533153653144836,
      "learning_rate": 4.901960784313725e-05,
      "loss": 0.1368,
      "step": 3775
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.42594558000564575,
      "learning_rate": 4.89795918367347e-05,
      "loss": 0.1134,
      "step": 3776
    },
    {
      "epoch": 7.554,
      "grad_norm": 0.440003901720047,
      "learning_rate": 4.8939575830332134e-05,
      "loss": 0.1227,
      "step": 3777
    },
    {
      "epoch": 7.556,
      "grad_norm": 1.4099136590957642,
      "learning_rate": 4.889955982392958e-05,
      "loss": 0.1416,
      "step": 3778
    },
    {
      "epoch": 7.558,
      "grad_norm": 0.6810293197631836,
      "learning_rate": 4.8859543817527015e-05,
      "loss": 0.1027,
      "step": 3779
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.6318795680999756,
      "learning_rate": 4.881952781112445e-05,
      "loss": 0.102,
      "step": 3780
    },
    {
      "epoch": 7.562,
      "grad_norm": 0.4222712218761444,
      "learning_rate": 4.877951180472189e-05,
      "loss": 0.1088,
      "step": 3781
    },
    {
      "epoch": 7.564,
      "grad_norm": 0.5180584788322449,
      "learning_rate": 4.8739495798319326e-05,
      "loss": 0.1114,
      "step": 3782
    },
    {
      "epoch": 7.566,
      "grad_norm": 0.5098769664764404,
      "learning_rate": 4.869947979191677e-05,
      "loss": 0.1158,
      "step": 3783
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.5598168969154358,
      "learning_rate": 4.865946378551421e-05,
      "loss": 0.0827,
      "step": 3784
    },
    {
      "epoch": 7.57,
      "grad_norm": 0.4441635012626648,
      "learning_rate": 4.861944777911165e-05,
      "loss": 0.1049,
      "step": 3785
    },
    {
      "epoch": 7.572,
      "grad_norm": 0.5287596583366394,
      "learning_rate": 4.857943177270909e-05,
      "loss": 0.1574,
      "step": 3786
    },
    {
      "epoch": 7.574,
      "grad_norm": 0.329094260931015,
      "learning_rate": 4.8539415766306526e-05,
      "loss": 0.096,
      "step": 3787
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.4256606996059418,
      "learning_rate": 4.849939975990396e-05,
      "loss": 0.1082,
      "step": 3788
    },
    {
      "epoch": 7.578,
      "grad_norm": 0.4119819700717926,
      "learning_rate": 4.84593837535014e-05,
      "loss": 0.1001,
      "step": 3789
    },
    {
      "epoch": 7.58,
      "grad_norm": 0.6914125084877014,
      "learning_rate": 4.8419367747098844e-05,
      "loss": 0.1287,
      "step": 3790
    },
    {
      "epoch": 7.582,
      "grad_norm": 0.44537708163261414,
      "learning_rate": 4.837935174069628e-05,
      "loss": 0.0993,
      "step": 3791
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.6314331293106079,
      "learning_rate": 4.8339335734293725e-05,
      "loss": 0.1526,
      "step": 3792
    },
    {
      "epoch": 7.586,
      "grad_norm": 0.3746093809604645,
      "learning_rate": 4.8299319727891155e-05,
      "loss": 0.1154,
      "step": 3793
    },
    {
      "epoch": 7.588,
      "grad_norm": 0.6526929140090942,
      "learning_rate": 4.82593037214886e-05,
      "loss": 0.1289,
      "step": 3794
    },
    {
      "epoch": 7.59,
      "grad_norm": 0.4109307825565338,
      "learning_rate": 4.8219287715086036e-05,
      "loss": 0.0856,
      "step": 3795
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.6558199524879456,
      "learning_rate": 4.817927170868347e-05,
      "loss": 0.1123,
      "step": 3796
    },
    {
      "epoch": 7.594,
      "grad_norm": 0.4853591322898865,
      "learning_rate": 4.813925570228092e-05,
      "loss": 0.1007,
      "step": 3797
    },
    {
      "epoch": 7.596,
      "grad_norm": 0.7200170159339905,
      "learning_rate": 4.809923969587835e-05,
      "loss": 0.128,
      "step": 3798
    },
    {
      "epoch": 7.598,
      "grad_norm": 0.4895782470703125,
      "learning_rate": 4.805922368947579e-05,
      "loss": 0.1008,
      "step": 3799
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.6543378829956055,
      "learning_rate": 4.801920768307323e-05,
      "loss": 0.1088,
      "step": 3800
    },
    {
      "epoch": 7.602,
      "grad_norm": 0.6653955578804016,
      "learning_rate": 4.797919167667067e-05,
      "loss": 0.0874,
      "step": 3801
    },
    {
      "epoch": 7.604,
      "grad_norm": 0.45935359597206116,
      "learning_rate": 4.793917567026811e-05,
      "loss": 0.1464,
      "step": 3802
    },
    {
      "epoch": 7.606,
      "grad_norm": 0.523718535900116,
      "learning_rate": 4.7899159663865554e-05,
      "loss": 0.1119,
      "step": 3803
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.9141249656677246,
      "learning_rate": 4.7859143657462984e-05,
      "loss": 0.1133,
      "step": 3804
    },
    {
      "epoch": 7.61,
      "grad_norm": 0.583408534526825,
      "learning_rate": 4.781912765106043e-05,
      "loss": 0.1218,
      "step": 3805
    },
    {
      "epoch": 7.612,
      "grad_norm": 1.6382882595062256,
      "learning_rate": 4.7779111644657865e-05,
      "loss": 0.112,
      "step": 3806
    },
    {
      "epoch": 7.614,
      "grad_norm": 0.5176500082015991,
      "learning_rate": 4.77390956382553e-05,
      "loss": 0.1367,
      "step": 3807
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.41284066438674927,
      "learning_rate": 4.7699079631852746e-05,
      "loss": 0.082,
      "step": 3808
    },
    {
      "epoch": 7.618,
      "grad_norm": 0.5670035481452942,
      "learning_rate": 4.765906362545018e-05,
      "loss": 0.1444,
      "step": 3809
    },
    {
      "epoch": 7.62,
      "grad_norm": 0.6022132039070129,
      "learning_rate": 4.761904761904762e-05,
      "loss": 0.1127,
      "step": 3810
    },
    {
      "epoch": 7.622,
      "grad_norm": 0.9205618500709534,
      "learning_rate": 4.757903161264506e-05,
      "loss": 0.1288,
      "step": 3811
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.36783361434936523,
      "learning_rate": 4.75390156062425e-05,
      "loss": 0.0858,
      "step": 3812
    },
    {
      "epoch": 7.626,
      "grad_norm": 0.5863220691680908,
      "learning_rate": 4.749899959983994e-05,
      "loss": 0.1476,
      "step": 3813
    },
    {
      "epoch": 7.628,
      "grad_norm": 0.6026643514633179,
      "learning_rate": 4.7458983593437376e-05,
      "loss": 0.1119,
      "step": 3814
    },
    {
      "epoch": 7.63,
      "grad_norm": 0.832815945148468,
      "learning_rate": 4.741896758703482e-05,
      "loss": 0.1477,
      "step": 3815
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.4660094976425171,
      "learning_rate": 4.737895158063225e-05,
      "loss": 0.1261,
      "step": 3816
    },
    {
      "epoch": 7.634,
      "grad_norm": 0.869973361492157,
      "learning_rate": 4.7338935574229694e-05,
      "loss": 0.1433,
      "step": 3817
    },
    {
      "epoch": 7.636,
      "grad_norm": 0.4072176516056061,
      "learning_rate": 4.729891956782713e-05,
      "loss": 0.1117,
      "step": 3818
    },
    {
      "epoch": 7.638,
      "grad_norm": 0.718721330165863,
      "learning_rate": 4.7258903561424575e-05,
      "loss": 0.1309,
      "step": 3819
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.5332087874412537,
      "learning_rate": 4.721888755502201e-05,
      "loss": 0.101,
      "step": 3820
    },
    {
      "epoch": 7.642,
      "grad_norm": 0.5821316242218018,
      "learning_rate": 4.717887154861945e-05,
      "loss": 0.105,
      "step": 3821
    },
    {
      "epoch": 7.644,
      "grad_norm": 0.42894673347473145,
      "learning_rate": 4.7138855542216886e-05,
      "loss": 0.115,
      "step": 3822
    },
    {
      "epoch": 7.646,
      "grad_norm": 0.5140872597694397,
      "learning_rate": 4.7098839535814323e-05,
      "loss": 0.1596,
      "step": 3823
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.5333967208862305,
      "learning_rate": 4.705882352941177e-05,
      "loss": 0.0892,
      "step": 3824
    },
    {
      "epoch": 7.65,
      "grad_norm": 0.4689098298549652,
      "learning_rate": 4.7018807523009204e-05,
      "loss": 0.1235,
      "step": 3825
    },
    {
      "epoch": 7.652,
      "grad_norm": 0.7975550293922424,
      "learning_rate": 4.697879151660665e-05,
      "loss": 0.1072,
      "step": 3826
    },
    {
      "epoch": 7.654,
      "grad_norm": 0.5008146166801453,
      "learning_rate": 4.6938775510204086e-05,
      "loss": 0.1186,
      "step": 3827
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.5075953006744385,
      "learning_rate": 4.689875950380152e-05,
      "loss": 0.0803,
      "step": 3828
    },
    {
      "epoch": 7.658,
      "grad_norm": 0.5177711248397827,
      "learning_rate": 4.685874349739896e-05,
      "loss": 0.1043,
      "step": 3829
    },
    {
      "epoch": 7.66,
      "grad_norm": 0.6028585433959961,
      "learning_rate": 4.68187274909964e-05,
      "loss": 0.1115,
      "step": 3830
    },
    {
      "epoch": 7.662,
      "grad_norm": 0.5204060673713684,
      "learning_rate": 4.677871148459384e-05,
      "loss": 0.1091,
      "step": 3831
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.7108899354934692,
      "learning_rate": 4.673869547819128e-05,
      "loss": 0.1284,
      "step": 3832
    },
    {
      "epoch": 7.666,
      "grad_norm": 0.4357546865940094,
      "learning_rate": 4.669867947178872e-05,
      "loss": 0.0982,
      "step": 3833
    },
    {
      "epoch": 7.668,
      "grad_norm": 0.5126529932022095,
      "learning_rate": 4.665866346538615e-05,
      "loss": 0.1291,
      "step": 3834
    },
    {
      "epoch": 7.67,
      "grad_norm": 0.5384914875030518,
      "learning_rate": 4.6618647458983596e-05,
      "loss": 0.1315,
      "step": 3835
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.38609012961387634,
      "learning_rate": 4.657863145258103e-05,
      "loss": 0.0772,
      "step": 3836
    },
    {
      "epoch": 7.674,
      "grad_norm": 0.7449805736541748,
      "learning_rate": 4.653861544617848e-05,
      "loss": 0.128,
      "step": 3837
    },
    {
      "epoch": 7.676,
      "grad_norm": 0.6193042397499084,
      "learning_rate": 4.6498599439775914e-05,
      "loss": 0.1615,
      "step": 3838
    },
    {
      "epoch": 7.678,
      "grad_norm": 0.6129113435745239,
      "learning_rate": 4.645858343337335e-05,
      "loss": 0.1346,
      "step": 3839
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.6058474183082581,
      "learning_rate": 4.641856742697079e-05,
      "loss": 0.1152,
      "step": 3840
    },
    {
      "epoch": 7.682,
      "grad_norm": 0.5015217065811157,
      "learning_rate": 4.6378551420568226e-05,
      "loss": 0.1047,
      "step": 3841
    },
    {
      "epoch": 7.684,
      "grad_norm": 0.6825374960899353,
      "learning_rate": 4.633853541416567e-05,
      "loss": 0.1102,
      "step": 3842
    },
    {
      "epoch": 7.686,
      "grad_norm": 0.750670313835144,
      "learning_rate": 4.629851940776311e-05,
      "loss": 0.1082,
      "step": 3843
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.5209071040153503,
      "learning_rate": 4.625850340136055e-05,
      "loss": 0.1412,
      "step": 3844
    },
    {
      "epoch": 7.6899999999999995,
      "grad_norm": 0.5929247736930847,
      "learning_rate": 4.621848739495799e-05,
      "loss": 0.1152,
      "step": 3845
    },
    {
      "epoch": 7.692,
      "grad_norm": 0.33293837308883667,
      "learning_rate": 4.6178471388555425e-05,
      "loss": 0.0769,
      "step": 3846
    },
    {
      "epoch": 7.694,
      "grad_norm": 0.6197779774665833,
      "learning_rate": 4.613845538215286e-05,
      "loss": 0.1163,
      "step": 3847
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.43898725509643555,
      "learning_rate": 4.60984393757503e-05,
      "loss": 0.0998,
      "step": 3848
    },
    {
      "epoch": 7.698,
      "grad_norm": 0.7473560571670532,
      "learning_rate": 4.605842336934774e-05,
      "loss": 0.1275,
      "step": 3849
    },
    {
      "epoch": 7.7,
      "grad_norm": 1.5847333669662476,
      "learning_rate": 4.601840736294518e-05,
      "loss": 0.139,
      "step": 3850
    },
    {
      "epoch": 7.702,
      "grad_norm": 0.5886403322219849,
      "learning_rate": 4.5978391356542624e-05,
      "loss": 0.1162,
      "step": 3851
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.4263143837451935,
      "learning_rate": 4.5938375350140055e-05,
      "loss": 0.0837,
      "step": 3852
    },
    {
      "epoch": 7.7059999999999995,
      "grad_norm": 0.45184168219566345,
      "learning_rate": 4.58983593437375e-05,
      "loss": 0.0991,
      "step": 3853
    },
    {
      "epoch": 7.708,
      "grad_norm": 0.45108023285865784,
      "learning_rate": 4.5858343337334936e-05,
      "loss": 0.1266,
      "step": 3854
    },
    {
      "epoch": 7.71,
      "grad_norm": 0.5083075761795044,
      "learning_rate": 4.581832733093237e-05,
      "loss": 0.1181,
      "step": 3855
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.3842693567276001,
      "learning_rate": 4.577831132452982e-05,
      "loss": 0.1227,
      "step": 3856
    },
    {
      "epoch": 7.714,
      "grad_norm": 0.5961604118347168,
      "learning_rate": 4.5738295318127254e-05,
      "loss": 0.1398,
      "step": 3857
    },
    {
      "epoch": 7.716,
      "grad_norm": 0.40584418177604675,
      "learning_rate": 4.569827931172469e-05,
      "loss": 0.0889,
      "step": 3858
    },
    {
      "epoch": 7.718,
      "grad_norm": 0.565292239189148,
      "learning_rate": 4.565826330532213e-05,
      "loss": 0.1171,
      "step": 3859
    },
    {
      "epoch": 7.72,
      "grad_norm": 1.1304945945739746,
      "learning_rate": 4.561824729891957e-05,
      "loss": 0.14,
      "step": 3860
    },
    {
      "epoch": 7.7219999999999995,
      "grad_norm": 0.3796123266220093,
      "learning_rate": 4.557823129251701e-05,
      "loss": 0.0844,
      "step": 3861
    },
    {
      "epoch": 7.724,
      "grad_norm": 0.46250519156455994,
      "learning_rate": 4.5538215286114446e-05,
      "loss": 0.1137,
      "step": 3862
    },
    {
      "epoch": 7.726,
      "grad_norm": 0.5347692966461182,
      "learning_rate": 4.549819927971189e-05,
      "loss": 0.1391,
      "step": 3863
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.4415748417377472,
      "learning_rate": 4.545818327330932e-05,
      "loss": 0.1099,
      "step": 3864
    },
    {
      "epoch": 7.73,
      "grad_norm": 0.9447327256202698,
      "learning_rate": 4.5418167266906764e-05,
      "loss": 0.1315,
      "step": 3865
    },
    {
      "epoch": 7.732,
      "grad_norm": 0.2634848356246948,
      "learning_rate": 4.53781512605042e-05,
      "loss": 0.0751,
      "step": 3866
    },
    {
      "epoch": 7.734,
      "grad_norm": 0.7490336894989014,
      "learning_rate": 4.5338135254101645e-05,
      "loss": 0.1377,
      "step": 3867
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.4474245607852936,
      "learning_rate": 4.529811924769908e-05,
      "loss": 0.1248,
      "step": 3868
    },
    {
      "epoch": 7.7379999999999995,
      "grad_norm": 0.8846324682235718,
      "learning_rate": 4.5258103241296527e-05,
      "loss": 0.1516,
      "step": 3869
    },
    {
      "epoch": 7.74,
      "grad_norm": 1.9054714441299438,
      "learning_rate": 4.521808723489396e-05,
      "loss": 0.1515,
      "step": 3870
    },
    {
      "epoch": 7.742,
      "grad_norm": 0.5897082090377808,
      "learning_rate": 4.51780712284914e-05,
      "loss": 0.1249,
      "step": 3871
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.561970591545105,
      "learning_rate": 4.513805522208884e-05,
      "loss": 0.1095,
      "step": 3872
    },
    {
      "epoch": 7.746,
      "grad_norm": 0.4450480043888092,
      "learning_rate": 4.5098039215686275e-05,
      "loss": 0.1116,
      "step": 3873
    },
    {
      "epoch": 7.748,
      "grad_norm": 0.5897471308708191,
      "learning_rate": 4.505802320928372e-05,
      "loss": 0.1125,
      "step": 3874
    },
    {
      "epoch": 7.75,
      "grad_norm": 0.5710397362709045,
      "learning_rate": 4.5018007202881156e-05,
      "loss": 0.1222,
      "step": 3875
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.3860020339488983,
      "learning_rate": 4.497799119647859e-05,
      "loss": 0.0786,
      "step": 3876
    },
    {
      "epoch": 7.754,
      "grad_norm": 0.5460359454154968,
      "learning_rate": 4.493797519007603e-05,
      "loss": 0.1249,
      "step": 3877
    },
    {
      "epoch": 7.756,
      "grad_norm": 0.5044054985046387,
      "learning_rate": 4.4897959183673474e-05,
      "loss": 0.1325,
      "step": 3878
    },
    {
      "epoch": 7.758,
      "grad_norm": 0.5347551703453064,
      "learning_rate": 4.485794317727091e-05,
      "loss": 0.1506,
      "step": 3879
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.6180174946784973,
      "learning_rate": 4.481792717086835e-05,
      "loss": 0.1532,
      "step": 3880
    },
    {
      "epoch": 7.7620000000000005,
      "grad_norm": 0.6755573749542236,
      "learning_rate": 4.477791116446579e-05,
      "loss": 0.1549,
      "step": 3881
    },
    {
      "epoch": 7.764,
      "grad_norm": 0.596030592918396,
      "learning_rate": 4.473789515806322e-05,
      "loss": 0.1252,
      "step": 3882
    },
    {
      "epoch": 7.766,
      "grad_norm": 0.3994142413139343,
      "learning_rate": 4.469787915166067e-05,
      "loss": 0.1019,
      "step": 3883
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.47548508644104004,
      "learning_rate": 4.4657863145258104e-05,
      "loss": 0.1225,
      "step": 3884
    },
    {
      "epoch": 7.77,
      "grad_norm": 0.4500311315059662,
      "learning_rate": 4.461784713885555e-05,
      "loss": 0.0959,
      "step": 3885
    },
    {
      "epoch": 7.772,
      "grad_norm": 0.32711589336395264,
      "learning_rate": 4.4577831132452985e-05,
      "loss": 0.0901,
      "step": 3886
    },
    {
      "epoch": 7.774,
      "grad_norm": 1.4505369663238525,
      "learning_rate": 4.453781512605042e-05,
      "loss": 0.1271,
      "step": 3887
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.396385133266449,
      "learning_rate": 4.449779911964786e-05,
      "loss": 0.0824,
      "step": 3888
    },
    {
      "epoch": 7.7780000000000005,
      "grad_norm": 0.6263915300369263,
      "learning_rate": 4.4457783113245296e-05,
      "loss": 0.143,
      "step": 3889
    },
    {
      "epoch": 7.78,
      "grad_norm": 0.6756431460380554,
      "learning_rate": 4.441776710684274e-05,
      "loss": 0.1003,
      "step": 3890
    },
    {
      "epoch": 7.782,
      "grad_norm": 0.532635509967804,
      "learning_rate": 4.437775110044018e-05,
      "loss": 0.1179,
      "step": 3891
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.5559109449386597,
      "learning_rate": 4.433773509403762e-05,
      "loss": 0.1263,
      "step": 3892
    },
    {
      "epoch": 7.786,
      "grad_norm": 0.5489665865898132,
      "learning_rate": 4.429771908763505e-05,
      "loss": 0.1226,
      "step": 3893
    },
    {
      "epoch": 7.788,
      "grad_norm": 0.5697391033172607,
      "learning_rate": 4.4257703081232496e-05,
      "loss": 0.1032,
      "step": 3894
    },
    {
      "epoch": 7.79,
      "grad_norm": 0.47951599955558777,
      "learning_rate": 4.421768707482993e-05,
      "loss": 0.0829,
      "step": 3895
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.7153584361076355,
      "learning_rate": 4.417767106842737e-05,
      "loss": 0.1532,
      "step": 3896
    },
    {
      "epoch": 7.7940000000000005,
      "grad_norm": 0.3969821333885193,
      "learning_rate": 4.4137655062024814e-05,
      "loss": 0.1112,
      "step": 3897
    },
    {
      "epoch": 7.796,
      "grad_norm": 0.6975289583206177,
      "learning_rate": 4.409763905562225e-05,
      "loss": 0.151,
      "step": 3898
    },
    {
      "epoch": 7.798,
      "grad_norm": 0.42192596197128296,
      "learning_rate": 4.405762304921969e-05,
      "loss": 0.0877,
      "step": 3899
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.556228518486023,
      "learning_rate": 4.4017607042817125e-05,
      "loss": 0.0882,
      "step": 3900
    },
    {
      "epoch": 7.802,
      "grad_norm": 0.54117751121521,
      "learning_rate": 4.397759103641457e-05,
      "loss": 0.1344,
      "step": 3901
    },
    {
      "epoch": 7.804,
      "grad_norm": 0.6254649758338928,
      "learning_rate": 4.3937575030012006e-05,
      "loss": 0.1398,
      "step": 3902
    },
    {
      "epoch": 7.806,
      "grad_norm": 0.7589601874351501,
      "learning_rate": 4.389755902360945e-05,
      "loss": 0.1219,
      "step": 3903
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.6941039562225342,
      "learning_rate": 4.385754301720689e-05,
      "loss": 0.1497,
      "step": 3904
    },
    {
      "epoch": 7.8100000000000005,
      "grad_norm": 0.4143906533718109,
      "learning_rate": 4.3817527010804324e-05,
      "loss": 0.116,
      "step": 3905
    },
    {
      "epoch": 7.812,
      "grad_norm": 0.6222282648086548,
      "learning_rate": 4.377751100440176e-05,
      "loss": 0.1205,
      "step": 3906
    },
    {
      "epoch": 7.814,
      "grad_norm": 0.5253493785858154,
      "learning_rate": 4.37374949979992e-05,
      "loss": 0.1019,
      "step": 3907
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.42288869619369507,
      "learning_rate": 4.369747899159664e-05,
      "loss": 0.1021,
      "step": 3908
    },
    {
      "epoch": 7.818,
      "grad_norm": 0.5064882636070251,
      "learning_rate": 4.365746298519408e-05,
      "loss": 0.1221,
      "step": 3909
    },
    {
      "epoch": 7.82,
      "grad_norm": 0.8956250548362732,
      "learning_rate": 4.3617446978791524e-05,
      "loss": 0.1261,
      "step": 3910
    },
    {
      "epoch": 7.822,
      "grad_norm": 0.7086002826690674,
      "learning_rate": 4.3577430972388954e-05,
      "loss": 0.1188,
      "step": 3911
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.8615729808807373,
      "learning_rate": 4.35374149659864e-05,
      "loss": 0.0995,
      "step": 3912
    },
    {
      "epoch": 7.826,
      "grad_norm": 0.39062559604644775,
      "learning_rate": 4.3497398959583835e-05,
      "loss": 0.0876,
      "step": 3913
    },
    {
      "epoch": 7.828,
      "grad_norm": 0.4893966317176819,
      "learning_rate": 4.345738295318127e-05,
      "loss": 0.093,
      "step": 3914
    },
    {
      "epoch": 7.83,
      "grad_norm": 0.39708787202835083,
      "learning_rate": 4.3417366946778716e-05,
      "loss": 0.0796,
      "step": 3915
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.7303552627563477,
      "learning_rate": 4.337735094037615e-05,
      "loss": 0.1272,
      "step": 3916
    },
    {
      "epoch": 7.834,
      "grad_norm": 0.6317194700241089,
      "learning_rate": 4.333733493397359e-05,
      "loss": 0.1274,
      "step": 3917
    },
    {
      "epoch": 7.836,
      "grad_norm": 0.7103927731513977,
      "learning_rate": 4.329731892757103e-05,
      "loss": 0.1069,
      "step": 3918
    },
    {
      "epoch": 7.838,
      "grad_norm": 0.6279875636100769,
      "learning_rate": 4.325730292116847e-05,
      "loss": 0.1409,
      "step": 3919
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.4855700433254242,
      "learning_rate": 4.321728691476591e-05,
      "loss": 0.119,
      "step": 3920
    },
    {
      "epoch": 7.842,
      "grad_norm": 0.37464576959609985,
      "learning_rate": 4.3177270908363346e-05,
      "loss": 0.0788,
      "step": 3921
    },
    {
      "epoch": 7.844,
      "grad_norm": 0.515663743019104,
      "learning_rate": 4.313725490196079e-05,
      "loss": 0.1341,
      "step": 3922
    },
    {
      "epoch": 7.846,
      "grad_norm": 0.6053274869918823,
      "learning_rate": 4.309723889555822e-05,
      "loss": 0.1071,
      "step": 3923
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.40766972303390503,
      "learning_rate": 4.3057222889155664e-05,
      "loss": 0.1009,
      "step": 3924
    },
    {
      "epoch": 7.85,
      "grad_norm": 0.47976458072662354,
      "learning_rate": 4.30172068827531e-05,
      "loss": 0.1079,
      "step": 3925
    },
    {
      "epoch": 7.852,
      "grad_norm": 0.7382153868675232,
      "learning_rate": 4.2977190876350545e-05,
      "loss": 0.139,
      "step": 3926
    },
    {
      "epoch": 7.854,
      "grad_norm": 0.4432190954685211,
      "learning_rate": 4.293717486994798e-05,
      "loss": 0.0896,
      "step": 3927
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.8323428630828857,
      "learning_rate": 4.289715886354542e-05,
      "loss": 0.1122,
      "step": 3928
    },
    {
      "epoch": 7.858,
      "grad_norm": 0.6386328935623169,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.1204,
      "step": 3929
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.5849472880363464,
      "learning_rate": 4.2817126850740293e-05,
      "loss": 0.1279,
      "step": 3930
    },
    {
      "epoch": 7.862,
      "grad_norm": 1.4661047458648682,
      "learning_rate": 4.277711084433774e-05,
      "loss": 0.1056,
      "step": 3931
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.4219181537628174,
      "learning_rate": 4.2737094837935174e-05,
      "loss": 0.1127,
      "step": 3932
    },
    {
      "epoch": 7.866,
      "grad_norm": 0.5560926198959351,
      "learning_rate": 4.269707883153262e-05,
      "loss": 0.1305,
      "step": 3933
    },
    {
      "epoch": 7.868,
      "grad_norm": 0.440454363822937,
      "learning_rate": 4.2657062825130056e-05,
      "loss": 0.1058,
      "step": 3934
    },
    {
      "epoch": 7.87,
      "grad_norm": 0.5122339725494385,
      "learning_rate": 4.261704681872749e-05,
      "loss": 0.0961,
      "step": 3935
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.49000823497772217,
      "learning_rate": 4.257703081232493e-05,
      "loss": 0.1259,
      "step": 3936
    },
    {
      "epoch": 7.874,
      "grad_norm": 0.5153105854988098,
      "learning_rate": 4.2537014805922374e-05,
      "loss": 0.1489,
      "step": 3937
    },
    {
      "epoch": 7.876,
      "grad_norm": 0.32638856768608093,
      "learning_rate": 4.249699879951981e-05,
      "loss": 0.11,
      "step": 3938
    },
    {
      "epoch": 7.878,
      "grad_norm": 0.6201865673065186,
      "learning_rate": 4.245698279311725e-05,
      "loss": 0.1697,
      "step": 3939
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.6414996981620789,
      "learning_rate": 4.241696678671469e-05,
      "loss": 0.0962,
      "step": 3940
    },
    {
      "epoch": 7.882,
      "grad_norm": 0.5555335283279419,
      "learning_rate": 4.237695078031212e-05,
      "loss": 0.0939,
      "step": 3941
    },
    {
      "epoch": 7.884,
      "grad_norm": 0.5073264241218567,
      "learning_rate": 4.2336934773909566e-05,
      "loss": 0.1362,
      "step": 3942
    },
    {
      "epoch": 7.886,
      "grad_norm": 0.5875450372695923,
      "learning_rate": 4.2296918767507e-05,
      "loss": 0.1024,
      "step": 3943
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.6672810316085815,
      "learning_rate": 4.225690276110445e-05,
      "loss": 0.108,
      "step": 3944
    },
    {
      "epoch": 7.89,
      "grad_norm": 0.710149884223938,
      "learning_rate": 4.2216886754701884e-05,
      "loss": 0.1132,
      "step": 3945
    },
    {
      "epoch": 7.892,
      "grad_norm": 1.1592381000518799,
      "learning_rate": 4.217687074829932e-05,
      "loss": 0.1474,
      "step": 3946
    },
    {
      "epoch": 7.894,
      "grad_norm": 0.8453391194343567,
      "learning_rate": 4.213685474189676e-05,
      "loss": 0.1614,
      "step": 3947
    },
    {
      "epoch": 7.896,
      "grad_norm": 1.2835004329681396,
      "learning_rate": 4.2096838735494196e-05,
      "loss": 0.1525,
      "step": 3948
    },
    {
      "epoch": 7.898,
      "grad_norm": 0.6053327918052673,
      "learning_rate": 4.205682272909164e-05,
      "loss": 0.1401,
      "step": 3949
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.5644324421882629,
      "learning_rate": 4.201680672268908e-05,
      "loss": 0.1322,
      "step": 3950
    },
    {
      "epoch": 7.902,
      "grad_norm": 0.43620234727859497,
      "learning_rate": 4.197679071628652e-05,
      "loss": 0.1379,
      "step": 3951
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.4558800160884857,
      "learning_rate": 4.193677470988396e-05,
      "loss": 0.1189,
      "step": 3952
    },
    {
      "epoch": 7.906,
      "grad_norm": 0.4055621027946472,
      "learning_rate": 4.1896758703481395e-05,
      "loss": 0.1131,
      "step": 3953
    },
    {
      "epoch": 7.908,
      "grad_norm": 0.4840191602706909,
      "learning_rate": 4.185674269707883e-05,
      "loss": 0.133,
      "step": 3954
    },
    {
      "epoch": 7.91,
      "grad_norm": 0.521873414516449,
      "learning_rate": 4.181672669067627e-05,
      "loss": 0.1323,
      "step": 3955
    },
    {
      "epoch": 7.912,
      "grad_norm": 1.2159556150436401,
      "learning_rate": 4.177671068427371e-05,
      "loss": 0.1305,
      "step": 3956
    },
    {
      "epoch": 7.914,
      "grad_norm": 0.4802549481391907,
      "learning_rate": 4.173669467787115e-05,
      "loss": 0.1119,
      "step": 3957
    },
    {
      "epoch": 7.916,
      "grad_norm": 0.4599970877170563,
      "learning_rate": 4.1696678671468594e-05,
      "loss": 0.1612,
      "step": 3958
    },
    {
      "epoch": 7.918,
      "grad_norm": 0.4212503433227539,
      "learning_rate": 4.1656662665066025e-05,
      "loss": 0.1075,
      "step": 3959
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.5628971457481384,
      "learning_rate": 4.161664665866347e-05,
      "loss": 0.1153,
      "step": 3960
    },
    {
      "epoch": 7.922,
      "grad_norm": 0.49444952607154846,
      "learning_rate": 4.1576630652260906e-05,
      "loss": 0.0886,
      "step": 3961
    },
    {
      "epoch": 7.924,
      "grad_norm": 0.6588990092277527,
      "learning_rate": 4.153661464585834e-05,
      "loss": 0.107,
      "step": 3962
    },
    {
      "epoch": 7.926,
      "grad_norm": 0.9991551637649536,
      "learning_rate": 4.149659863945579e-05,
      "loss": 0.1277,
      "step": 3963
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.6414743065834045,
      "learning_rate": 4.1456582633053224e-05,
      "loss": 0.1236,
      "step": 3964
    },
    {
      "epoch": 7.93,
      "grad_norm": 0.5222769975662231,
      "learning_rate": 4.141656662665066e-05,
      "loss": 0.1326,
      "step": 3965
    },
    {
      "epoch": 7.932,
      "grad_norm": 0.4457944929599762,
      "learning_rate": 4.13765506202481e-05,
      "loss": 0.1158,
      "step": 3966
    },
    {
      "epoch": 7.934,
      "grad_norm": 0.4814772605895996,
      "learning_rate": 4.133653461384554e-05,
      "loss": 0.1202,
      "step": 3967
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.840684711933136,
      "learning_rate": 4.129651860744298e-05,
      "loss": 0.1138,
      "step": 3968
    },
    {
      "epoch": 7.938,
      "grad_norm": 0.6228768825531006,
      "learning_rate": 4.125650260104042e-05,
      "loss": 0.1174,
      "step": 3969
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 0.6407037377357483,
      "learning_rate": 4.121648659463786e-05,
      "loss": 0.141,
      "step": 3970
    },
    {
      "epoch": 7.942,
      "grad_norm": 0.6025834679603577,
      "learning_rate": 4.11764705882353e-05,
      "loss": 0.1178,
      "step": 3971
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.612045168876648,
      "learning_rate": 4.1136454581832734e-05,
      "loss": 0.1284,
      "step": 3972
    },
    {
      "epoch": 7.946,
      "grad_norm": 0.7182304859161377,
      "learning_rate": 4.109643857543017e-05,
      "loss": 0.1555,
      "step": 3973
    },
    {
      "epoch": 7.948,
      "grad_norm": 0.7306797504425049,
      "learning_rate": 4.1056422569027615e-05,
      "loss": 0.1225,
      "step": 3974
    },
    {
      "epoch": 7.95,
      "grad_norm": 0.4075043499469757,
      "learning_rate": 4.101640656262505e-05,
      "loss": 0.0866,
      "step": 3975
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.5703368782997131,
      "learning_rate": 4.0976390556222497e-05,
      "loss": 0.1262,
      "step": 3976
    },
    {
      "epoch": 7.954,
      "grad_norm": 0.7241409420967102,
      "learning_rate": 4.093637454981993e-05,
      "loss": 0.1631,
      "step": 3977
    },
    {
      "epoch": 7.9559999999999995,
      "grad_norm": 0.45586833357810974,
      "learning_rate": 4.089635854341737e-05,
      "loss": 0.0914,
      "step": 3978
    },
    {
      "epoch": 7.958,
      "grad_norm": 1.18431556224823,
      "learning_rate": 4.085634253701481e-05,
      "loss": 0.1309,
      "step": 3979
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.3473885953426361,
      "learning_rate": 4.0816326530612245e-05,
      "loss": 0.0967,
      "step": 3980
    },
    {
      "epoch": 7.962,
      "grad_norm": 0.6059147715568542,
      "learning_rate": 4.077631052420969e-05,
      "loss": 0.1401,
      "step": 3981
    },
    {
      "epoch": 7.964,
      "grad_norm": 0.4143354594707489,
      "learning_rate": 4.073629451780712e-05,
      "loss": 0.0849,
      "step": 3982
    },
    {
      "epoch": 7.966,
      "grad_norm": 0.4465894103050232,
      "learning_rate": 4.069627851140456e-05,
      "loss": 0.0918,
      "step": 3983
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.6296901106834412,
      "learning_rate": 4.0656262505002e-05,
      "loss": 0.1228,
      "step": 3984
    },
    {
      "epoch": 7.97,
      "grad_norm": 0.5013659000396729,
      "learning_rate": 4.0616246498599444e-05,
      "loss": 0.1209,
      "step": 3985
    },
    {
      "epoch": 7.9719999999999995,
      "grad_norm": 0.9917059540748596,
      "learning_rate": 4.057623049219688e-05,
      "loss": 0.1349,
      "step": 3986
    },
    {
      "epoch": 7.974,
      "grad_norm": 0.5414991974830627,
      "learning_rate": 4.053621448579432e-05,
      "loss": 0.135,
      "step": 3987
    },
    {
      "epoch": 7.976,
      "grad_norm": 0.6097732186317444,
      "learning_rate": 4.0496198479391756e-05,
      "loss": 0.1366,
      "step": 3988
    },
    {
      "epoch": 7.978,
      "grad_norm": 0.5813039541244507,
      "learning_rate": 4.045618247298919e-05,
      "loss": 0.101,
      "step": 3989
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.5733145475387573,
      "learning_rate": 4.041616646658664e-05,
      "loss": 0.1534,
      "step": 3990
    },
    {
      "epoch": 7.982,
      "grad_norm": 0.5118032693862915,
      "learning_rate": 4.0376150460184074e-05,
      "loss": 0.1312,
      "step": 3991
    },
    {
      "epoch": 7.984,
      "grad_norm": 0.45760324597358704,
      "learning_rate": 4.033613445378152e-05,
      "loss": 0.1194,
      "step": 3992
    },
    {
      "epoch": 7.986,
      "grad_norm": 0.46225595474243164,
      "learning_rate": 4.0296118447378955e-05,
      "loss": 0.1378,
      "step": 3993
    },
    {
      "epoch": 7.9879999999999995,
      "grad_norm": 0.787483811378479,
      "learning_rate": 4.025610244097639e-05,
      "loss": 0.1494,
      "step": 3994
    },
    {
      "epoch": 7.99,
      "grad_norm": 0.8466051816940308,
      "learning_rate": 4.021608643457383e-05,
      "loss": 0.1482,
      "step": 3995
    },
    {
      "epoch": 7.992,
      "grad_norm": 0.41439324617385864,
      "learning_rate": 4.0176070428171266e-05,
      "loss": 0.0999,
      "step": 3996
    },
    {
      "epoch": 7.994,
      "grad_norm": 0.8530663847923279,
      "learning_rate": 4.013605442176871e-05,
      "loss": 0.1568,
      "step": 3997
    },
    {
      "epoch": 7.996,
      "grad_norm": 0.5376242399215698,
      "learning_rate": 4.009603841536615e-05,
      "loss": 0.1574,
      "step": 3998
    },
    {
      "epoch": 7.998,
      "grad_norm": 0.7132706642150879,
      "learning_rate": 4.005602240896359e-05,
      "loss": 0.1747,
      "step": 3999
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.8595070838928223,
      "learning_rate": 4.001600640256102e-05,
      "loss": 0.1129,
      "step": 4000
    },
    {
      "epoch": 8.002,
      "grad_norm": 0.3440576195716858,
      "learning_rate": 3.9975990396158466e-05,
      "loss": 0.0724,
      "step": 4001
    },
    {
      "epoch": 8.004,
      "grad_norm": 0.35646119713783264,
      "learning_rate": 3.99359743897559e-05,
      "loss": 0.1235,
      "step": 4002
    },
    {
      "epoch": 8.006,
      "grad_norm": 0.24009667336940765,
      "learning_rate": 3.9895958383353347e-05,
      "loss": 0.0748,
      "step": 4003
    },
    {
      "epoch": 8.008,
      "grad_norm": 0.48024749755859375,
      "learning_rate": 3.9855942376950784e-05,
      "loss": 0.1177,
      "step": 4004
    },
    {
      "epoch": 8.01,
      "grad_norm": 0.378246545791626,
      "learning_rate": 3.981592637054822e-05,
      "loss": 0.094,
      "step": 4005
    },
    {
      "epoch": 8.012,
      "grad_norm": 0.3982827067375183,
      "learning_rate": 3.977591036414566e-05,
      "loss": 0.1196,
      "step": 4006
    },
    {
      "epoch": 8.014,
      "grad_norm": 0.4203987717628479,
      "learning_rate": 3.9735894357743095e-05,
      "loss": 0.109,
      "step": 4007
    },
    {
      "epoch": 8.016,
      "grad_norm": 0.3591456413269043,
      "learning_rate": 3.969587835134054e-05,
      "loss": 0.083,
      "step": 4008
    },
    {
      "epoch": 8.018,
      "grad_norm": 0.3516215980052948,
      "learning_rate": 3.9655862344937976e-05,
      "loss": 0.096,
      "step": 4009
    },
    {
      "epoch": 8.02,
      "grad_norm": 0.414462149143219,
      "learning_rate": 3.961584633853542e-05,
      "loss": 0.108,
      "step": 4010
    },
    {
      "epoch": 8.022,
      "grad_norm": 0.34572672843933105,
      "learning_rate": 3.957583033213286e-05,
      "loss": 0.1049,
      "step": 4011
    },
    {
      "epoch": 8.024,
      "grad_norm": 0.5035776495933533,
      "learning_rate": 3.9535814325730294e-05,
      "loss": 0.1433,
      "step": 4012
    },
    {
      "epoch": 8.026,
      "grad_norm": 0.3279533088207245,
      "learning_rate": 3.949579831932773e-05,
      "loss": 0.094,
      "step": 4013
    },
    {
      "epoch": 8.028,
      "grad_norm": 0.36492693424224854,
      "learning_rate": 3.945578231292517e-05,
      "loss": 0.0831,
      "step": 4014
    },
    {
      "epoch": 8.03,
      "grad_norm": 0.4870404303073883,
      "learning_rate": 3.941576630652261e-05,
      "loss": 0.1325,
      "step": 4015
    },
    {
      "epoch": 8.032,
      "grad_norm": 0.29635271430015564,
      "learning_rate": 3.937575030012005e-05,
      "loss": 0.0739,
      "step": 4016
    },
    {
      "epoch": 8.034,
      "grad_norm": 0.475094735622406,
      "learning_rate": 3.9335734293717494e-05,
      "loss": 0.1046,
      "step": 4017
    },
    {
      "epoch": 8.036,
      "grad_norm": 0.3889043927192688,
      "learning_rate": 3.9295718287314924e-05,
      "loss": 0.0989,
      "step": 4018
    },
    {
      "epoch": 8.038,
      "grad_norm": 0.42134514451026917,
      "learning_rate": 3.925570228091237e-05,
      "loss": 0.1206,
      "step": 4019
    },
    {
      "epoch": 8.04,
      "grad_norm": 0.331815242767334,
      "learning_rate": 3.9215686274509805e-05,
      "loss": 0.083,
      "step": 4020
    },
    {
      "epoch": 8.042,
      "grad_norm": 0.3689109981060028,
      "learning_rate": 3.917567026810724e-05,
      "loss": 0.0812,
      "step": 4021
    },
    {
      "epoch": 8.044,
      "grad_norm": 0.5247299671173096,
      "learning_rate": 3.9135654261704686e-05,
      "loss": 0.1058,
      "step": 4022
    },
    {
      "epoch": 8.046,
      "grad_norm": 0.4340679347515106,
      "learning_rate": 3.909563825530212e-05,
      "loss": 0.1312,
      "step": 4023
    },
    {
      "epoch": 8.048,
      "grad_norm": 0.35609450936317444,
      "learning_rate": 3.905562224889956e-05,
      "loss": 0.0877,
      "step": 4024
    },
    {
      "epoch": 8.05,
      "grad_norm": 0.519216001033783,
      "learning_rate": 3.9015606242497e-05,
      "loss": 0.0726,
      "step": 4025
    },
    {
      "epoch": 8.052,
      "grad_norm": 0.2847779393196106,
      "learning_rate": 3.897559023609444e-05,
      "loss": 0.0799,
      "step": 4026
    },
    {
      "epoch": 8.054,
      "grad_norm": 0.4854060113430023,
      "learning_rate": 3.893557422969188e-05,
      "loss": 0.1307,
      "step": 4027
    },
    {
      "epoch": 8.056,
      "grad_norm": 0.43560007214546204,
      "learning_rate": 3.8895558223289316e-05,
      "loss": 0.1197,
      "step": 4028
    },
    {
      "epoch": 8.058,
      "grad_norm": 0.4648907780647278,
      "learning_rate": 3.885554221688676e-05,
      "loss": 0.1057,
      "step": 4029
    },
    {
      "epoch": 8.06,
      "grad_norm": 0.7483433485031128,
      "learning_rate": 3.881552621048419e-05,
      "loss": 0.1136,
      "step": 4030
    },
    {
      "epoch": 8.062,
      "grad_norm": 0.3896527588367462,
      "learning_rate": 3.8775510204081634e-05,
      "loss": 0.0905,
      "step": 4031
    },
    {
      "epoch": 8.064,
      "grad_norm": 0.4262152314186096,
      "learning_rate": 3.873549419767907e-05,
      "loss": 0.1129,
      "step": 4032
    },
    {
      "epoch": 8.066,
      "grad_norm": 0.4422173798084259,
      "learning_rate": 3.8695478191276515e-05,
      "loss": 0.1165,
      "step": 4033
    },
    {
      "epoch": 8.068,
      "grad_norm": 0.4238499701023102,
      "learning_rate": 3.865546218487395e-05,
      "loss": 0.1222,
      "step": 4034
    },
    {
      "epoch": 8.07,
      "grad_norm": 0.6345570087432861,
      "learning_rate": 3.8615446178471396e-05,
      "loss": 0.1002,
      "step": 4035
    },
    {
      "epoch": 8.072,
      "grad_norm": 0.6135135889053345,
      "learning_rate": 3.8575430172068826e-05,
      "loss": 0.1241,
      "step": 4036
    },
    {
      "epoch": 8.074,
      "grad_norm": 0.4179784655570984,
      "learning_rate": 3.853541416566627e-05,
      "loss": 0.1213,
      "step": 4037
    },
    {
      "epoch": 8.076,
      "grad_norm": 0.3709390461444855,
      "learning_rate": 3.849539815926371e-05,
      "loss": 0.1059,
      "step": 4038
    },
    {
      "epoch": 8.078,
      "grad_norm": 0.3975006639957428,
      "learning_rate": 3.8455382152861144e-05,
      "loss": 0.1077,
      "step": 4039
    },
    {
      "epoch": 8.08,
      "grad_norm": 0.5318056344985962,
      "learning_rate": 3.841536614645859e-05,
      "loss": 0.1109,
      "step": 4040
    },
    {
      "epoch": 8.082,
      "grad_norm": 0.38293588161468506,
      "learning_rate": 3.8375350140056026e-05,
      "loss": 0.087,
      "step": 4041
    },
    {
      "epoch": 8.084,
      "grad_norm": 0.3410661518573761,
      "learning_rate": 3.833533413365346e-05,
      "loss": 0.0798,
      "step": 4042
    },
    {
      "epoch": 8.086,
      "grad_norm": 0.7608605027198792,
      "learning_rate": 3.82953181272509e-05,
      "loss": 0.1181,
      "step": 4043
    },
    {
      "epoch": 8.088,
      "grad_norm": 0.4579680263996124,
      "learning_rate": 3.8255302120848344e-05,
      "loss": 0.1166,
      "step": 4044
    },
    {
      "epoch": 8.09,
      "grad_norm": 0.845486044883728,
      "learning_rate": 3.821528611444578e-05,
      "loss": 0.0942,
      "step": 4045
    },
    {
      "epoch": 8.092,
      "grad_norm": 0.501477062702179,
      "learning_rate": 3.817527010804322e-05,
      "loss": 0.129,
      "step": 4046
    },
    {
      "epoch": 8.094,
      "grad_norm": 0.4346827566623688,
      "learning_rate": 3.813525410164066e-05,
      "loss": 0.1092,
      "step": 4047
    },
    {
      "epoch": 8.096,
      "grad_norm": 0.4412859380245209,
      "learning_rate": 3.809523809523809e-05,
      "loss": 0.0759,
      "step": 4048
    },
    {
      "epoch": 8.098,
      "grad_norm": 0.45438152551651,
      "learning_rate": 3.8055222088835536e-05,
      "loss": 0.0788,
      "step": 4049
    },
    {
      "epoch": 8.1,
      "grad_norm": 0.35139891505241394,
      "learning_rate": 3.801520608243297e-05,
      "loss": 0.1064,
      "step": 4050
    },
    {
      "epoch": 8.102,
      "grad_norm": 0.4356532692909241,
      "learning_rate": 3.797519007603042e-05,
      "loss": 0.1248,
      "step": 4051
    },
    {
      "epoch": 8.104,
      "grad_norm": 0.5297211408615112,
      "learning_rate": 3.7935174069627854e-05,
      "loss": 0.1067,
      "step": 4052
    },
    {
      "epoch": 8.106,
      "grad_norm": 0.4536554217338562,
      "learning_rate": 3.789515806322529e-05,
      "loss": 0.1021,
      "step": 4053
    },
    {
      "epoch": 8.108,
      "grad_norm": 0.4497925937175751,
      "learning_rate": 3.785514205682273e-05,
      "loss": 0.0955,
      "step": 4054
    },
    {
      "epoch": 8.11,
      "grad_norm": 0.33396291732788086,
      "learning_rate": 3.7815126050420166e-05,
      "loss": 0.1098,
      "step": 4055
    },
    {
      "epoch": 8.112,
      "grad_norm": 0.3994367718696594,
      "learning_rate": 3.777511004401761e-05,
      "loss": 0.0983,
      "step": 4056
    },
    {
      "epoch": 8.114,
      "grad_norm": 0.28116506338119507,
      "learning_rate": 3.773509403761505e-05,
      "loss": 0.0734,
      "step": 4057
    },
    {
      "epoch": 8.116,
      "grad_norm": 0.3820231854915619,
      "learning_rate": 3.769507803121249e-05,
      "loss": 0.0924,
      "step": 4058
    },
    {
      "epoch": 8.118,
      "grad_norm": 0.6956213712692261,
      "learning_rate": 3.765506202480993e-05,
      "loss": 0.0939,
      "step": 4059
    },
    {
      "epoch": 8.12,
      "grad_norm": 0.4212368130683899,
      "learning_rate": 3.7615046018407365e-05,
      "loss": 0.1101,
      "step": 4060
    },
    {
      "epoch": 8.122,
      "grad_norm": 0.5630717277526855,
      "learning_rate": 3.75750300120048e-05,
      "loss": 0.0884,
      "step": 4061
    },
    {
      "epoch": 8.124,
      "grad_norm": 0.33989638090133667,
      "learning_rate": 3.753501400560224e-05,
      "loss": 0.0699,
      "step": 4062
    },
    {
      "epoch": 8.126,
      "grad_norm": 0.46350109577178955,
      "learning_rate": 3.749499799919968e-05,
      "loss": 0.1098,
      "step": 4063
    },
    {
      "epoch": 8.128,
      "grad_norm": 0.3052024245262146,
      "learning_rate": 3.745498199279712e-05,
      "loss": 0.0739,
      "step": 4064
    },
    {
      "epoch": 8.13,
      "grad_norm": 0.442625492811203,
      "learning_rate": 3.7414965986394564e-05,
      "loss": 0.1036,
      "step": 4065
    },
    {
      "epoch": 8.132,
      "grad_norm": 0.5739833116531372,
      "learning_rate": 3.7374949979991995e-05,
      "loss": 0.1127,
      "step": 4066
    },
    {
      "epoch": 8.134,
      "grad_norm": 0.5121166110038757,
      "learning_rate": 3.733493397358944e-05,
      "loss": 0.1199,
      "step": 4067
    },
    {
      "epoch": 8.136,
      "grad_norm": 0.4213239848613739,
      "learning_rate": 3.7294917967186876e-05,
      "loss": 0.1003,
      "step": 4068
    },
    {
      "epoch": 8.138,
      "grad_norm": 0.5369391441345215,
      "learning_rate": 3.725490196078432e-05,
      "loss": 0.1415,
      "step": 4069
    },
    {
      "epoch": 8.14,
      "grad_norm": 1.3762670755386353,
      "learning_rate": 3.721488595438176e-05,
      "loss": 0.1192,
      "step": 4070
    },
    {
      "epoch": 8.142,
      "grad_norm": 0.5401626825332642,
      "learning_rate": 3.7174869947979194e-05,
      "loss": 0.148,
      "step": 4071
    },
    {
      "epoch": 8.144,
      "grad_norm": 0.49005138874053955,
      "learning_rate": 3.713485394157663e-05,
      "loss": 0.0941,
      "step": 4072
    },
    {
      "epoch": 8.146,
      "grad_norm": 0.3965403735637665,
      "learning_rate": 3.709483793517407e-05,
      "loss": 0.0937,
      "step": 4073
    },
    {
      "epoch": 8.148,
      "grad_norm": 0.38658347725868225,
      "learning_rate": 3.705482192877151e-05,
      "loss": 0.1168,
      "step": 4074
    },
    {
      "epoch": 8.15,
      "grad_norm": 0.35495996475219727,
      "learning_rate": 3.701480592236895e-05,
      "loss": 0.0927,
      "step": 4075
    },
    {
      "epoch": 8.152,
      "grad_norm": 0.5200589299201965,
      "learning_rate": 3.697478991596639e-05,
      "loss": 0.1245,
      "step": 4076
    },
    {
      "epoch": 8.154,
      "grad_norm": 0.46666616201400757,
      "learning_rate": 3.693477390956382e-05,
      "loss": 0.0942,
      "step": 4077
    },
    {
      "epoch": 8.156,
      "grad_norm": 0.3660443425178528,
      "learning_rate": 3.689475790316127e-05,
      "loss": 0.096,
      "step": 4078
    },
    {
      "epoch": 8.158,
      "grad_norm": 0.3216494619846344,
      "learning_rate": 3.6854741896758704e-05,
      "loss": 0.0858,
      "step": 4079
    },
    {
      "epoch": 8.16,
      "grad_norm": 0.38074812293052673,
      "learning_rate": 3.681472589035614e-05,
      "loss": 0.1141,
      "step": 4080
    },
    {
      "epoch": 8.162,
      "grad_norm": 0.5282157063484192,
      "learning_rate": 3.6774709883953585e-05,
      "loss": 0.1163,
      "step": 4081
    },
    {
      "epoch": 8.164,
      "grad_norm": 0.4834944009780884,
      "learning_rate": 3.673469387755102e-05,
      "loss": 0.0959,
      "step": 4082
    },
    {
      "epoch": 8.166,
      "grad_norm": 0.49306520819664,
      "learning_rate": 3.669467787114846e-05,
      "loss": 0.1335,
      "step": 4083
    },
    {
      "epoch": 8.168,
      "grad_norm": 0.4193532168865204,
      "learning_rate": 3.66546618647459e-05,
      "loss": 0.1002,
      "step": 4084
    },
    {
      "epoch": 8.17,
      "grad_norm": 0.3782297372817993,
      "learning_rate": 3.661464585834334e-05,
      "loss": 0.0977,
      "step": 4085
    },
    {
      "epoch": 8.172,
      "grad_norm": 0.5143343210220337,
      "learning_rate": 3.657462985194078e-05,
      "loss": 0.0914,
      "step": 4086
    },
    {
      "epoch": 8.174,
      "grad_norm": 0.42044541239738464,
      "learning_rate": 3.6534613845538215e-05,
      "loss": 0.1191,
      "step": 4087
    },
    {
      "epoch": 8.176,
      "grad_norm": 0.467669278383255,
      "learning_rate": 3.649459783913566e-05,
      "loss": 0.1181,
      "step": 4088
    },
    {
      "epoch": 8.178,
      "grad_norm": 0.36907991766929626,
      "learning_rate": 3.645458183273309e-05,
      "loss": 0.096,
      "step": 4089
    },
    {
      "epoch": 8.18,
      "grad_norm": 0.3865630626678467,
      "learning_rate": 3.641456582633053e-05,
      "loss": 0.0745,
      "step": 4090
    },
    {
      "epoch": 8.182,
      "grad_norm": 0.39206644892692566,
      "learning_rate": 3.637454981992797e-05,
      "loss": 0.095,
      "step": 4091
    },
    {
      "epoch": 8.184,
      "grad_norm": 0.44340166449546814,
      "learning_rate": 3.6334533813525414e-05,
      "loss": 0.0889,
      "step": 4092
    },
    {
      "epoch": 8.186,
      "grad_norm": 0.41816556453704834,
      "learning_rate": 3.629451780712285e-05,
      "loss": 0.1229,
      "step": 4093
    },
    {
      "epoch": 8.188,
      "grad_norm": 0.4117770493030548,
      "learning_rate": 3.625450180072029e-05,
      "loss": 0.083,
      "step": 4094
    },
    {
      "epoch": 8.19,
      "grad_norm": 0.4269512891769409,
      "learning_rate": 3.6214485794317726e-05,
      "loss": 0.1243,
      "step": 4095
    },
    {
      "epoch": 8.192,
      "grad_norm": 0.6405969262123108,
      "learning_rate": 3.617446978791516e-05,
      "loss": 0.139,
      "step": 4096
    },
    {
      "epoch": 8.194,
      "grad_norm": 0.3847982883453369,
      "learning_rate": 3.613445378151261e-05,
      "loss": 0.1063,
      "step": 4097
    },
    {
      "epoch": 8.196,
      "grad_norm": 0.7348908185958862,
      "learning_rate": 3.6094437775110044e-05,
      "loss": 0.0989,
      "step": 4098
    },
    {
      "epoch": 8.198,
      "grad_norm": 0.36624711751937866,
      "learning_rate": 3.605442176870749e-05,
      "loss": 0.0894,
      "step": 4099
    },
    {
      "epoch": 8.2,
      "grad_norm": 0.5131208300590515,
      "learning_rate": 3.6014405762304925e-05,
      "loss": 0.108,
      "step": 4100
    },
    {
      "epoch": 8.202,
      "grad_norm": 0.8680956363677979,
      "learning_rate": 3.597438975590236e-05,
      "loss": 0.1078,
      "step": 4101
    },
    {
      "epoch": 8.204,
      "grad_norm": 0.418832927942276,
      "learning_rate": 3.59343737494998e-05,
      "loss": 0.1288,
      "step": 4102
    },
    {
      "epoch": 8.206,
      "grad_norm": 0.41309788823127747,
      "learning_rate": 3.589435774309724e-05,
      "loss": 0.0953,
      "step": 4103
    },
    {
      "epoch": 8.208,
      "grad_norm": 0.42103323340415955,
      "learning_rate": 3.585434173669468e-05,
      "loss": 0.0929,
      "step": 4104
    },
    {
      "epoch": 8.21,
      "grad_norm": 0.4128718376159668,
      "learning_rate": 3.581432573029212e-05,
      "loss": 0.1083,
      "step": 4105
    },
    {
      "epoch": 8.212,
      "grad_norm": 0.29413533210754395,
      "learning_rate": 3.577430972388956e-05,
      "loss": 0.0903,
      "step": 4106
    },
    {
      "epoch": 8.214,
      "grad_norm": 0.4043130576610565,
      "learning_rate": 3.573429371748699e-05,
      "loss": 0.0787,
      "step": 4107
    },
    {
      "epoch": 8.216,
      "grad_norm": 0.4132097661495209,
      "learning_rate": 3.5694277711084436e-05,
      "loss": 0.0981,
      "step": 4108
    },
    {
      "epoch": 8.218,
      "grad_norm": 0.3892105221748352,
      "learning_rate": 3.565426170468187e-05,
      "loss": 0.0911,
      "step": 4109
    },
    {
      "epoch": 8.22,
      "grad_norm": 0.3283676207065582,
      "learning_rate": 3.5614245698279317e-05,
      "loss": 0.0791,
      "step": 4110
    },
    {
      "epoch": 8.222,
      "grad_norm": 0.5576474666595459,
      "learning_rate": 3.5574229691876754e-05,
      "loss": 0.1016,
      "step": 4111
    },
    {
      "epoch": 8.224,
      "grad_norm": 0.5264884233474731,
      "learning_rate": 3.553421368547419e-05,
      "loss": 0.099,
      "step": 4112
    },
    {
      "epoch": 8.226,
      "grad_norm": 0.6567794680595398,
      "learning_rate": 3.549419767907163e-05,
      "loss": 0.0779,
      "step": 4113
    },
    {
      "epoch": 8.228,
      "grad_norm": 0.4553880989551544,
      "learning_rate": 3.5454181672669065e-05,
      "loss": 0.1528,
      "step": 4114
    },
    {
      "epoch": 8.23,
      "grad_norm": 0.4069265127182007,
      "learning_rate": 3.541416566626651e-05,
      "loss": 0.1126,
      "step": 4115
    },
    {
      "epoch": 8.232,
      "grad_norm": 0.6121687889099121,
      "learning_rate": 3.5374149659863946e-05,
      "loss": 0.1293,
      "step": 4116
    },
    {
      "epoch": 8.234,
      "grad_norm": 0.4035573899745941,
      "learning_rate": 3.533413365346139e-05,
      "loss": 0.1046,
      "step": 4117
    },
    {
      "epoch": 8.236,
      "grad_norm": 0.565243661403656,
      "learning_rate": 3.529411764705883e-05,
      "loss": 0.1297,
      "step": 4118
    },
    {
      "epoch": 8.238,
      "grad_norm": 0.6282405257225037,
      "learning_rate": 3.5254101640656264e-05,
      "loss": 0.1085,
      "step": 4119
    },
    {
      "epoch": 8.24,
      "grad_norm": 0.38230133056640625,
      "learning_rate": 3.52140856342537e-05,
      "loss": 0.0944,
      "step": 4120
    },
    {
      "epoch": 8.242,
      "grad_norm": 0.4374709725379944,
      "learning_rate": 3.517406962785114e-05,
      "loss": 0.1109,
      "step": 4121
    },
    {
      "epoch": 8.244,
      "grad_norm": 0.45875638723373413,
      "learning_rate": 3.513405362144858e-05,
      "loss": 0.1137,
      "step": 4122
    },
    {
      "epoch": 8.246,
      "grad_norm": 0.5319849848747253,
      "learning_rate": 3.509403761504602e-05,
      "loss": 0.1293,
      "step": 4123
    },
    {
      "epoch": 8.248,
      "grad_norm": 0.495947003364563,
      "learning_rate": 3.5054021608643464e-05,
      "loss": 0.1398,
      "step": 4124
    },
    {
      "epoch": 8.25,
      "grad_norm": 0.45291948318481445,
      "learning_rate": 3.5014005602240894e-05,
      "loss": 0.118,
      "step": 4125
    },
    {
      "epoch": 8.252,
      "grad_norm": 0.6006364822387695,
      "learning_rate": 3.497398959583834e-05,
      "loss": 0.1418,
      "step": 4126
    },
    {
      "epoch": 8.254,
      "grad_norm": 0.4397028684616089,
      "learning_rate": 3.4933973589435775e-05,
      "loss": 0.0824,
      "step": 4127
    },
    {
      "epoch": 8.256,
      "grad_norm": 0.4035651981830597,
      "learning_rate": 3.489395758303321e-05,
      "loss": 0.0842,
      "step": 4128
    },
    {
      "epoch": 8.258,
      "grad_norm": 0.441082239151001,
      "learning_rate": 3.4853941576630656e-05,
      "loss": 0.1235,
      "step": 4129
    },
    {
      "epoch": 8.26,
      "grad_norm": 0.2966970205307007,
      "learning_rate": 3.481392557022809e-05,
      "loss": 0.079,
      "step": 4130
    },
    {
      "epoch": 8.262,
      "grad_norm": 0.36494025588035583,
      "learning_rate": 3.477390956382553e-05,
      "loss": 0.0964,
      "step": 4131
    },
    {
      "epoch": 8.264,
      "grad_norm": 0.41990283131599426,
      "learning_rate": 3.473389355742297e-05,
      "loss": 0.0983,
      "step": 4132
    },
    {
      "epoch": 8.266,
      "grad_norm": 1.1130644083023071,
      "learning_rate": 3.469387755102041e-05,
      "loss": 0.1137,
      "step": 4133
    },
    {
      "epoch": 8.268,
      "grad_norm": 0.4152553081512451,
      "learning_rate": 3.465386154461785e-05,
      "loss": 0.1046,
      "step": 4134
    },
    {
      "epoch": 8.27,
      "grad_norm": 0.4115944504737854,
      "learning_rate": 3.461384553821529e-05,
      "loss": 0.1245,
      "step": 4135
    },
    {
      "epoch": 8.272,
      "grad_norm": 0.5513081550598145,
      "learning_rate": 3.457382953181273e-05,
      "loss": 0.1293,
      "step": 4136
    },
    {
      "epoch": 8.274000000000001,
      "grad_norm": 0.44338154792785645,
      "learning_rate": 3.453381352541017e-05,
      "loss": 0.1015,
      "step": 4137
    },
    {
      "epoch": 8.276,
      "grad_norm": 0.5930676460266113,
      "learning_rate": 3.4493797519007604e-05,
      "loss": 0.1484,
      "step": 4138
    },
    {
      "epoch": 8.278,
      "grad_norm": 0.5870168209075928,
      "learning_rate": 3.445378151260504e-05,
      "loss": 0.1176,
      "step": 4139
    },
    {
      "epoch": 8.28,
      "grad_norm": 0.7075915336608887,
      "learning_rate": 3.4413765506202485e-05,
      "loss": 0.0968,
      "step": 4140
    },
    {
      "epoch": 8.282,
      "grad_norm": 0.43397021293640137,
      "learning_rate": 3.437374949979992e-05,
      "loss": 0.1019,
      "step": 4141
    },
    {
      "epoch": 8.284,
      "grad_norm": 0.46899139881134033,
      "learning_rate": 3.4333733493397366e-05,
      "loss": 0.111,
      "step": 4142
    },
    {
      "epoch": 8.286,
      "grad_norm": 0.42189809679985046,
      "learning_rate": 3.4293717486994796e-05,
      "loss": 0.0891,
      "step": 4143
    },
    {
      "epoch": 8.288,
      "grad_norm": 0.5513610243797302,
      "learning_rate": 3.425370148059224e-05,
      "loss": 0.0996,
      "step": 4144
    },
    {
      "epoch": 8.29,
      "grad_norm": 0.5075617432594299,
      "learning_rate": 3.421368547418968e-05,
      "loss": 0.1206,
      "step": 4145
    },
    {
      "epoch": 8.292,
      "grad_norm": 0.37912771105766296,
      "learning_rate": 3.4173669467787114e-05,
      "loss": 0.1031,
      "step": 4146
    },
    {
      "epoch": 8.294,
      "grad_norm": 0.4767104387283325,
      "learning_rate": 3.413365346138456e-05,
      "loss": 0.096,
      "step": 4147
    },
    {
      "epoch": 8.296,
      "grad_norm": 0.4778522253036499,
      "learning_rate": 3.4093637454981995e-05,
      "loss": 0.114,
      "step": 4148
    },
    {
      "epoch": 8.298,
      "grad_norm": 0.4099927544593811,
      "learning_rate": 3.405362144857943e-05,
      "loss": 0.0944,
      "step": 4149
    },
    {
      "epoch": 8.3,
      "grad_norm": 0.6213188767433167,
      "learning_rate": 3.401360544217687e-05,
      "loss": 0.139,
      "step": 4150
    },
    {
      "epoch": 8.302,
      "grad_norm": 0.4396991729736328,
      "learning_rate": 3.3973589435774314e-05,
      "loss": 0.1038,
      "step": 4151
    },
    {
      "epoch": 8.304,
      "grad_norm": 0.4439414441585541,
      "learning_rate": 3.393357342937175e-05,
      "loss": 0.1035,
      "step": 4152
    },
    {
      "epoch": 8.306,
      "grad_norm": 0.437791645526886,
      "learning_rate": 3.389355742296919e-05,
      "loss": 0.119,
      "step": 4153
    },
    {
      "epoch": 8.308,
      "grad_norm": 0.4665718674659729,
      "learning_rate": 3.385354141656663e-05,
      "loss": 0.1101,
      "step": 4154
    },
    {
      "epoch": 8.31,
      "grad_norm": 0.4896693527698517,
      "learning_rate": 3.381352541016406e-05,
      "loss": 0.1081,
      "step": 4155
    },
    {
      "epoch": 8.312,
      "grad_norm": 0.4219852685928345,
      "learning_rate": 3.3773509403761506e-05,
      "loss": 0.1064,
      "step": 4156
    },
    {
      "epoch": 8.314,
      "grad_norm": 0.33557483553886414,
      "learning_rate": 3.373349339735894e-05,
      "loss": 0.1028,
      "step": 4157
    },
    {
      "epoch": 8.316,
      "grad_norm": 0.592337429523468,
      "learning_rate": 3.369347739095639e-05,
      "loss": 0.0967,
      "step": 4158
    },
    {
      "epoch": 8.318,
      "grad_norm": 0.6327635645866394,
      "learning_rate": 3.3653461384553824e-05,
      "loss": 0.1174,
      "step": 4159
    },
    {
      "epoch": 8.32,
      "grad_norm": 0.49292680621147156,
      "learning_rate": 3.361344537815127e-05,
      "loss": 0.1399,
      "step": 4160
    },
    {
      "epoch": 8.322,
      "grad_norm": 0.3425522446632385,
      "learning_rate": 3.35734293717487e-05,
      "loss": 0.101,
      "step": 4161
    },
    {
      "epoch": 8.324,
      "grad_norm": 0.46725234389305115,
      "learning_rate": 3.3533413365346136e-05,
      "loss": 0.1127,
      "step": 4162
    },
    {
      "epoch": 8.326,
      "grad_norm": 0.5406239032745361,
      "learning_rate": 3.349339735894358e-05,
      "loss": 0.1021,
      "step": 4163
    },
    {
      "epoch": 8.328,
      "grad_norm": 0.5121909379959106,
      "learning_rate": 3.345338135254102e-05,
      "loss": 0.1089,
      "step": 4164
    },
    {
      "epoch": 8.33,
      "grad_norm": 0.39635488390922546,
      "learning_rate": 3.341336534613846e-05,
      "loss": 0.0928,
      "step": 4165
    },
    {
      "epoch": 8.332,
      "grad_norm": 0.40945327281951904,
      "learning_rate": 3.33733493397359e-05,
      "loss": 0.0901,
      "step": 4166
    },
    {
      "epoch": 8.334,
      "grad_norm": 0.4002663791179657,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.1005,
      "step": 4167
    },
    {
      "epoch": 8.336,
      "grad_norm": 0.4079776704311371,
      "learning_rate": 3.329331732693077e-05,
      "loss": 0.1104,
      "step": 4168
    },
    {
      "epoch": 8.338,
      "grad_norm": 0.353972464799881,
      "learning_rate": 3.3253301320528216e-05,
      "loss": 0.0981,
      "step": 4169
    },
    {
      "epoch": 8.34,
      "grad_norm": 0.36170023679733276,
      "learning_rate": 3.321328531412565e-05,
      "loss": 0.0952,
      "step": 4170
    },
    {
      "epoch": 8.342,
      "grad_norm": 0.6729292869567871,
      "learning_rate": 3.317326930772309e-05,
      "loss": 0.1279,
      "step": 4171
    },
    {
      "epoch": 8.344,
      "grad_norm": 0.42019975185394287,
      "learning_rate": 3.313325330132053e-05,
      "loss": 0.1065,
      "step": 4172
    },
    {
      "epoch": 8.346,
      "grad_norm": 0.6588832139968872,
      "learning_rate": 3.3093237294917965e-05,
      "loss": 0.0996,
      "step": 4173
    },
    {
      "epoch": 8.348,
      "grad_norm": 0.44441941380500793,
      "learning_rate": 3.305322128851541e-05,
      "loss": 0.0791,
      "step": 4174
    },
    {
      "epoch": 8.35,
      "grad_norm": 0.4708944261074066,
      "learning_rate": 3.3013205282112846e-05,
      "loss": 0.1607,
      "step": 4175
    },
    {
      "epoch": 8.352,
      "grad_norm": 0.383065789937973,
      "learning_rate": 3.297318927571029e-05,
      "loss": 0.112,
      "step": 4176
    },
    {
      "epoch": 8.354,
      "grad_norm": 0.5078157782554626,
      "learning_rate": 3.2933173269307727e-05,
      "loss": 0.1048,
      "step": 4177
    },
    {
      "epoch": 8.356,
      "grad_norm": 0.7513482570648193,
      "learning_rate": 3.2893157262905164e-05,
      "loss": 0.1382,
      "step": 4178
    },
    {
      "epoch": 8.358,
      "grad_norm": 0.5805079340934753,
      "learning_rate": 3.28531412565026e-05,
      "loss": 0.1146,
      "step": 4179
    },
    {
      "epoch": 8.36,
      "grad_norm": 0.5551007390022278,
      "learning_rate": 3.281312525010004e-05,
      "loss": 0.1133,
      "step": 4180
    },
    {
      "epoch": 8.362,
      "grad_norm": 0.36776959896087646,
      "learning_rate": 3.277310924369748e-05,
      "loss": 0.097,
      "step": 4181
    },
    {
      "epoch": 8.364,
      "grad_norm": 0.36619168519973755,
      "learning_rate": 3.273309323729492e-05,
      "loss": 0.1015,
      "step": 4182
    },
    {
      "epoch": 8.366,
      "grad_norm": 0.31297916173934937,
      "learning_rate": 3.269307723089236e-05,
      "loss": 0.0714,
      "step": 4183
    },
    {
      "epoch": 8.368,
      "grad_norm": 0.5400555729866028,
      "learning_rate": 3.265306122448979e-05,
      "loss": 0.1413,
      "step": 4184
    },
    {
      "epoch": 8.37,
      "grad_norm": 0.4557523727416992,
      "learning_rate": 3.261304521808724e-05,
      "loss": 0.0821,
      "step": 4185
    },
    {
      "epoch": 8.372,
      "grad_norm": 0.27620506286621094,
      "learning_rate": 3.2573029211684674e-05,
      "loss": 0.093,
      "step": 4186
    },
    {
      "epoch": 8.374,
      "grad_norm": 0.5693125128746033,
      "learning_rate": 3.253301320528211e-05,
      "loss": 0.1353,
      "step": 4187
    },
    {
      "epoch": 8.376,
      "grad_norm": 0.3736360967159271,
      "learning_rate": 3.2492997198879555e-05,
      "loss": 0.0992,
      "step": 4188
    },
    {
      "epoch": 8.378,
      "grad_norm": 0.39580973982810974,
      "learning_rate": 3.245298119247699e-05,
      "loss": 0.0959,
      "step": 4189
    },
    {
      "epoch": 8.38,
      "grad_norm": 0.5213382244110107,
      "learning_rate": 3.241296518607443e-05,
      "loss": 0.1049,
      "step": 4190
    },
    {
      "epoch": 8.382,
      "grad_norm": 0.5697663426399231,
      "learning_rate": 3.237294917967187e-05,
      "loss": 0.1167,
      "step": 4191
    },
    {
      "epoch": 8.384,
      "grad_norm": 0.3443889021873474,
      "learning_rate": 3.233293317326931e-05,
      "loss": 0.088,
      "step": 4192
    },
    {
      "epoch": 8.386,
      "grad_norm": 0.44020476937294006,
      "learning_rate": 3.229291716686675e-05,
      "loss": 0.1154,
      "step": 4193
    },
    {
      "epoch": 8.388,
      "grad_norm": 0.5364933013916016,
      "learning_rate": 3.225290116046419e-05,
      "loss": 0.1181,
      "step": 4194
    },
    {
      "epoch": 8.39,
      "grad_norm": 0.6879401206970215,
      "learning_rate": 3.221288515406163e-05,
      "loss": 0.1396,
      "step": 4195
    },
    {
      "epoch": 8.392,
      "grad_norm": 0.3859749436378479,
      "learning_rate": 3.2172869147659066e-05,
      "loss": 0.0934,
      "step": 4196
    },
    {
      "epoch": 8.394,
      "grad_norm": 0.5425480604171753,
      "learning_rate": 3.21328531412565e-05,
      "loss": 0.1032,
      "step": 4197
    },
    {
      "epoch": 8.396,
      "grad_norm": 0.5237037539482117,
      "learning_rate": 3.209283713485394e-05,
      "loss": 0.1136,
      "step": 4198
    },
    {
      "epoch": 8.398,
      "grad_norm": 0.46084827184677124,
      "learning_rate": 3.2052821128451384e-05,
      "loss": 0.1216,
      "step": 4199
    },
    {
      "epoch": 8.4,
      "grad_norm": 0.46569228172302246,
      "learning_rate": 3.201280512204882e-05,
      "loss": 0.1186,
      "step": 4200
    },
    {
      "epoch": 8.402,
      "grad_norm": 0.4126104414463043,
      "learning_rate": 3.1972789115646265e-05,
      "loss": 0.1125,
      "step": 4201
    },
    {
      "epoch": 8.404,
      "grad_norm": 0.52143794298172,
      "learning_rate": 3.1932773109243696e-05,
      "loss": 0.1097,
      "step": 4202
    },
    {
      "epoch": 8.406,
      "grad_norm": 0.3875996470451355,
      "learning_rate": 3.189275710284114e-05,
      "loss": 0.1055,
      "step": 4203
    },
    {
      "epoch": 8.408,
      "grad_norm": 0.5030147433280945,
      "learning_rate": 3.185274109643858e-05,
      "loss": 0.1217,
      "step": 4204
    },
    {
      "epoch": 8.41,
      "grad_norm": 0.45885127782821655,
      "learning_rate": 3.1812725090036014e-05,
      "loss": 0.0987,
      "step": 4205
    },
    {
      "epoch": 8.412,
      "grad_norm": 0.45027899742126465,
      "learning_rate": 3.177270908363346e-05,
      "loss": 0.1106,
      "step": 4206
    },
    {
      "epoch": 8.414,
      "grad_norm": 0.38682064414024353,
      "learning_rate": 3.1732693077230895e-05,
      "loss": 0.0944,
      "step": 4207
    },
    {
      "epoch": 8.416,
      "grad_norm": 0.42369958758354187,
      "learning_rate": 3.169267707082833e-05,
      "loss": 0.0967,
      "step": 4208
    },
    {
      "epoch": 8.418,
      "grad_norm": 0.43004247546195984,
      "learning_rate": 3.165266106442577e-05,
      "loss": 0.1209,
      "step": 4209
    },
    {
      "epoch": 8.42,
      "grad_norm": 0.404944509267807,
      "learning_rate": 3.161264505802321e-05,
      "loss": 0.1115,
      "step": 4210
    },
    {
      "epoch": 8.422,
      "grad_norm": 0.7195178270339966,
      "learning_rate": 3.157262905162065e-05,
      "loss": 0.1308,
      "step": 4211
    },
    {
      "epoch": 8.424,
      "grad_norm": 0.484889417886734,
      "learning_rate": 3.153261304521809e-05,
      "loss": 0.0958,
      "step": 4212
    },
    {
      "epoch": 8.426,
      "grad_norm": 0.6941999793052673,
      "learning_rate": 3.149259703881553e-05,
      "loss": 0.1323,
      "step": 4213
    },
    {
      "epoch": 8.428,
      "grad_norm": 0.4403989017009735,
      "learning_rate": 3.145258103241296e-05,
      "loss": 0.0783,
      "step": 4214
    },
    {
      "epoch": 8.43,
      "grad_norm": 0.38627758622169495,
      "learning_rate": 3.1412565026010406e-05,
      "loss": 0.0904,
      "step": 4215
    },
    {
      "epoch": 8.432,
      "grad_norm": 0.5100911259651184,
      "learning_rate": 3.137254901960784e-05,
      "loss": 0.1197,
      "step": 4216
    },
    {
      "epoch": 8.434,
      "grad_norm": 0.3498448431491852,
      "learning_rate": 3.1332533013205287e-05,
      "loss": 0.1024,
      "step": 4217
    },
    {
      "epoch": 8.436,
      "grad_norm": 0.44431090354919434,
      "learning_rate": 3.1292517006802724e-05,
      "loss": 0.1157,
      "step": 4218
    },
    {
      "epoch": 8.438,
      "grad_norm": 0.45539987087249756,
      "learning_rate": 3.125250100040016e-05,
      "loss": 0.1102,
      "step": 4219
    },
    {
      "epoch": 8.44,
      "grad_norm": 0.5411505699157715,
      "learning_rate": 3.12124849939976e-05,
      "loss": 0.1379,
      "step": 4220
    },
    {
      "epoch": 8.442,
      "grad_norm": 0.6287105679512024,
      "learning_rate": 3.1172468987595035e-05,
      "loss": 0.1097,
      "step": 4221
    },
    {
      "epoch": 8.444,
      "grad_norm": 0.38348904252052307,
      "learning_rate": 3.113245298119248e-05,
      "loss": 0.0841,
      "step": 4222
    },
    {
      "epoch": 8.446,
      "grad_norm": 0.4554559290409088,
      "learning_rate": 3.1092436974789916e-05,
      "loss": 0.126,
      "step": 4223
    },
    {
      "epoch": 8.448,
      "grad_norm": 0.4212051331996918,
      "learning_rate": 3.105242096838736e-05,
      "loss": 0.1022,
      "step": 4224
    },
    {
      "epoch": 8.45,
      "grad_norm": 0.5790700912475586,
      "learning_rate": 3.10124049619848e-05,
      "loss": 0.0854,
      "step": 4225
    },
    {
      "epoch": 8.452,
      "grad_norm": 0.379417359828949,
      "learning_rate": 3.0972388955582234e-05,
      "loss": 0.0841,
      "step": 4226
    },
    {
      "epoch": 8.454,
      "grad_norm": 0.5131919384002686,
      "learning_rate": 3.093237294917967e-05,
      "loss": 0.1028,
      "step": 4227
    },
    {
      "epoch": 8.456,
      "grad_norm": 0.44476139545440674,
      "learning_rate": 3.0892356942777115e-05,
      "loss": 0.1299,
      "step": 4228
    },
    {
      "epoch": 8.458,
      "grad_norm": 0.31663647294044495,
      "learning_rate": 3.085234093637455e-05,
      "loss": 0.0856,
      "step": 4229
    },
    {
      "epoch": 8.46,
      "grad_norm": 0.7794100046157837,
      "learning_rate": 3.081232492997199e-05,
      "loss": 0.1268,
      "step": 4230
    },
    {
      "epoch": 8.462,
      "grad_norm": 0.45569685101509094,
      "learning_rate": 3.0772308923569434e-05,
      "loss": 0.0964,
      "step": 4231
    },
    {
      "epoch": 8.464,
      "grad_norm": 0.5026775598526001,
      "learning_rate": 3.0732292917166864e-05,
      "loss": 0.1194,
      "step": 4232
    },
    {
      "epoch": 8.466,
      "grad_norm": 0.45038384199142456,
      "learning_rate": 3.069227691076431e-05,
      "loss": 0.1203,
      "step": 4233
    },
    {
      "epoch": 8.468,
      "grad_norm": 0.4402273893356323,
      "learning_rate": 3.0652260904361745e-05,
      "loss": 0.1028,
      "step": 4234
    },
    {
      "epoch": 8.47,
      "grad_norm": 0.509453296661377,
      "learning_rate": 3.061224489795919e-05,
      "loss": 0.1244,
      "step": 4235
    },
    {
      "epoch": 8.472,
      "grad_norm": 0.30271533131599426,
      "learning_rate": 3.0572228891556626e-05,
      "loss": 0.0739,
      "step": 4236
    },
    {
      "epoch": 8.474,
      "grad_norm": 0.5679349303245544,
      "learning_rate": 3.053221288515406e-05,
      "loss": 0.1148,
      "step": 4237
    },
    {
      "epoch": 8.475999999999999,
      "grad_norm": 0.5225644111633301,
      "learning_rate": 3.04921968787515e-05,
      "loss": 0.0969,
      "step": 4238
    },
    {
      "epoch": 8.478,
      "grad_norm": 0.32767677307128906,
      "learning_rate": 3.045218087234894e-05,
      "loss": 0.0718,
      "step": 4239
    },
    {
      "epoch": 8.48,
      "grad_norm": 0.4371460974216461,
      "learning_rate": 3.041216486594638e-05,
      "loss": 0.0985,
      "step": 4240
    },
    {
      "epoch": 8.482,
      "grad_norm": 0.49269869923591614,
      "learning_rate": 3.037214885954382e-05,
      "loss": 0.1021,
      "step": 4241
    },
    {
      "epoch": 8.484,
      "grad_norm": 0.4059157073497772,
      "learning_rate": 3.033213285314126e-05,
      "loss": 0.1052,
      "step": 4242
    },
    {
      "epoch": 8.486,
      "grad_norm": 0.5063432455062866,
      "learning_rate": 3.02921168467387e-05,
      "loss": 0.1116,
      "step": 4243
    },
    {
      "epoch": 8.488,
      "grad_norm": 0.5334231853485107,
      "learning_rate": 3.0252100840336133e-05,
      "loss": 0.1266,
      "step": 4244
    },
    {
      "epoch": 8.49,
      "grad_norm": 0.5413596630096436,
      "learning_rate": 3.0212084833933574e-05,
      "loss": 0.1436,
      "step": 4245
    },
    {
      "epoch": 8.492,
      "grad_norm": 0.46235865354537964,
      "learning_rate": 3.0172068827531014e-05,
      "loss": 0.1097,
      "step": 4246
    },
    {
      "epoch": 8.494,
      "grad_norm": 0.3884005844593048,
      "learning_rate": 3.0132052821128455e-05,
      "loss": 0.1004,
      "step": 4247
    },
    {
      "epoch": 8.496,
      "grad_norm": 0.5273085832595825,
      "learning_rate": 3.0092036814725892e-05,
      "loss": 0.0981,
      "step": 4248
    },
    {
      "epoch": 8.498,
      "grad_norm": 0.4879322052001953,
      "learning_rate": 3.0052020808323332e-05,
      "loss": 0.1307,
      "step": 4249
    },
    {
      "epoch": 8.5,
      "grad_norm": 0.38027283549308777,
      "learning_rate": 3.0012004801920766e-05,
      "loss": 0.0934,
      "step": 4250
    },
    {
      "epoch": 8.502,
      "grad_norm": 0.5368924140930176,
      "learning_rate": 2.9971988795518207e-05,
      "loss": 0.1329,
      "step": 4251
    },
    {
      "epoch": 8.504,
      "grad_norm": 0.35786423087120056,
      "learning_rate": 2.9931972789115647e-05,
      "loss": 0.0923,
      "step": 4252
    },
    {
      "epoch": 8.506,
      "grad_norm": 0.48789918422698975,
      "learning_rate": 2.9891956782713088e-05,
      "loss": 0.1284,
      "step": 4253
    },
    {
      "epoch": 8.508,
      "grad_norm": 0.38922083377838135,
      "learning_rate": 2.985194077631053e-05,
      "loss": 0.0946,
      "step": 4254
    },
    {
      "epoch": 8.51,
      "grad_norm": 0.477927565574646,
      "learning_rate": 2.981192476990797e-05,
      "loss": 0.1213,
      "step": 4255
    },
    {
      "epoch": 8.512,
      "grad_norm": 0.5826550722122192,
      "learning_rate": 2.9771908763505403e-05,
      "loss": 0.1102,
      "step": 4256
    },
    {
      "epoch": 8.514,
      "grad_norm": 0.71830153465271,
      "learning_rate": 2.9731892757102843e-05,
      "loss": 0.1331,
      "step": 4257
    },
    {
      "epoch": 8.516,
      "grad_norm": 0.48716986179351807,
      "learning_rate": 2.969187675070028e-05,
      "loss": 0.1373,
      "step": 4258
    },
    {
      "epoch": 8.518,
      "grad_norm": 0.45197612047195435,
      "learning_rate": 2.965186074429772e-05,
      "loss": 0.0981,
      "step": 4259
    },
    {
      "epoch": 8.52,
      "grad_norm": 0.8336470723152161,
      "learning_rate": 2.961184473789516e-05,
      "loss": 0.1011,
      "step": 4260
    },
    {
      "epoch": 8.522,
      "grad_norm": 0.400319367647171,
      "learning_rate": 2.9571828731492595e-05,
      "loss": 0.1017,
      "step": 4261
    },
    {
      "epoch": 8.524000000000001,
      "grad_norm": 0.5760491490364075,
      "learning_rate": 2.9531812725090036e-05,
      "loss": 0.1314,
      "step": 4262
    },
    {
      "epoch": 8.526,
      "grad_norm": 0.43268021941185,
      "learning_rate": 2.9491796718687476e-05,
      "loss": 0.1006,
      "step": 4263
    },
    {
      "epoch": 8.528,
      "grad_norm": 0.4054381549358368,
      "learning_rate": 2.9451780712284917e-05,
      "loss": 0.1039,
      "step": 4264
    },
    {
      "epoch": 8.53,
      "grad_norm": 0.45835012197494507,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 0.1149,
      "step": 4265
    },
    {
      "epoch": 8.532,
      "grad_norm": 0.42320409417152405,
      "learning_rate": 2.9371748699479794e-05,
      "loss": 0.0797,
      "step": 4266
    },
    {
      "epoch": 8.534,
      "grad_norm": 0.5261272192001343,
      "learning_rate": 2.9331732693077228e-05,
      "loss": 0.1233,
      "step": 4267
    },
    {
      "epoch": 8.536,
      "grad_norm": 0.30956244468688965,
      "learning_rate": 2.929171668667467e-05,
      "loss": 0.0809,
      "step": 4268
    },
    {
      "epoch": 8.538,
      "grad_norm": 0.5070328116416931,
      "learning_rate": 2.925170068027211e-05,
      "loss": 0.1013,
      "step": 4269
    },
    {
      "epoch": 8.54,
      "grad_norm": 0.45230668783187866,
      "learning_rate": 2.921168467386955e-05,
      "loss": 0.0952,
      "step": 4270
    },
    {
      "epoch": 8.542,
      "grad_norm": 0.4560154676437378,
      "learning_rate": 2.917166866746699e-05,
      "loss": 0.0934,
      "step": 4271
    },
    {
      "epoch": 8.544,
      "grad_norm": 0.4932778477668762,
      "learning_rate": 2.913165266106443e-05,
      "loss": 0.1058,
      "step": 4272
    },
    {
      "epoch": 8.546,
      "grad_norm": 0.48229625821113586,
      "learning_rate": 2.9091636654661864e-05,
      "loss": 0.0958,
      "step": 4273
    },
    {
      "epoch": 8.548,
      "grad_norm": 0.4108912944793701,
      "learning_rate": 2.9051620648259305e-05,
      "loss": 0.1165,
      "step": 4274
    },
    {
      "epoch": 8.55,
      "grad_norm": 0.4328085780143738,
      "learning_rate": 2.9011604641856742e-05,
      "loss": 0.0744,
      "step": 4275
    },
    {
      "epoch": 8.552,
      "grad_norm": 0.6401262879371643,
      "learning_rate": 2.8971588635454183e-05,
      "loss": 0.1302,
      "step": 4276
    },
    {
      "epoch": 8.554,
      "grad_norm": 0.4606500267982483,
      "learning_rate": 2.8931572629051623e-05,
      "loss": 0.1058,
      "step": 4277
    },
    {
      "epoch": 8.556000000000001,
      "grad_norm": 0.3780839443206787,
      "learning_rate": 2.8891556622649064e-05,
      "loss": 0.1232,
      "step": 4278
    },
    {
      "epoch": 8.558,
      "grad_norm": 0.4394082725048065,
      "learning_rate": 2.8851540616246497e-05,
      "loss": 0.1038,
      "step": 4279
    },
    {
      "epoch": 8.56,
      "grad_norm": 0.45094043016433716,
      "learning_rate": 2.8811524609843938e-05,
      "loss": 0.1146,
      "step": 4280
    },
    {
      "epoch": 8.562,
      "grad_norm": 0.482793390750885,
      "learning_rate": 2.877150860344138e-05,
      "loss": 0.1041,
      "step": 4281
    },
    {
      "epoch": 8.564,
      "grad_norm": 0.41067835688591003,
      "learning_rate": 2.8731492597038816e-05,
      "loss": 0.0891,
      "step": 4282
    },
    {
      "epoch": 8.566,
      "grad_norm": 0.4412156641483307,
      "learning_rate": 2.8691476590636256e-05,
      "loss": 0.119,
      "step": 4283
    },
    {
      "epoch": 8.568,
      "grad_norm": 0.3425866365432739,
      "learning_rate": 2.8651460584233697e-05,
      "loss": 0.0848,
      "step": 4284
    },
    {
      "epoch": 8.57,
      "grad_norm": 1.5003345012664795,
      "learning_rate": 2.861144457783113e-05,
      "loss": 0.117,
      "step": 4285
    },
    {
      "epoch": 8.572,
      "grad_norm": 0.5321928858757019,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.1343,
      "step": 4286
    },
    {
      "epoch": 8.574,
      "grad_norm": 0.396127313375473,
      "learning_rate": 2.853141256502601e-05,
      "loss": 0.0945,
      "step": 4287
    },
    {
      "epoch": 8.576,
      "grad_norm": 0.5436384677886963,
      "learning_rate": 2.8491396558623452e-05,
      "loss": 0.1201,
      "step": 4288
    },
    {
      "epoch": 8.578,
      "grad_norm": 0.43802744150161743,
      "learning_rate": 2.8451380552220892e-05,
      "loss": 0.1079,
      "step": 4289
    },
    {
      "epoch": 8.58,
      "grad_norm": 0.4930124878883362,
      "learning_rate": 2.841136454581833e-05,
      "loss": 0.1255,
      "step": 4290
    },
    {
      "epoch": 8.582,
      "grad_norm": 0.5994726419448853,
      "learning_rate": 2.8371348539415767e-05,
      "loss": 0.1391,
      "step": 4291
    },
    {
      "epoch": 8.584,
      "grad_norm": 0.5317903161048889,
      "learning_rate": 2.8331332533013204e-05,
      "loss": 0.107,
      "step": 4292
    },
    {
      "epoch": 8.586,
      "grad_norm": 0.5619238018989563,
      "learning_rate": 2.8291316526610644e-05,
      "loss": 0.1055,
      "step": 4293
    },
    {
      "epoch": 8.588,
      "grad_norm": 0.50093013048172,
      "learning_rate": 2.8251300520208085e-05,
      "loss": 0.1455,
      "step": 4294
    },
    {
      "epoch": 8.59,
      "grad_norm": 0.5084438323974609,
      "learning_rate": 2.8211284513805525e-05,
      "loss": 0.1061,
      "step": 4295
    },
    {
      "epoch": 8.592,
      "grad_norm": 0.47319409251213074,
      "learning_rate": 2.8171268507402966e-05,
      "loss": 0.0899,
      "step": 4296
    },
    {
      "epoch": 8.594,
      "grad_norm": 0.3491595387458801,
      "learning_rate": 2.81312525010004e-05,
      "loss": 0.0882,
      "step": 4297
    },
    {
      "epoch": 8.596,
      "grad_norm": 0.6126877665519714,
      "learning_rate": 2.809123649459784e-05,
      "loss": 0.0905,
      "step": 4298
    },
    {
      "epoch": 8.598,
      "grad_norm": 0.5053337812423706,
      "learning_rate": 2.8051220488195277e-05,
      "loss": 0.1367,
      "step": 4299
    },
    {
      "epoch": 8.6,
      "grad_norm": 0.6556789875030518,
      "learning_rate": 2.8011204481792718e-05,
      "loss": 0.1188,
      "step": 4300
    },
    {
      "epoch": 8.602,
      "grad_norm": 0.3373563289642334,
      "learning_rate": 2.797118847539016e-05,
      "loss": 0.0844,
      "step": 4301
    },
    {
      "epoch": 8.604,
      "grad_norm": 0.47313085198402405,
      "learning_rate": 2.79311724689876e-05,
      "loss": 0.1047,
      "step": 4302
    },
    {
      "epoch": 8.606,
      "grad_norm": 0.46306008100509644,
      "learning_rate": 2.7891156462585033e-05,
      "loss": 0.1118,
      "step": 4303
    },
    {
      "epoch": 8.608,
      "grad_norm": 0.4347535967826843,
      "learning_rate": 2.7851140456182473e-05,
      "loss": 0.1008,
      "step": 4304
    },
    {
      "epoch": 8.61,
      "grad_norm": 0.5677775144577026,
      "learning_rate": 2.7811124449779914e-05,
      "loss": 0.1496,
      "step": 4305
    },
    {
      "epoch": 8.612,
      "grad_norm": 0.5192933082580566,
      "learning_rate": 2.7771108443377354e-05,
      "loss": 0.1202,
      "step": 4306
    },
    {
      "epoch": 8.614,
      "grad_norm": 0.4964066445827484,
      "learning_rate": 2.773109243697479e-05,
      "loss": 0.089,
      "step": 4307
    },
    {
      "epoch": 8.616,
      "grad_norm": 0.6179014444351196,
      "learning_rate": 2.7691076430572232e-05,
      "loss": 0.154,
      "step": 4308
    },
    {
      "epoch": 8.618,
      "grad_norm": 0.5284407138824463,
      "learning_rate": 2.7651060424169666e-05,
      "loss": 0.1295,
      "step": 4309
    },
    {
      "epoch": 8.62,
      "grad_norm": 0.5144823789596558,
      "learning_rate": 2.7611044417767106e-05,
      "loss": 0.1349,
      "step": 4310
    },
    {
      "epoch": 8.622,
      "grad_norm": 0.4994406998157501,
      "learning_rate": 2.7571028411364547e-05,
      "loss": 0.0859,
      "step": 4311
    },
    {
      "epoch": 8.624,
      "grad_norm": 0.6584689617156982,
      "learning_rate": 2.7531012404961987e-05,
      "loss": 0.1203,
      "step": 4312
    },
    {
      "epoch": 8.626,
      "grad_norm": 0.5739440321922302,
      "learning_rate": 2.7490996398559428e-05,
      "loss": 0.1276,
      "step": 4313
    },
    {
      "epoch": 8.628,
      "grad_norm": 0.4385263919830322,
      "learning_rate": 2.7450980392156865e-05,
      "loss": 0.1254,
      "step": 4314
    },
    {
      "epoch": 8.63,
      "grad_norm": 0.4456677734851837,
      "learning_rate": 2.7410964385754302e-05,
      "loss": 0.112,
      "step": 4315
    },
    {
      "epoch": 8.632,
      "grad_norm": 0.5269618630409241,
      "learning_rate": 2.737094837935174e-05,
      "loss": 0.1008,
      "step": 4316
    },
    {
      "epoch": 8.634,
      "grad_norm": 0.5023514032363892,
      "learning_rate": 2.733093237294918e-05,
      "loss": 0.1238,
      "step": 4317
    },
    {
      "epoch": 8.636,
      "grad_norm": 0.6111395359039307,
      "learning_rate": 2.729091636654662e-05,
      "loss": 0.1383,
      "step": 4318
    },
    {
      "epoch": 8.638,
      "grad_norm": 0.5637800693511963,
      "learning_rate": 2.725090036014406e-05,
      "loss": 0.1125,
      "step": 4319
    },
    {
      "epoch": 8.64,
      "grad_norm": 0.4932102859020233,
      "learning_rate": 2.72108843537415e-05,
      "loss": 0.1406,
      "step": 4320
    },
    {
      "epoch": 8.642,
      "grad_norm": 0.5570008158683777,
      "learning_rate": 2.7170868347338935e-05,
      "loss": 0.1431,
      "step": 4321
    },
    {
      "epoch": 8.644,
      "grad_norm": 0.38332146406173706,
      "learning_rate": 2.7130852340936375e-05,
      "loss": 0.0798,
      "step": 4322
    },
    {
      "epoch": 8.646,
      "grad_norm": 0.40916186571121216,
      "learning_rate": 2.7090836334533816e-05,
      "loss": 0.0996,
      "step": 4323
    },
    {
      "epoch": 8.648,
      "grad_norm": 0.39534127712249756,
      "learning_rate": 2.7050820328131253e-05,
      "loss": 0.0842,
      "step": 4324
    },
    {
      "epoch": 8.65,
      "grad_norm": 0.4714505672454834,
      "learning_rate": 2.7010804321728694e-05,
      "loss": 0.0869,
      "step": 4325
    },
    {
      "epoch": 8.652,
      "grad_norm": 0.46570488810539246,
      "learning_rate": 2.6970788315326134e-05,
      "loss": 0.1301,
      "step": 4326
    },
    {
      "epoch": 8.654,
      "grad_norm": 0.41413938999176025,
      "learning_rate": 2.6930772308923568e-05,
      "loss": 0.1016,
      "step": 4327
    },
    {
      "epoch": 8.656,
      "grad_norm": 0.5302191376686096,
      "learning_rate": 2.689075630252101e-05,
      "loss": 0.1427,
      "step": 4328
    },
    {
      "epoch": 8.658,
      "grad_norm": 0.32840695977211,
      "learning_rate": 2.685074029611845e-05,
      "loss": 0.108,
      "step": 4329
    },
    {
      "epoch": 8.66,
      "grad_norm": 0.41523465514183044,
      "learning_rate": 2.681072428971589e-05,
      "loss": 0.1017,
      "step": 4330
    },
    {
      "epoch": 8.662,
      "grad_norm": 0.3539276719093323,
      "learning_rate": 2.6770708283313327e-05,
      "loss": 0.0867,
      "step": 4331
    },
    {
      "epoch": 8.664,
      "grad_norm": 0.38835620880126953,
      "learning_rate": 2.6730692276910767e-05,
      "loss": 0.1104,
      "step": 4332
    },
    {
      "epoch": 8.666,
      "grad_norm": 0.45739591121673584,
      "learning_rate": 2.66906762705082e-05,
      "loss": 0.0864,
      "step": 4333
    },
    {
      "epoch": 8.668,
      "grad_norm": 0.3352396786212921,
      "learning_rate": 2.665066026410564e-05,
      "loss": 0.1048,
      "step": 4334
    },
    {
      "epoch": 8.67,
      "grad_norm": 0.4808627665042877,
      "learning_rate": 2.6610644257703082e-05,
      "loss": 0.1183,
      "step": 4335
    },
    {
      "epoch": 8.672,
      "grad_norm": 0.7072374820709229,
      "learning_rate": 2.6570628251300522e-05,
      "loss": 0.1496,
      "step": 4336
    },
    {
      "epoch": 8.674,
      "grad_norm": 0.30003947019577026,
      "learning_rate": 2.6530612244897963e-05,
      "loss": 0.0796,
      "step": 4337
    },
    {
      "epoch": 8.676,
      "grad_norm": 0.5264267921447754,
      "learning_rate": 2.6490596238495404e-05,
      "loss": 0.1483,
      "step": 4338
    },
    {
      "epoch": 8.678,
      "grad_norm": 0.41430407762527466,
      "learning_rate": 2.6450580232092837e-05,
      "loss": 0.1046,
      "step": 4339
    },
    {
      "epoch": 8.68,
      "grad_norm": 0.4741009473800659,
      "learning_rate": 2.6410564225690278e-05,
      "loss": 0.1393,
      "step": 4340
    },
    {
      "epoch": 8.682,
      "grad_norm": 0.33590182662010193,
      "learning_rate": 2.6370548219287715e-05,
      "loss": 0.0993,
      "step": 4341
    },
    {
      "epoch": 8.684,
      "grad_norm": 0.4098253846168518,
      "learning_rate": 2.6330532212885155e-05,
      "loss": 0.0979,
      "step": 4342
    },
    {
      "epoch": 8.686,
      "grad_norm": 0.5701387524604797,
      "learning_rate": 2.6290516206482596e-05,
      "loss": 0.1211,
      "step": 4343
    },
    {
      "epoch": 8.688,
      "grad_norm": 0.3951205611228943,
      "learning_rate": 2.6250500200080037e-05,
      "loss": 0.1086,
      "step": 4344
    },
    {
      "epoch": 8.69,
      "grad_norm": 0.3892943561077118,
      "learning_rate": 2.621048419367747e-05,
      "loss": 0.0796,
      "step": 4345
    },
    {
      "epoch": 8.692,
      "grad_norm": 0.559539794921875,
      "learning_rate": 2.617046818727491e-05,
      "loss": 0.1213,
      "step": 4346
    },
    {
      "epoch": 8.693999999999999,
      "grad_norm": 0.567331075668335,
      "learning_rate": 2.613045218087235e-05,
      "loss": 0.1239,
      "step": 4347
    },
    {
      "epoch": 8.696,
      "grad_norm": 0.5636032819747925,
      "learning_rate": 2.609043617446979e-05,
      "loss": 0.1158,
      "step": 4348
    },
    {
      "epoch": 8.698,
      "grad_norm": 0.4056744873523712,
      "learning_rate": 2.605042016806723e-05,
      "loss": 0.1097,
      "step": 4349
    },
    {
      "epoch": 8.7,
      "grad_norm": 0.5397958159446716,
      "learning_rate": 2.601040416166467e-05,
      "loss": 0.1337,
      "step": 4350
    },
    {
      "epoch": 8.702,
      "grad_norm": 0.37815406918525696,
      "learning_rate": 2.5970388155262103e-05,
      "loss": 0.1021,
      "step": 4351
    },
    {
      "epoch": 8.704,
      "grad_norm": 0.35722616314888,
      "learning_rate": 2.5930372148859544e-05,
      "loss": 0.0848,
      "step": 4352
    },
    {
      "epoch": 8.706,
      "grad_norm": 0.5762346386909485,
      "learning_rate": 2.5890356142456984e-05,
      "loss": 0.1381,
      "step": 4353
    },
    {
      "epoch": 8.708,
      "grad_norm": 0.4759502708911896,
      "learning_rate": 2.5850340136054425e-05,
      "loss": 0.1326,
      "step": 4354
    },
    {
      "epoch": 8.71,
      "grad_norm": 0.5182126760482788,
      "learning_rate": 2.5810324129651865e-05,
      "loss": 0.1057,
      "step": 4355
    },
    {
      "epoch": 8.712,
      "grad_norm": 0.4409542381763458,
      "learning_rate": 2.57703081232493e-05,
      "loss": 0.1094,
      "step": 4356
    },
    {
      "epoch": 8.714,
      "grad_norm": 0.8290045857429504,
      "learning_rate": 2.573029211684674e-05,
      "loss": 0.1321,
      "step": 4357
    },
    {
      "epoch": 8.716,
      "grad_norm": 0.32438936829566956,
      "learning_rate": 2.5690276110444177e-05,
      "loss": 0.0921,
      "step": 4358
    },
    {
      "epoch": 8.718,
      "grad_norm": 0.4181520342826843,
      "learning_rate": 2.5650260104041617e-05,
      "loss": 0.0935,
      "step": 4359
    },
    {
      "epoch": 8.72,
      "grad_norm": 0.43150749802589417,
      "learning_rate": 2.5610244097639058e-05,
      "loss": 0.1019,
      "step": 4360
    },
    {
      "epoch": 8.722,
      "grad_norm": 0.43936362862586975,
      "learning_rate": 2.5570228091236498e-05,
      "loss": 0.1097,
      "step": 4361
    },
    {
      "epoch": 8.724,
      "grad_norm": 0.47370854020118713,
      "learning_rate": 2.5530212084833932e-05,
      "loss": 0.1219,
      "step": 4362
    },
    {
      "epoch": 8.725999999999999,
      "grad_norm": 0.529634952545166,
      "learning_rate": 2.5490196078431373e-05,
      "loss": 0.1447,
      "step": 4363
    },
    {
      "epoch": 8.728,
      "grad_norm": 0.5147427916526794,
      "learning_rate": 2.5450180072028813e-05,
      "loss": 0.1127,
      "step": 4364
    },
    {
      "epoch": 8.73,
      "grad_norm": 0.5404065251350403,
      "learning_rate": 2.541016406562625e-05,
      "loss": 0.1413,
      "step": 4365
    },
    {
      "epoch": 8.732,
      "grad_norm": 0.42627838253974915,
      "learning_rate": 2.537014805922369e-05,
      "loss": 0.093,
      "step": 4366
    },
    {
      "epoch": 8.734,
      "grad_norm": 0.4582412540912628,
      "learning_rate": 2.533013205282113e-05,
      "loss": 0.1027,
      "step": 4367
    },
    {
      "epoch": 8.736,
      "grad_norm": 0.5610328912734985,
      "learning_rate": 2.5290116046418565e-05,
      "loss": 0.1064,
      "step": 4368
    },
    {
      "epoch": 8.738,
      "grad_norm": 0.5030485391616821,
      "learning_rate": 2.5250100040016006e-05,
      "loss": 0.1036,
      "step": 4369
    },
    {
      "epoch": 8.74,
      "grad_norm": 0.5254888534545898,
      "learning_rate": 2.5210084033613446e-05,
      "loss": 0.1025,
      "step": 4370
    },
    {
      "epoch": 8.742,
      "grad_norm": 0.5016798377037048,
      "learning_rate": 2.5170068027210887e-05,
      "loss": 0.111,
      "step": 4371
    },
    {
      "epoch": 8.744,
      "grad_norm": 0.7214571237564087,
      "learning_rate": 2.5130052020808327e-05,
      "loss": 0.155,
      "step": 4372
    },
    {
      "epoch": 8.746,
      "grad_norm": 0.5542469024658203,
      "learning_rate": 2.5090036014405764e-05,
      "loss": 0.1322,
      "step": 4373
    },
    {
      "epoch": 8.748,
      "grad_norm": 0.6659739017486572,
      "learning_rate": 2.50500200080032e-05,
      "loss": 0.1259,
      "step": 4374
    },
    {
      "epoch": 8.75,
      "grad_norm": 0.5306603908538818,
      "learning_rate": 2.501000400160064e-05,
      "loss": 0.1191,
      "step": 4375
    },
    {
      "epoch": 8.752,
      "grad_norm": 0.5443093180656433,
      "learning_rate": 2.496998799519808e-05,
      "loss": 0.1361,
      "step": 4376
    },
    {
      "epoch": 8.754,
      "grad_norm": 0.4320335388183594,
      "learning_rate": 2.492997198879552e-05,
      "loss": 0.0936,
      "step": 4377
    },
    {
      "epoch": 8.756,
      "grad_norm": 0.9186419248580933,
      "learning_rate": 2.488995598239296e-05,
      "loss": 0.1442,
      "step": 4378
    },
    {
      "epoch": 8.758,
      "grad_norm": 0.4344758093357086,
      "learning_rate": 2.4849939975990397e-05,
      "loss": 0.0858,
      "step": 4379
    },
    {
      "epoch": 8.76,
      "grad_norm": 0.5628881454467773,
      "learning_rate": 2.4809923969587838e-05,
      "loss": 0.1407,
      "step": 4380
    },
    {
      "epoch": 8.762,
      "grad_norm": 0.5238290429115295,
      "learning_rate": 2.4769907963185275e-05,
      "loss": 0.1399,
      "step": 4381
    },
    {
      "epoch": 8.764,
      "grad_norm": 0.5027657747268677,
      "learning_rate": 2.4729891956782712e-05,
      "loss": 0.112,
      "step": 4382
    },
    {
      "epoch": 8.766,
      "grad_norm": 0.47191762924194336,
      "learning_rate": 2.4689875950380153e-05,
      "loss": 0.1361,
      "step": 4383
    },
    {
      "epoch": 8.768,
      "grad_norm": 0.43182089924812317,
      "learning_rate": 2.4649859943977593e-05,
      "loss": 0.1185,
      "step": 4384
    },
    {
      "epoch": 8.77,
      "grad_norm": 0.3350708782672882,
      "learning_rate": 2.460984393757503e-05,
      "loss": 0.0738,
      "step": 4385
    },
    {
      "epoch": 8.772,
      "grad_norm": 0.30614665150642395,
      "learning_rate": 2.456982793117247e-05,
      "loss": 0.0877,
      "step": 4386
    },
    {
      "epoch": 8.774000000000001,
      "grad_norm": 0.5380082130432129,
      "learning_rate": 2.452981192476991e-05,
      "loss": 0.0956,
      "step": 4387
    },
    {
      "epoch": 8.776,
      "grad_norm": 0.6353188753128052,
      "learning_rate": 2.448979591836735e-05,
      "loss": 0.1109,
      "step": 4388
    },
    {
      "epoch": 8.778,
      "grad_norm": 0.6842931509017944,
      "learning_rate": 2.444977991196479e-05,
      "loss": 0.0991,
      "step": 4389
    },
    {
      "epoch": 8.78,
      "grad_norm": 0.5432853698730469,
      "learning_rate": 2.4409763905562226e-05,
      "loss": 0.1409,
      "step": 4390
    },
    {
      "epoch": 8.782,
      "grad_norm": 0.3940705955028534,
      "learning_rate": 2.4369747899159663e-05,
      "loss": 0.0907,
      "step": 4391
    },
    {
      "epoch": 8.784,
      "grad_norm": 0.7875823974609375,
      "learning_rate": 2.4329731892757104e-05,
      "loss": 0.1417,
      "step": 4392
    },
    {
      "epoch": 8.786,
      "grad_norm": 0.5717892050743103,
      "learning_rate": 2.4289715886354544e-05,
      "loss": 0.1657,
      "step": 4393
    },
    {
      "epoch": 8.788,
      "grad_norm": 0.4996457099914551,
      "learning_rate": 2.424969987995198e-05,
      "loss": 0.11,
      "step": 4394
    },
    {
      "epoch": 8.79,
      "grad_norm": 0.5022803544998169,
      "learning_rate": 2.4209683873549422e-05,
      "loss": 0.1203,
      "step": 4395
    },
    {
      "epoch": 8.792,
      "grad_norm": 0.4781288206577301,
      "learning_rate": 2.4169667867146862e-05,
      "loss": 0.1241,
      "step": 4396
    },
    {
      "epoch": 8.794,
      "grad_norm": 0.42354828119277954,
      "learning_rate": 2.41296518607443e-05,
      "loss": 0.0939,
      "step": 4397
    },
    {
      "epoch": 8.796,
      "grad_norm": 0.5145953893661499,
      "learning_rate": 2.4089635854341737e-05,
      "loss": 0.1202,
      "step": 4398
    },
    {
      "epoch": 8.798,
      "grad_norm": 0.6157521605491638,
      "learning_rate": 2.4049619847939174e-05,
      "loss": 0.0986,
      "step": 4399
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.3149678707122803,
      "learning_rate": 2.4009603841536614e-05,
      "loss": 0.0908,
      "step": 4400
    },
    {
      "epoch": 8.802,
      "grad_norm": 0.9512622952461243,
      "learning_rate": 2.3969587835134055e-05,
      "loss": 0.2818,
      "step": 4401
    },
    {
      "epoch": 8.804,
      "grad_norm": 0.5987499952316284,
      "learning_rate": 2.3929571828731492e-05,
      "loss": 0.1291,
      "step": 4402
    },
    {
      "epoch": 8.806000000000001,
      "grad_norm": 0.5623078942298889,
      "learning_rate": 2.3889555822328933e-05,
      "loss": 0.1291,
      "step": 4403
    },
    {
      "epoch": 8.808,
      "grad_norm": 0.3469023108482361,
      "learning_rate": 2.3849539815926373e-05,
      "loss": 0.0835,
      "step": 4404
    },
    {
      "epoch": 8.81,
      "grad_norm": 0.38504689931869507,
      "learning_rate": 2.380952380952381e-05,
      "loss": 0.0992,
      "step": 4405
    },
    {
      "epoch": 8.812,
      "grad_norm": 0.9707695245742798,
      "learning_rate": 2.376950780312125e-05,
      "loss": 0.1134,
      "step": 4406
    },
    {
      "epoch": 8.814,
      "grad_norm": 0.4592146575450897,
      "learning_rate": 2.3729491796718688e-05,
      "loss": 0.0783,
      "step": 4407
    },
    {
      "epoch": 8.816,
      "grad_norm": 0.4371642768383026,
      "learning_rate": 2.3689475790316125e-05,
      "loss": 0.1137,
      "step": 4408
    },
    {
      "epoch": 8.818,
      "grad_norm": 0.4262218177318573,
      "learning_rate": 2.3649459783913565e-05,
      "loss": 0.0855,
      "step": 4409
    },
    {
      "epoch": 8.82,
      "grad_norm": 2.939797878265381,
      "learning_rate": 2.3609443777511006e-05,
      "loss": 0.14,
      "step": 4410
    },
    {
      "epoch": 8.822,
      "grad_norm": 0.529656171798706,
      "learning_rate": 2.3569427771108443e-05,
      "loss": 0.1217,
      "step": 4411
    },
    {
      "epoch": 8.824,
      "grad_norm": 0.6582798361778259,
      "learning_rate": 2.3529411764705884e-05,
      "loss": 0.1192,
      "step": 4412
    },
    {
      "epoch": 8.826,
      "grad_norm": 0.5855600237846375,
      "learning_rate": 2.3489395758303324e-05,
      "loss": 0.1448,
      "step": 4413
    },
    {
      "epoch": 8.828,
      "grad_norm": 0.4362543523311615,
      "learning_rate": 2.344937975190076e-05,
      "loss": 0.0995,
      "step": 4414
    },
    {
      "epoch": 8.83,
      "grad_norm": 0.5690029263496399,
      "learning_rate": 2.34093637454982e-05,
      "loss": 0.0933,
      "step": 4415
    },
    {
      "epoch": 8.832,
      "grad_norm": 0.4686397314071655,
      "learning_rate": 2.336934773909564e-05,
      "loss": 0.1079,
      "step": 4416
    },
    {
      "epoch": 8.834,
      "grad_norm": 0.6761687397956848,
      "learning_rate": 2.3329331732693076e-05,
      "loss": 0.1328,
      "step": 4417
    },
    {
      "epoch": 8.836,
      "grad_norm": 0.4661153554916382,
      "learning_rate": 2.3289315726290517e-05,
      "loss": 0.1467,
      "step": 4418
    },
    {
      "epoch": 8.838,
      "grad_norm": 0.383068710565567,
      "learning_rate": 2.3249299719887957e-05,
      "loss": 0.1065,
      "step": 4419
    },
    {
      "epoch": 8.84,
      "grad_norm": 0.4988880753517151,
      "learning_rate": 2.3209283713485394e-05,
      "loss": 0.0986,
      "step": 4420
    },
    {
      "epoch": 8.842,
      "grad_norm": 0.43791213631629944,
      "learning_rate": 2.3169267707082835e-05,
      "loss": 0.1311,
      "step": 4421
    },
    {
      "epoch": 8.844,
      "grad_norm": 0.353910356760025,
      "learning_rate": 2.3129251700680275e-05,
      "loss": 0.1028,
      "step": 4422
    },
    {
      "epoch": 8.846,
      "grad_norm": 0.4635344445705414,
      "learning_rate": 2.3089235694277712e-05,
      "loss": 0.095,
      "step": 4423
    },
    {
      "epoch": 8.848,
      "grad_norm": 0.519178569316864,
      "learning_rate": 2.304921968787515e-05,
      "loss": 0.1039,
      "step": 4424
    },
    {
      "epoch": 8.85,
      "grad_norm": 0.41177037358283997,
      "learning_rate": 2.300920368147259e-05,
      "loss": 0.1062,
      "step": 4425
    },
    {
      "epoch": 8.852,
      "grad_norm": 0.49432268738746643,
      "learning_rate": 2.2969187675070027e-05,
      "loss": 0.1217,
      "step": 4426
    },
    {
      "epoch": 8.854,
      "grad_norm": 0.5173324346542358,
      "learning_rate": 2.2929171668667468e-05,
      "loss": 0.1441,
      "step": 4427
    },
    {
      "epoch": 8.856,
      "grad_norm": 0.45909011363983154,
      "learning_rate": 2.288915566226491e-05,
      "loss": 0.1249,
      "step": 4428
    },
    {
      "epoch": 8.858,
      "grad_norm": 0.4589233100414276,
      "learning_rate": 2.2849139655862345e-05,
      "loss": 0.1399,
      "step": 4429
    },
    {
      "epoch": 8.86,
      "grad_norm": 0.42532554268836975,
      "learning_rate": 2.2809123649459786e-05,
      "loss": 0.1152,
      "step": 4430
    },
    {
      "epoch": 8.862,
      "grad_norm": 0.602852463722229,
      "learning_rate": 2.2769107643057223e-05,
      "loss": 0.0941,
      "step": 4431
    },
    {
      "epoch": 8.864,
      "grad_norm": 0.5508999824523926,
      "learning_rate": 2.272909163665466e-05,
      "loss": 0.128,
      "step": 4432
    },
    {
      "epoch": 8.866,
      "grad_norm": 0.40243878960609436,
      "learning_rate": 2.26890756302521e-05,
      "loss": 0.0845,
      "step": 4433
    },
    {
      "epoch": 8.868,
      "grad_norm": 0.3765706419944763,
      "learning_rate": 2.264905962384954e-05,
      "loss": 0.087,
      "step": 4434
    },
    {
      "epoch": 8.87,
      "grad_norm": 0.46440792083740234,
      "learning_rate": 2.260904361744698e-05,
      "loss": 0.0938,
      "step": 4435
    },
    {
      "epoch": 8.872,
      "grad_norm": 0.5306360125541687,
      "learning_rate": 2.256902761104442e-05,
      "loss": 0.0978,
      "step": 4436
    },
    {
      "epoch": 8.874,
      "grad_norm": 0.4497171938419342,
      "learning_rate": 2.252901160464186e-05,
      "loss": 0.0931,
      "step": 4437
    },
    {
      "epoch": 8.876,
      "grad_norm": 0.4631139934062958,
      "learning_rate": 2.2488995598239297e-05,
      "loss": 0.1426,
      "step": 4438
    },
    {
      "epoch": 8.878,
      "grad_norm": 0.4052026569843292,
      "learning_rate": 2.2448979591836737e-05,
      "loss": 0.0813,
      "step": 4439
    },
    {
      "epoch": 8.88,
      "grad_norm": 0.6525852680206299,
      "learning_rate": 2.2408963585434174e-05,
      "loss": 0.1534,
      "step": 4440
    },
    {
      "epoch": 8.882,
      "grad_norm": 0.5710646510124207,
      "learning_rate": 2.236894757903161e-05,
      "loss": 0.1475,
      "step": 4441
    },
    {
      "epoch": 8.884,
      "grad_norm": 0.3558066785335541,
      "learning_rate": 2.2328931572629052e-05,
      "loss": 0.0974,
      "step": 4442
    },
    {
      "epoch": 8.886,
      "grad_norm": 0.4530501365661621,
      "learning_rate": 2.2288915566226492e-05,
      "loss": 0.1182,
      "step": 4443
    },
    {
      "epoch": 8.888,
      "grad_norm": 0.4121604859828949,
      "learning_rate": 2.224889955982393e-05,
      "loss": 0.1105,
      "step": 4444
    },
    {
      "epoch": 8.89,
      "grad_norm": 0.32188838720321655,
      "learning_rate": 2.220888355342137e-05,
      "loss": 0.1022,
      "step": 4445
    },
    {
      "epoch": 8.892,
      "grad_norm": 0.717849612236023,
      "learning_rate": 2.216886754701881e-05,
      "loss": 0.1167,
      "step": 4446
    },
    {
      "epoch": 8.894,
      "grad_norm": 0.44809961318969727,
      "learning_rate": 2.2128851540616248e-05,
      "loss": 0.1089,
      "step": 4447
    },
    {
      "epoch": 8.896,
      "grad_norm": 0.3488702178001404,
      "learning_rate": 2.2088835534213685e-05,
      "loss": 0.076,
      "step": 4448
    },
    {
      "epoch": 8.898,
      "grad_norm": 0.37201806902885437,
      "learning_rate": 2.2048819527811125e-05,
      "loss": 0.0797,
      "step": 4449
    },
    {
      "epoch": 8.9,
      "grad_norm": 0.546765148639679,
      "learning_rate": 2.2008803521408563e-05,
      "loss": 0.132,
      "step": 4450
    },
    {
      "epoch": 8.902,
      "grad_norm": 0.519809901714325,
      "learning_rate": 2.1968787515006003e-05,
      "loss": 0.1438,
      "step": 4451
    },
    {
      "epoch": 8.904,
      "grad_norm": 0.3004528880119324,
      "learning_rate": 2.1928771508603444e-05,
      "loss": 0.0717,
      "step": 4452
    },
    {
      "epoch": 8.906,
      "grad_norm": 0.46144452691078186,
      "learning_rate": 2.188875550220088e-05,
      "loss": 0.1069,
      "step": 4453
    },
    {
      "epoch": 8.908,
      "grad_norm": 0.2419303059577942,
      "learning_rate": 2.184873949579832e-05,
      "loss": 0.0732,
      "step": 4454
    },
    {
      "epoch": 8.91,
      "grad_norm": 0.47399282455444336,
      "learning_rate": 2.1808723489395762e-05,
      "loss": 0.0967,
      "step": 4455
    },
    {
      "epoch": 8.912,
      "grad_norm": 0.39084410667419434,
      "learning_rate": 2.17687074829932e-05,
      "loss": 0.0962,
      "step": 4456
    },
    {
      "epoch": 8.914,
      "grad_norm": 0.5386091470718384,
      "learning_rate": 2.1728691476590636e-05,
      "loss": 0.1346,
      "step": 4457
    },
    {
      "epoch": 8.916,
      "grad_norm": 0.4069133400917053,
      "learning_rate": 2.1688675470188077e-05,
      "loss": 0.0992,
      "step": 4458
    },
    {
      "epoch": 8.918,
      "grad_norm": 0.43810588121414185,
      "learning_rate": 2.1648659463785514e-05,
      "loss": 0.1065,
      "step": 4459
    },
    {
      "epoch": 8.92,
      "grad_norm": 0.3033212423324585,
      "learning_rate": 2.1608643457382954e-05,
      "loss": 0.0897,
      "step": 4460
    },
    {
      "epoch": 8.922,
      "grad_norm": 0.6183590292930603,
      "learning_rate": 2.1568627450980395e-05,
      "loss": 0.1348,
      "step": 4461
    },
    {
      "epoch": 8.924,
      "grad_norm": 0.4578729569911957,
      "learning_rate": 2.1528611444577832e-05,
      "loss": 0.1035,
      "step": 4462
    },
    {
      "epoch": 8.926,
      "grad_norm": 0.42440399527549744,
      "learning_rate": 2.1488595438175272e-05,
      "loss": 0.0978,
      "step": 4463
    },
    {
      "epoch": 8.928,
      "grad_norm": 0.39510205388069153,
      "learning_rate": 2.144857943177271e-05,
      "loss": 0.0767,
      "step": 4464
    },
    {
      "epoch": 8.93,
      "grad_norm": 0.4483630955219269,
      "learning_rate": 2.1408563425370147e-05,
      "loss": 0.102,
      "step": 4465
    },
    {
      "epoch": 8.932,
      "grad_norm": 0.8051626086235046,
      "learning_rate": 2.1368547418967587e-05,
      "loss": 0.1331,
      "step": 4466
    },
    {
      "epoch": 8.934,
      "grad_norm": 0.46165063977241516,
      "learning_rate": 2.1328531412565028e-05,
      "loss": 0.14,
      "step": 4467
    },
    {
      "epoch": 8.936,
      "grad_norm": 0.42385879158973694,
      "learning_rate": 2.1288515406162465e-05,
      "loss": 0.1243,
      "step": 4468
    },
    {
      "epoch": 8.938,
      "grad_norm": 0.4006398320198059,
      "learning_rate": 2.1248499399759905e-05,
      "loss": 0.102,
      "step": 4469
    },
    {
      "epoch": 8.94,
      "grad_norm": 0.4122774302959442,
      "learning_rate": 2.1208483393357346e-05,
      "loss": 0.103,
      "step": 4470
    },
    {
      "epoch": 8.942,
      "grad_norm": 0.37782469391822815,
      "learning_rate": 2.1168467386954783e-05,
      "loss": 0.0773,
      "step": 4471
    },
    {
      "epoch": 8.943999999999999,
      "grad_norm": 0.36455395817756653,
      "learning_rate": 2.1128451380552224e-05,
      "loss": 0.0916,
      "step": 4472
    },
    {
      "epoch": 8.946,
      "grad_norm": 0.5074961185455322,
      "learning_rate": 2.108843537414966e-05,
      "loss": 0.1302,
      "step": 4473
    },
    {
      "epoch": 8.948,
      "grad_norm": 0.46187615394592285,
      "learning_rate": 2.1048419367747098e-05,
      "loss": 0.0871,
      "step": 4474
    },
    {
      "epoch": 8.95,
      "grad_norm": 0.4197063744068146,
      "learning_rate": 2.100840336134454e-05,
      "loss": 0.0919,
      "step": 4475
    },
    {
      "epoch": 8.952,
      "grad_norm": 0.4830494225025177,
      "learning_rate": 2.096838735494198e-05,
      "loss": 0.0964,
      "step": 4476
    },
    {
      "epoch": 8.954,
      "grad_norm": 0.48296108841896057,
      "learning_rate": 2.0928371348539416e-05,
      "loss": 0.1127,
      "step": 4477
    },
    {
      "epoch": 8.956,
      "grad_norm": 0.5357764363288879,
      "learning_rate": 2.0888355342136857e-05,
      "loss": 0.1506,
      "step": 4478
    },
    {
      "epoch": 8.958,
      "grad_norm": 0.6368206739425659,
      "learning_rate": 2.0848339335734297e-05,
      "loss": 0.1125,
      "step": 4479
    },
    {
      "epoch": 8.96,
      "grad_norm": 0.4390064477920532,
      "learning_rate": 2.0808323329331734e-05,
      "loss": 0.1166,
      "step": 4480
    },
    {
      "epoch": 8.962,
      "grad_norm": 0.38682281970977783,
      "learning_rate": 2.076830732292917e-05,
      "loss": 0.0839,
      "step": 4481
    },
    {
      "epoch": 8.964,
      "grad_norm": 0.5226352214813232,
      "learning_rate": 2.0728291316526612e-05,
      "loss": 0.1144,
      "step": 4482
    },
    {
      "epoch": 8.966,
      "grad_norm": 0.47288843989372253,
      "learning_rate": 2.068827531012405e-05,
      "loss": 0.1075,
      "step": 4483
    },
    {
      "epoch": 8.968,
      "grad_norm": 0.3622300326824188,
      "learning_rate": 2.064825930372149e-05,
      "loss": 0.1089,
      "step": 4484
    },
    {
      "epoch": 8.97,
      "grad_norm": 0.5444117188453674,
      "learning_rate": 2.060824329731893e-05,
      "loss": 0.1612,
      "step": 4485
    },
    {
      "epoch": 8.972,
      "grad_norm": 0.4950075149536133,
      "learning_rate": 2.0568227290916367e-05,
      "loss": 0.1155,
      "step": 4486
    },
    {
      "epoch": 8.974,
      "grad_norm": 0.48965907096862793,
      "learning_rate": 2.0528211284513808e-05,
      "loss": 0.0934,
      "step": 4487
    },
    {
      "epoch": 8.975999999999999,
      "grad_norm": 0.4982490539550781,
      "learning_rate": 2.0488195278111248e-05,
      "loss": 0.1285,
      "step": 4488
    },
    {
      "epoch": 8.978,
      "grad_norm": 0.4658552408218384,
      "learning_rate": 2.0448179271708685e-05,
      "loss": 0.1014,
      "step": 4489
    },
    {
      "epoch": 8.98,
      "grad_norm": 0.5862506628036499,
      "learning_rate": 2.0408163265306123e-05,
      "loss": 0.1495,
      "step": 4490
    },
    {
      "epoch": 8.982,
      "grad_norm": 0.47310999035835266,
      "learning_rate": 2.036814725890356e-05,
      "loss": 0.1003,
      "step": 4491
    },
    {
      "epoch": 8.984,
      "grad_norm": 0.40509793162345886,
      "learning_rate": 2.0328131252501e-05,
      "loss": 0.1215,
      "step": 4492
    },
    {
      "epoch": 8.986,
      "grad_norm": 0.3367988169193268,
      "learning_rate": 2.028811524609844e-05,
      "loss": 0.0861,
      "step": 4493
    },
    {
      "epoch": 8.988,
      "grad_norm": 0.47266721725463867,
      "learning_rate": 2.0248099239695878e-05,
      "loss": 0.1215,
      "step": 4494
    },
    {
      "epoch": 8.99,
      "grad_norm": 0.4992906153202057,
      "learning_rate": 2.020808323329332e-05,
      "loss": 0.1322,
      "step": 4495
    },
    {
      "epoch": 8.992,
      "grad_norm": 0.40023595094680786,
      "learning_rate": 2.016806722689076e-05,
      "loss": 0.1197,
      "step": 4496
    },
    {
      "epoch": 8.994,
      "grad_norm": 0.5220248699188232,
      "learning_rate": 2.0128051220488196e-05,
      "loss": 0.12,
      "step": 4497
    },
    {
      "epoch": 8.996,
      "grad_norm": 0.3961021900177002,
      "learning_rate": 2.0088035214085633e-05,
      "loss": 0.0813,
      "step": 4498
    },
    {
      "epoch": 8.998,
      "grad_norm": 0.40491628646850586,
      "learning_rate": 2.0048019207683074e-05,
      "loss": 0.0966,
      "step": 4499
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.3317788541316986,
      "learning_rate": 2.000800320128051e-05,
      "loss": 0.0867,
      "step": 4500
    },
    {
      "epoch": 9.002,
      "grad_norm": 0.31046292185783386,
      "learning_rate": 1.996798719487795e-05,
      "loss": 0.1054,
      "step": 4501
    },
    {
      "epoch": 9.004,
      "grad_norm": 0.3787451982498169,
      "learning_rate": 1.9927971188475392e-05,
      "loss": 0.0909,
      "step": 4502
    },
    {
      "epoch": 9.006,
      "grad_norm": 0.39509183168411255,
      "learning_rate": 1.988795518207283e-05,
      "loss": 0.1088,
      "step": 4503
    },
    {
      "epoch": 9.008,
      "grad_norm": 0.32502463459968567,
      "learning_rate": 1.984793917567027e-05,
      "loss": 0.0945,
      "step": 4504
    },
    {
      "epoch": 9.01,
      "grad_norm": 0.36984366178512573,
      "learning_rate": 1.980792316926771e-05,
      "loss": 0.0921,
      "step": 4505
    },
    {
      "epoch": 9.012,
      "grad_norm": 0.34427040815353394,
      "learning_rate": 1.9767907162865147e-05,
      "loss": 0.0946,
      "step": 4506
    },
    {
      "epoch": 9.014,
      "grad_norm": 0.4321267902851105,
      "learning_rate": 1.9727891156462584e-05,
      "loss": 0.1139,
      "step": 4507
    },
    {
      "epoch": 9.016,
      "grad_norm": 0.4761504530906677,
      "learning_rate": 1.9687875150060025e-05,
      "loss": 0.1373,
      "step": 4508
    },
    {
      "epoch": 9.018,
      "grad_norm": 0.35133251547813416,
      "learning_rate": 1.9647859143657462e-05,
      "loss": 0.0938,
      "step": 4509
    },
    {
      "epoch": 9.02,
      "grad_norm": 0.301077276468277,
      "learning_rate": 1.9607843137254903e-05,
      "loss": 0.0911,
      "step": 4510
    },
    {
      "epoch": 9.022,
      "grad_norm": 0.4124363958835602,
      "learning_rate": 1.9567827130852343e-05,
      "loss": 0.0845,
      "step": 4511
    },
    {
      "epoch": 9.024,
      "grad_norm": 0.3904160261154175,
      "learning_rate": 1.952781112444978e-05,
      "loss": 0.1129,
      "step": 4512
    },
    {
      "epoch": 9.026,
      "grad_norm": 0.4913538098335266,
      "learning_rate": 1.948779511804722e-05,
      "loss": 0.1293,
      "step": 4513
    },
    {
      "epoch": 9.028,
      "grad_norm": 0.4165605902671814,
      "learning_rate": 1.9447779111644658e-05,
      "loss": 0.1292,
      "step": 4514
    },
    {
      "epoch": 9.03,
      "grad_norm": 0.40159982442855835,
      "learning_rate": 1.9407763105242095e-05,
      "loss": 0.1152,
      "step": 4515
    },
    {
      "epoch": 9.032,
      "grad_norm": 0.5230951905250549,
      "learning_rate": 1.9367747098839535e-05,
      "loss": 0.1348,
      "step": 4516
    },
    {
      "epoch": 9.034,
      "grad_norm": 0.2831956148147583,
      "learning_rate": 1.9327731092436976e-05,
      "loss": 0.0766,
      "step": 4517
    },
    {
      "epoch": 9.036,
      "grad_norm": 0.34901922941207886,
      "learning_rate": 1.9287715086034413e-05,
      "loss": 0.0713,
      "step": 4518
    },
    {
      "epoch": 9.038,
      "grad_norm": 0.3252241909503937,
      "learning_rate": 1.9247699079631854e-05,
      "loss": 0.0789,
      "step": 4519
    },
    {
      "epoch": 9.04,
      "grad_norm": 0.4560665786266327,
      "learning_rate": 1.9207683073229294e-05,
      "loss": 0.0962,
      "step": 4520
    },
    {
      "epoch": 9.042,
      "grad_norm": 0.2536480724811554,
      "learning_rate": 1.916766706682673e-05,
      "loss": 0.0819,
      "step": 4521
    },
    {
      "epoch": 9.044,
      "grad_norm": 0.46348124742507935,
      "learning_rate": 1.9127651060424172e-05,
      "loss": 0.1025,
      "step": 4522
    },
    {
      "epoch": 9.046,
      "grad_norm": 0.48519667983055115,
      "learning_rate": 1.908763505402161e-05,
      "loss": 0.1193,
      "step": 4523
    },
    {
      "epoch": 9.048,
      "grad_norm": 0.3862888813018799,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 0.0896,
      "step": 4524
    },
    {
      "epoch": 9.05,
      "grad_norm": 0.31714949011802673,
      "learning_rate": 1.9007603041216487e-05,
      "loss": 0.0914,
      "step": 4525
    },
    {
      "epoch": 9.052,
      "grad_norm": 0.42402413487434387,
      "learning_rate": 1.8967587034813927e-05,
      "loss": 0.0848,
      "step": 4526
    },
    {
      "epoch": 9.054,
      "grad_norm": 0.4263048470020294,
      "learning_rate": 1.8927571028411364e-05,
      "loss": 0.0899,
      "step": 4527
    },
    {
      "epoch": 9.056,
      "grad_norm": 0.40258166193962097,
      "learning_rate": 1.8887555022008805e-05,
      "loss": 0.0867,
      "step": 4528
    },
    {
      "epoch": 9.058,
      "grad_norm": 0.369441956281662,
      "learning_rate": 1.8847539015606245e-05,
      "loss": 0.0917,
      "step": 4529
    },
    {
      "epoch": 9.06,
      "grad_norm": 0.42032063007354736,
      "learning_rate": 1.8807523009203682e-05,
      "loss": 0.1003,
      "step": 4530
    },
    {
      "epoch": 9.062,
      "grad_norm": 0.47179317474365234,
      "learning_rate": 1.876750700280112e-05,
      "loss": 0.0985,
      "step": 4531
    },
    {
      "epoch": 9.064,
      "grad_norm": 0.35226917266845703,
      "learning_rate": 1.872749099639856e-05,
      "loss": 0.0954,
      "step": 4532
    },
    {
      "epoch": 9.066,
      "grad_norm": 0.3427393138408661,
      "learning_rate": 1.8687474989995997e-05,
      "loss": 0.0977,
      "step": 4533
    },
    {
      "epoch": 9.068,
      "grad_norm": 0.45811131596565247,
      "learning_rate": 1.8647458983593438e-05,
      "loss": 0.1019,
      "step": 4534
    },
    {
      "epoch": 9.07,
      "grad_norm": 0.29180142283439636,
      "learning_rate": 1.860744297719088e-05,
      "loss": 0.0864,
      "step": 4535
    },
    {
      "epoch": 9.072,
      "grad_norm": 0.44703179597854614,
      "learning_rate": 1.8567426970788315e-05,
      "loss": 0.1054,
      "step": 4536
    },
    {
      "epoch": 9.074,
      "grad_norm": 0.3870581388473511,
      "learning_rate": 1.8527410964385756e-05,
      "loss": 0.0704,
      "step": 4537
    },
    {
      "epoch": 9.076,
      "grad_norm": 0.3765884041786194,
      "learning_rate": 1.8487394957983196e-05,
      "loss": 0.0724,
      "step": 4538
    },
    {
      "epoch": 9.078,
      "grad_norm": 0.3544178903102875,
      "learning_rate": 1.8447378951580634e-05,
      "loss": 0.0919,
      "step": 4539
    },
    {
      "epoch": 9.08,
      "grad_norm": 0.4883938729763031,
      "learning_rate": 1.840736294517807e-05,
      "loss": 0.1192,
      "step": 4540
    },
    {
      "epoch": 9.082,
      "grad_norm": 0.3994889259338379,
      "learning_rate": 1.836734693877551e-05,
      "loss": 0.1141,
      "step": 4541
    },
    {
      "epoch": 9.084,
      "grad_norm": 0.457084059715271,
      "learning_rate": 1.832733093237295e-05,
      "loss": 0.0927,
      "step": 4542
    },
    {
      "epoch": 9.086,
      "grad_norm": 0.36683064699172974,
      "learning_rate": 1.828731492597039e-05,
      "loss": 0.0775,
      "step": 4543
    },
    {
      "epoch": 9.088,
      "grad_norm": 0.44945216178894043,
      "learning_rate": 1.824729891956783e-05,
      "loss": 0.0929,
      "step": 4544
    },
    {
      "epoch": 9.09,
      "grad_norm": 0.4998871684074402,
      "learning_rate": 1.8207282913165267e-05,
      "loss": 0.083,
      "step": 4545
    },
    {
      "epoch": 9.092,
      "grad_norm": 0.4325075149536133,
      "learning_rate": 1.8167266906762707e-05,
      "loss": 0.1254,
      "step": 4546
    },
    {
      "epoch": 9.094,
      "grad_norm": 0.5440376400947571,
      "learning_rate": 1.8127250900360144e-05,
      "loss": 0.1517,
      "step": 4547
    },
    {
      "epoch": 9.096,
      "grad_norm": 0.509327232837677,
      "learning_rate": 1.808723489395758e-05,
      "loss": 0.0891,
      "step": 4548
    },
    {
      "epoch": 9.098,
      "grad_norm": 0.3841560184955597,
      "learning_rate": 1.8047218887555022e-05,
      "loss": 0.0706,
      "step": 4549
    },
    {
      "epoch": 9.1,
      "grad_norm": 0.46944284439086914,
      "learning_rate": 1.8007202881152462e-05,
      "loss": 0.0917,
      "step": 4550
    },
    {
      "epoch": 9.102,
      "grad_norm": 0.5443151593208313,
      "learning_rate": 1.79671868747499e-05,
      "loss": 0.1018,
      "step": 4551
    },
    {
      "epoch": 9.104,
      "grad_norm": 0.37471234798431396,
      "learning_rate": 1.792717086834734e-05,
      "loss": 0.0766,
      "step": 4552
    },
    {
      "epoch": 9.106,
      "grad_norm": 0.4093073010444641,
      "learning_rate": 1.788715486194478e-05,
      "loss": 0.1155,
      "step": 4553
    },
    {
      "epoch": 9.108,
      "grad_norm": 0.5014792084693909,
      "learning_rate": 1.7847138855542218e-05,
      "loss": 0.0904,
      "step": 4554
    },
    {
      "epoch": 9.11,
      "grad_norm": 0.4366936981678009,
      "learning_rate": 1.7807122849139658e-05,
      "loss": 0.1021,
      "step": 4555
    },
    {
      "epoch": 9.112,
      "grad_norm": 0.5330082774162292,
      "learning_rate": 1.7767106842737095e-05,
      "loss": 0.1398,
      "step": 4556
    },
    {
      "epoch": 9.114,
      "grad_norm": 0.5309694409370422,
      "learning_rate": 1.7727090836334533e-05,
      "loss": 0.1152,
      "step": 4557
    },
    {
      "epoch": 9.116,
      "grad_norm": 0.4183180332183838,
      "learning_rate": 1.7687074829931973e-05,
      "loss": 0.0977,
      "step": 4558
    },
    {
      "epoch": 9.118,
      "grad_norm": 0.5101739764213562,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 0.1189,
      "step": 4559
    },
    {
      "epoch": 9.12,
      "grad_norm": 0.39162784814834595,
      "learning_rate": 1.760704281712685e-05,
      "loss": 0.1088,
      "step": 4560
    },
    {
      "epoch": 9.122,
      "grad_norm": 0.45043033361434937,
      "learning_rate": 1.756702681072429e-05,
      "loss": 0.097,
      "step": 4561
    },
    {
      "epoch": 9.124,
      "grad_norm": 0.44530540704727173,
      "learning_rate": 1.7527010804321732e-05,
      "loss": 0.1095,
      "step": 4562
    },
    {
      "epoch": 9.126,
      "grad_norm": 0.3429549038410187,
      "learning_rate": 1.748699479791917e-05,
      "loss": 0.0948,
      "step": 4563
    },
    {
      "epoch": 9.128,
      "grad_norm": 0.4320651590824127,
      "learning_rate": 1.7446978791516606e-05,
      "loss": 0.1052,
      "step": 4564
    },
    {
      "epoch": 9.13,
      "grad_norm": 0.3815833032131195,
      "learning_rate": 1.7406962785114047e-05,
      "loss": 0.0795,
      "step": 4565
    },
    {
      "epoch": 9.132,
      "grad_norm": 0.32625797390937805,
      "learning_rate": 1.7366946778711484e-05,
      "loss": 0.0775,
      "step": 4566
    },
    {
      "epoch": 9.134,
      "grad_norm": 0.5493628978729248,
      "learning_rate": 1.7326930772308924e-05,
      "loss": 0.1328,
      "step": 4567
    },
    {
      "epoch": 9.136,
      "grad_norm": 0.5134065747261047,
      "learning_rate": 1.7286914765906365e-05,
      "loss": 0.1273,
      "step": 4568
    },
    {
      "epoch": 9.138,
      "grad_norm": 0.5943578481674194,
      "learning_rate": 1.7246898759503802e-05,
      "loss": 0.1429,
      "step": 4569
    },
    {
      "epoch": 9.14,
      "grad_norm": 0.4740166962146759,
      "learning_rate": 1.7206882753101242e-05,
      "loss": 0.102,
      "step": 4570
    },
    {
      "epoch": 9.142,
      "grad_norm": 0.5335202813148499,
      "learning_rate": 1.7166866746698683e-05,
      "loss": 0.0982,
      "step": 4571
    },
    {
      "epoch": 9.144,
      "grad_norm": 0.2904672622680664,
      "learning_rate": 1.712685074029612e-05,
      "loss": 0.0859,
      "step": 4572
    },
    {
      "epoch": 9.146,
      "grad_norm": 0.5425336360931396,
      "learning_rate": 1.7086834733893557e-05,
      "loss": 0.088,
      "step": 4573
    },
    {
      "epoch": 9.148,
      "grad_norm": 0.6895272731781006,
      "learning_rate": 1.7046818727490998e-05,
      "loss": 0.1312,
      "step": 4574
    },
    {
      "epoch": 9.15,
      "grad_norm": 0.4096972942352295,
      "learning_rate": 1.7006802721088435e-05,
      "loss": 0.0982,
      "step": 4575
    },
    {
      "epoch": 9.152,
      "grad_norm": 0.475772887468338,
      "learning_rate": 1.6966786714685875e-05,
      "loss": 0.107,
      "step": 4576
    },
    {
      "epoch": 9.154,
      "grad_norm": 0.2883087694644928,
      "learning_rate": 1.6926770708283316e-05,
      "loss": 0.0674,
      "step": 4577
    },
    {
      "epoch": 9.156,
      "grad_norm": 0.3805459439754486,
      "learning_rate": 1.6886754701880753e-05,
      "loss": 0.0924,
      "step": 4578
    },
    {
      "epoch": 9.158,
      "grad_norm": 0.4069940745830536,
      "learning_rate": 1.6846738695478194e-05,
      "loss": 0.1086,
      "step": 4579
    },
    {
      "epoch": 9.16,
      "grad_norm": 0.44952788949012756,
      "learning_rate": 1.6806722689075634e-05,
      "loss": 0.1054,
      "step": 4580
    },
    {
      "epoch": 9.162,
      "grad_norm": 0.42927494645118713,
      "learning_rate": 1.6766706682673068e-05,
      "loss": 0.1145,
      "step": 4581
    },
    {
      "epoch": 9.164,
      "grad_norm": 0.6597323417663574,
      "learning_rate": 1.672669067627051e-05,
      "loss": 0.1313,
      "step": 4582
    },
    {
      "epoch": 9.166,
      "grad_norm": 0.5444678664207458,
      "learning_rate": 1.668667466986795e-05,
      "loss": 0.1119,
      "step": 4583
    },
    {
      "epoch": 9.168,
      "grad_norm": 0.438297837972641,
      "learning_rate": 1.6646658663465386e-05,
      "loss": 0.0932,
      "step": 4584
    },
    {
      "epoch": 9.17,
      "grad_norm": 0.47676363587379456,
      "learning_rate": 1.6606642657062827e-05,
      "loss": 0.0942,
      "step": 4585
    },
    {
      "epoch": 9.172,
      "grad_norm": 0.36239907145500183,
      "learning_rate": 1.6566626650660264e-05,
      "loss": 0.0807,
      "step": 4586
    },
    {
      "epoch": 9.174,
      "grad_norm": 0.4013284146785736,
      "learning_rate": 1.6526610644257704e-05,
      "loss": 0.0875,
      "step": 4587
    },
    {
      "epoch": 9.176,
      "grad_norm": 0.3893405497074127,
      "learning_rate": 1.6486594637855145e-05,
      "loss": 0.1132,
      "step": 4588
    },
    {
      "epoch": 9.178,
      "grad_norm": 0.653450608253479,
      "learning_rate": 1.6446578631452582e-05,
      "loss": 0.0851,
      "step": 4589
    },
    {
      "epoch": 9.18,
      "grad_norm": 0.5423597097396851,
      "learning_rate": 1.640656262505002e-05,
      "loss": 0.1235,
      "step": 4590
    },
    {
      "epoch": 9.182,
      "grad_norm": 0.5112569332122803,
      "learning_rate": 1.636654661864746e-05,
      "loss": 0.1181,
      "step": 4591
    },
    {
      "epoch": 9.184,
      "grad_norm": 0.5020125508308411,
      "learning_rate": 1.6326530612244897e-05,
      "loss": 0.1284,
      "step": 4592
    },
    {
      "epoch": 9.186,
      "grad_norm": 0.4107247292995453,
      "learning_rate": 1.6286514605842337e-05,
      "loss": 0.1114,
      "step": 4593
    },
    {
      "epoch": 9.188,
      "grad_norm": 0.5834400653839111,
      "learning_rate": 1.6246498599439778e-05,
      "loss": 0.1459,
      "step": 4594
    },
    {
      "epoch": 9.19,
      "grad_norm": 0.4112383723258972,
      "learning_rate": 1.6206482593037215e-05,
      "loss": 0.1104,
      "step": 4595
    },
    {
      "epoch": 9.192,
      "grad_norm": 0.46463024616241455,
      "learning_rate": 1.6166466586634655e-05,
      "loss": 0.0831,
      "step": 4596
    },
    {
      "epoch": 9.194,
      "grad_norm": 0.3933255076408386,
      "learning_rate": 1.6126450580232096e-05,
      "loss": 0.1012,
      "step": 4597
    },
    {
      "epoch": 9.196,
      "grad_norm": 0.36674726009368896,
      "learning_rate": 1.6086434573829533e-05,
      "loss": 0.0832,
      "step": 4598
    },
    {
      "epoch": 9.198,
      "grad_norm": 0.470737099647522,
      "learning_rate": 1.604641856742697e-05,
      "loss": 0.1375,
      "step": 4599
    },
    {
      "epoch": 9.2,
      "grad_norm": 0.5138746500015259,
      "learning_rate": 1.600640256102441e-05,
      "loss": 0.1105,
      "step": 4600
    },
    {
      "epoch": 9.202,
      "grad_norm": 0.6259369254112244,
      "learning_rate": 1.5966386554621848e-05,
      "loss": 0.1094,
      "step": 4601
    },
    {
      "epoch": 9.204,
      "grad_norm": 0.5894044637680054,
      "learning_rate": 1.592637054821929e-05,
      "loss": 0.0992,
      "step": 4602
    },
    {
      "epoch": 9.206,
      "grad_norm": 0.5257212519645691,
      "learning_rate": 1.588635454181673e-05,
      "loss": 0.0952,
      "step": 4603
    },
    {
      "epoch": 9.208,
      "grad_norm": 0.3350340723991394,
      "learning_rate": 1.5846338535414166e-05,
      "loss": 0.084,
      "step": 4604
    },
    {
      "epoch": 9.21,
      "grad_norm": 0.5454733371734619,
      "learning_rate": 1.5806322529011607e-05,
      "loss": 0.1001,
      "step": 4605
    },
    {
      "epoch": 9.212,
      "grad_norm": 0.4691697359085083,
      "learning_rate": 1.5766306522609044e-05,
      "loss": 0.1144,
      "step": 4606
    },
    {
      "epoch": 9.214,
      "grad_norm": 0.479599267244339,
      "learning_rate": 1.572629051620648e-05,
      "loss": 0.1073,
      "step": 4607
    },
    {
      "epoch": 9.216,
      "grad_norm": 0.564889132976532,
      "learning_rate": 1.568627450980392e-05,
      "loss": 0.1474,
      "step": 4608
    },
    {
      "epoch": 9.218,
      "grad_norm": 0.5250577926635742,
      "learning_rate": 1.5646258503401362e-05,
      "loss": 0.0947,
      "step": 4609
    },
    {
      "epoch": 9.22,
      "grad_norm": 0.5860328674316406,
      "learning_rate": 1.56062424969988e-05,
      "loss": 0.1034,
      "step": 4610
    },
    {
      "epoch": 9.222,
      "grad_norm": 0.424056738615036,
      "learning_rate": 1.556622649059624e-05,
      "loss": 0.115,
      "step": 4611
    },
    {
      "epoch": 9.224,
      "grad_norm": 0.38560834527015686,
      "learning_rate": 1.552621048419368e-05,
      "loss": 0.0954,
      "step": 4612
    },
    {
      "epoch": 9.226,
      "grad_norm": 0.4146198034286499,
      "learning_rate": 1.5486194477791117e-05,
      "loss": 0.1064,
      "step": 4613
    },
    {
      "epoch": 9.228,
      "grad_norm": 0.41465333104133606,
      "learning_rate": 1.5446178471388558e-05,
      "loss": 0.0938,
      "step": 4614
    },
    {
      "epoch": 9.23,
      "grad_norm": 0.649952232837677,
      "learning_rate": 1.5406162464985995e-05,
      "loss": 0.1409,
      "step": 4615
    },
    {
      "epoch": 9.232,
      "grad_norm": 0.6151290535926819,
      "learning_rate": 1.5366146458583432e-05,
      "loss": 0.1291,
      "step": 4616
    },
    {
      "epoch": 9.234,
      "grad_norm": 0.37952837347984314,
      "learning_rate": 1.5326130452180872e-05,
      "loss": 0.0992,
      "step": 4617
    },
    {
      "epoch": 9.236,
      "grad_norm": 0.5045314431190491,
      "learning_rate": 1.5286114445778313e-05,
      "loss": 0.1019,
      "step": 4618
    },
    {
      "epoch": 9.238,
      "grad_norm": 0.496938019990921,
      "learning_rate": 1.524609843937575e-05,
      "loss": 0.0858,
      "step": 4619
    },
    {
      "epoch": 9.24,
      "grad_norm": 0.447977751493454,
      "learning_rate": 1.520608243297319e-05,
      "loss": 0.105,
      "step": 4620
    },
    {
      "epoch": 9.242,
      "grad_norm": 0.4771571755409241,
      "learning_rate": 1.516606642657063e-05,
      "loss": 0.135,
      "step": 4621
    },
    {
      "epoch": 9.244,
      "grad_norm": 0.5140294432640076,
      "learning_rate": 1.5126050420168067e-05,
      "loss": 0.1292,
      "step": 4622
    },
    {
      "epoch": 9.246,
      "grad_norm": 0.4995426833629608,
      "learning_rate": 1.5086034413765507e-05,
      "loss": 0.1287,
      "step": 4623
    },
    {
      "epoch": 9.248,
      "grad_norm": 0.4493333101272583,
      "learning_rate": 1.5046018407362946e-05,
      "loss": 0.1055,
      "step": 4624
    },
    {
      "epoch": 9.25,
      "grad_norm": 0.4788810610771179,
      "learning_rate": 1.5006002400960383e-05,
      "loss": 0.1011,
      "step": 4625
    },
    {
      "epoch": 9.252,
      "grad_norm": 0.5154561400413513,
      "learning_rate": 1.4965986394557824e-05,
      "loss": 0.098,
      "step": 4626
    },
    {
      "epoch": 9.254,
      "grad_norm": 0.3913567066192627,
      "learning_rate": 1.4925970388155264e-05,
      "loss": 0.0785,
      "step": 4627
    },
    {
      "epoch": 9.256,
      "grad_norm": 0.4029310643672943,
      "learning_rate": 1.4885954381752701e-05,
      "loss": 0.094,
      "step": 4628
    },
    {
      "epoch": 9.258,
      "grad_norm": 0.452597439289093,
      "learning_rate": 1.484593837535014e-05,
      "loss": 0.0946,
      "step": 4629
    },
    {
      "epoch": 9.26,
      "grad_norm": 0.4247910976409912,
      "learning_rate": 1.480592236894758e-05,
      "loss": 0.0893,
      "step": 4630
    },
    {
      "epoch": 9.262,
      "grad_norm": 0.47829359769821167,
      "learning_rate": 1.4765906362545018e-05,
      "loss": 0.1239,
      "step": 4631
    },
    {
      "epoch": 9.264,
      "grad_norm": 0.3574643135070801,
      "learning_rate": 1.4725890356142458e-05,
      "loss": 0.0898,
      "step": 4632
    },
    {
      "epoch": 9.266,
      "grad_norm": 0.5743881464004517,
      "learning_rate": 1.4685874349739897e-05,
      "loss": 0.0971,
      "step": 4633
    },
    {
      "epoch": 9.268,
      "grad_norm": 0.5178125500679016,
      "learning_rate": 1.4645858343337334e-05,
      "loss": 0.1133,
      "step": 4634
    },
    {
      "epoch": 9.27,
      "grad_norm": 0.5361458659172058,
      "learning_rate": 1.4605842336934775e-05,
      "loss": 0.0794,
      "step": 4635
    },
    {
      "epoch": 9.272,
      "grad_norm": 0.45465242862701416,
      "learning_rate": 1.4565826330532215e-05,
      "loss": 0.0844,
      "step": 4636
    },
    {
      "epoch": 9.274000000000001,
      "grad_norm": 0.4882868230342865,
      "learning_rate": 1.4525810324129652e-05,
      "loss": 0.0901,
      "step": 4637
    },
    {
      "epoch": 9.276,
      "grad_norm": 0.5537570714950562,
      "learning_rate": 1.4485794317727091e-05,
      "loss": 0.1102,
      "step": 4638
    },
    {
      "epoch": 9.278,
      "grad_norm": 0.6542044878005981,
      "learning_rate": 1.4445778311324532e-05,
      "loss": 0.1197,
      "step": 4639
    },
    {
      "epoch": 9.28,
      "grad_norm": 0.5214497447013855,
      "learning_rate": 1.4405762304921969e-05,
      "loss": 0.1178,
      "step": 4640
    },
    {
      "epoch": 9.282,
      "grad_norm": 0.3319491147994995,
      "learning_rate": 1.4365746298519408e-05,
      "loss": 0.0934,
      "step": 4641
    },
    {
      "epoch": 9.284,
      "grad_norm": 0.49449408054351807,
      "learning_rate": 1.4325730292116848e-05,
      "loss": 0.1187,
      "step": 4642
    },
    {
      "epoch": 9.286,
      "grad_norm": 0.425048828125,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.1068,
      "step": 4643
    },
    {
      "epoch": 9.288,
      "grad_norm": 0.4286840260028839,
      "learning_rate": 1.4245698279311726e-05,
      "loss": 0.1226,
      "step": 4644
    },
    {
      "epoch": 9.29,
      "grad_norm": 0.5639187693595886,
      "learning_rate": 1.4205682272909165e-05,
      "loss": 0.1183,
      "step": 4645
    },
    {
      "epoch": 9.292,
      "grad_norm": 0.39190658926963806,
      "learning_rate": 1.4165666266506602e-05,
      "loss": 0.1031,
      "step": 4646
    },
    {
      "epoch": 9.294,
      "grad_norm": 0.3943256437778473,
      "learning_rate": 1.4125650260104042e-05,
      "loss": 0.0756,
      "step": 4647
    },
    {
      "epoch": 9.296,
      "grad_norm": 0.5539464950561523,
      "learning_rate": 1.4085634253701483e-05,
      "loss": 0.106,
      "step": 4648
    },
    {
      "epoch": 9.298,
      "grad_norm": 0.4251703917980194,
      "learning_rate": 1.404561824729892e-05,
      "loss": 0.0869,
      "step": 4649
    },
    {
      "epoch": 9.3,
      "grad_norm": 0.3836331367492676,
      "learning_rate": 1.4005602240896359e-05,
      "loss": 0.0888,
      "step": 4650
    },
    {
      "epoch": 9.302,
      "grad_norm": 0.38728833198547363,
      "learning_rate": 1.39655862344938e-05,
      "loss": 0.0829,
      "step": 4651
    },
    {
      "epoch": 9.304,
      "grad_norm": 0.44387543201446533,
      "learning_rate": 1.3925570228091237e-05,
      "loss": 0.1121,
      "step": 4652
    },
    {
      "epoch": 9.306,
      "grad_norm": 0.4947945773601532,
      "learning_rate": 1.3885554221688677e-05,
      "loss": 0.0746,
      "step": 4653
    },
    {
      "epoch": 9.308,
      "grad_norm": 0.5472503304481506,
      "learning_rate": 1.3845538215286116e-05,
      "loss": 0.0766,
      "step": 4654
    },
    {
      "epoch": 9.31,
      "grad_norm": 0.6668674349784851,
      "learning_rate": 1.3805522208883553e-05,
      "loss": 0.1042,
      "step": 4655
    },
    {
      "epoch": 9.312,
      "grad_norm": 0.43855106830596924,
      "learning_rate": 1.3765506202480994e-05,
      "loss": 0.0831,
      "step": 4656
    },
    {
      "epoch": 9.314,
      "grad_norm": 0.46057257056236267,
      "learning_rate": 1.3725490196078432e-05,
      "loss": 0.0936,
      "step": 4657
    },
    {
      "epoch": 9.316,
      "grad_norm": 0.4301955997943878,
      "learning_rate": 1.368547418967587e-05,
      "loss": 0.1034,
      "step": 4658
    },
    {
      "epoch": 9.318,
      "grad_norm": 0.461160272359848,
      "learning_rate": 1.364545818327331e-05,
      "loss": 0.1047,
      "step": 4659
    },
    {
      "epoch": 9.32,
      "grad_norm": 0.45732027292251587,
      "learning_rate": 1.360544217687075e-05,
      "loss": 0.0877,
      "step": 4660
    },
    {
      "epoch": 9.322,
      "grad_norm": 0.45111697912216187,
      "learning_rate": 1.3565426170468188e-05,
      "loss": 0.0987,
      "step": 4661
    },
    {
      "epoch": 9.324,
      "grad_norm": 0.4617931544780731,
      "learning_rate": 1.3525410164065627e-05,
      "loss": 0.1319,
      "step": 4662
    },
    {
      "epoch": 9.326,
      "grad_norm": 0.5498549342155457,
      "learning_rate": 1.3485394157663067e-05,
      "loss": 0.1101,
      "step": 4663
    },
    {
      "epoch": 9.328,
      "grad_norm": 0.4789237678050995,
      "learning_rate": 1.3445378151260504e-05,
      "loss": 0.0884,
      "step": 4664
    },
    {
      "epoch": 9.33,
      "grad_norm": 0.4282160997390747,
      "learning_rate": 1.3405362144857945e-05,
      "loss": 0.089,
      "step": 4665
    },
    {
      "epoch": 9.332,
      "grad_norm": 0.400064080953598,
      "learning_rate": 1.3365346138455384e-05,
      "loss": 0.0739,
      "step": 4666
    },
    {
      "epoch": 9.334,
      "grad_norm": 0.35187581181526184,
      "learning_rate": 1.332533013205282e-05,
      "loss": 0.0806,
      "step": 4667
    },
    {
      "epoch": 9.336,
      "grad_norm": 0.3573554456233978,
      "learning_rate": 1.3285314125650261e-05,
      "loss": 0.0755,
      "step": 4668
    },
    {
      "epoch": 9.338,
      "grad_norm": 0.23317080736160278,
      "learning_rate": 1.3245298119247702e-05,
      "loss": 0.0709,
      "step": 4669
    },
    {
      "epoch": 9.34,
      "grad_norm": 0.43135905265808105,
      "learning_rate": 1.3205282112845139e-05,
      "loss": 0.1067,
      "step": 4670
    },
    {
      "epoch": 9.342,
      "grad_norm": 0.4501818120479584,
      "learning_rate": 1.3165266106442578e-05,
      "loss": 0.097,
      "step": 4671
    },
    {
      "epoch": 9.344,
      "grad_norm": 0.3946738541126251,
      "learning_rate": 1.3125250100040018e-05,
      "loss": 0.0867,
      "step": 4672
    },
    {
      "epoch": 9.346,
      "grad_norm": 0.475610613822937,
      "learning_rate": 1.3085234093637455e-05,
      "loss": 0.0952,
      "step": 4673
    },
    {
      "epoch": 9.348,
      "grad_norm": 0.4697212278842926,
      "learning_rate": 1.3045218087234894e-05,
      "loss": 0.1249,
      "step": 4674
    },
    {
      "epoch": 9.35,
      "grad_norm": 0.45763975381851196,
      "learning_rate": 1.3005202080832335e-05,
      "loss": 0.0946,
      "step": 4675
    },
    {
      "epoch": 9.352,
      "grad_norm": 0.3348924219608307,
      "learning_rate": 1.2965186074429772e-05,
      "loss": 0.0781,
      "step": 4676
    },
    {
      "epoch": 9.354,
      "grad_norm": 0.4620816111564636,
      "learning_rate": 1.2925170068027212e-05,
      "loss": 0.1174,
      "step": 4677
    },
    {
      "epoch": 9.356,
      "grad_norm": 0.3171100914478302,
      "learning_rate": 1.288515406162465e-05,
      "loss": 0.0903,
      "step": 4678
    },
    {
      "epoch": 9.358,
      "grad_norm": 0.5537885427474976,
      "learning_rate": 1.2845138055222088e-05,
      "loss": 0.1122,
      "step": 4679
    },
    {
      "epoch": 9.36,
      "grad_norm": 0.44221749901771545,
      "learning_rate": 1.2805122048819529e-05,
      "loss": 0.0874,
      "step": 4680
    },
    {
      "epoch": 9.362,
      "grad_norm": 0.4402795732021332,
      "learning_rate": 1.2765106042416966e-05,
      "loss": 0.1294,
      "step": 4681
    },
    {
      "epoch": 9.364,
      "grad_norm": 0.6356022953987122,
      "learning_rate": 1.2725090036014407e-05,
      "loss": 0.1846,
      "step": 4682
    },
    {
      "epoch": 9.366,
      "grad_norm": 0.42422595620155334,
      "learning_rate": 1.2685074029611845e-05,
      "loss": 0.1218,
      "step": 4683
    },
    {
      "epoch": 9.368,
      "grad_norm": 0.4347022473812103,
      "learning_rate": 1.2645058023209283e-05,
      "loss": 0.0962,
      "step": 4684
    },
    {
      "epoch": 9.37,
      "grad_norm": 0.41696515679359436,
      "learning_rate": 1.2605042016806723e-05,
      "loss": 0.1116,
      "step": 4685
    },
    {
      "epoch": 9.372,
      "grad_norm": 0.44334888458251953,
      "learning_rate": 1.2565026010404164e-05,
      "loss": 0.1208,
      "step": 4686
    },
    {
      "epoch": 9.374,
      "grad_norm": 0.3731164038181305,
      "learning_rate": 1.25250100040016e-05,
      "loss": 0.0921,
      "step": 4687
    },
    {
      "epoch": 9.376,
      "grad_norm": 0.3779321610927582,
      "learning_rate": 1.248499399759904e-05,
      "loss": 0.0974,
      "step": 4688
    },
    {
      "epoch": 9.378,
      "grad_norm": 0.46792781352996826,
      "learning_rate": 1.244497799119648e-05,
      "loss": 0.1217,
      "step": 4689
    },
    {
      "epoch": 9.38,
      "grad_norm": 0.3662785291671753,
      "learning_rate": 1.2404961984793919e-05,
      "loss": 0.0985,
      "step": 4690
    },
    {
      "epoch": 9.382,
      "grad_norm": 0.5551946759223938,
      "learning_rate": 1.2364945978391356e-05,
      "loss": 0.0977,
      "step": 4691
    },
    {
      "epoch": 9.384,
      "grad_norm": 0.3680093288421631,
      "learning_rate": 1.2324929971988797e-05,
      "loss": 0.0813,
      "step": 4692
    },
    {
      "epoch": 9.386,
      "grad_norm": 0.5163401961326599,
      "learning_rate": 1.2284913965586235e-05,
      "loss": 0.1324,
      "step": 4693
    },
    {
      "epoch": 9.388,
      "grad_norm": 0.48932626843452454,
      "learning_rate": 1.2244897959183674e-05,
      "loss": 0.1159,
      "step": 4694
    },
    {
      "epoch": 9.39,
      "grad_norm": 0.5647056698799133,
      "learning_rate": 1.2204881952781113e-05,
      "loss": 0.0793,
      "step": 4695
    },
    {
      "epoch": 9.392,
      "grad_norm": 0.43846195936203003,
      "learning_rate": 1.2164865946378552e-05,
      "loss": 0.0814,
      "step": 4696
    },
    {
      "epoch": 9.394,
      "grad_norm": 0.5849988460540771,
      "learning_rate": 1.212484993997599e-05,
      "loss": 0.1438,
      "step": 4697
    },
    {
      "epoch": 9.396,
      "grad_norm": 0.5353466272354126,
      "learning_rate": 1.2084833933573431e-05,
      "loss": 0.1154,
      "step": 4698
    },
    {
      "epoch": 9.398,
      "grad_norm": 0.4711751937866211,
      "learning_rate": 1.2044817927170868e-05,
      "loss": 0.1076,
      "step": 4699
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.4668048024177551,
      "learning_rate": 1.2004801920768307e-05,
      "loss": 0.1191,
      "step": 4700
    },
    {
      "epoch": 9.402,
      "grad_norm": 0.42384156584739685,
      "learning_rate": 1.1964785914365746e-05,
      "loss": 0.0797,
      "step": 4701
    },
    {
      "epoch": 9.404,
      "grad_norm": 0.36690425872802734,
      "learning_rate": 1.1924769907963187e-05,
      "loss": 0.0898,
      "step": 4702
    },
    {
      "epoch": 9.406,
      "grad_norm": 0.4306248724460602,
      "learning_rate": 1.1884753901560625e-05,
      "loss": 0.1009,
      "step": 4703
    },
    {
      "epoch": 9.408,
      "grad_norm": 0.40650424361228943,
      "learning_rate": 1.1844737895158062e-05,
      "loss": 0.0942,
      "step": 4704
    },
    {
      "epoch": 9.41,
      "grad_norm": 0.5701740384101868,
      "learning_rate": 1.1804721888755503e-05,
      "loss": 0.1437,
      "step": 4705
    },
    {
      "epoch": 9.412,
      "grad_norm": 0.4953077733516693,
      "learning_rate": 1.1764705882352942e-05,
      "loss": 0.119,
      "step": 4706
    },
    {
      "epoch": 9.414,
      "grad_norm": 0.3962818384170532,
      "learning_rate": 1.172468987595038e-05,
      "loss": 0.0892,
      "step": 4707
    },
    {
      "epoch": 9.416,
      "grad_norm": 0.450610488653183,
      "learning_rate": 1.168467386954782e-05,
      "loss": 0.0893,
      "step": 4708
    },
    {
      "epoch": 9.418,
      "grad_norm": 0.45512545108795166,
      "learning_rate": 1.1644657863145258e-05,
      "loss": 0.0991,
      "step": 4709
    },
    {
      "epoch": 9.42,
      "grad_norm": 0.36140990257263184,
      "learning_rate": 1.1604641856742697e-05,
      "loss": 0.0644,
      "step": 4710
    },
    {
      "epoch": 9.422,
      "grad_norm": 0.45177802443504333,
      "learning_rate": 1.1564625850340138e-05,
      "loss": 0.0906,
      "step": 4711
    },
    {
      "epoch": 9.424,
      "grad_norm": 0.6064013838768005,
      "learning_rate": 1.1524609843937575e-05,
      "loss": 0.1209,
      "step": 4712
    },
    {
      "epoch": 9.426,
      "grad_norm": 0.4261934757232666,
      "learning_rate": 1.1484593837535014e-05,
      "loss": 0.111,
      "step": 4713
    },
    {
      "epoch": 9.428,
      "grad_norm": 0.37111490964889526,
      "learning_rate": 1.1444577831132454e-05,
      "loss": 0.0886,
      "step": 4714
    },
    {
      "epoch": 9.43,
      "grad_norm": 0.3033951222896576,
      "learning_rate": 1.1404561824729893e-05,
      "loss": 0.0978,
      "step": 4715
    },
    {
      "epoch": 9.432,
      "grad_norm": 0.5264898538589478,
      "learning_rate": 1.136454581832733e-05,
      "loss": 0.1156,
      "step": 4716
    },
    {
      "epoch": 9.434,
      "grad_norm": 0.4431917369365692,
      "learning_rate": 1.132452981192477e-05,
      "loss": 0.1081,
      "step": 4717
    },
    {
      "epoch": 9.436,
      "grad_norm": 0.4544840157032013,
      "learning_rate": 1.128451380552221e-05,
      "loss": 0.0812,
      "step": 4718
    },
    {
      "epoch": 9.438,
      "grad_norm": 0.485632061958313,
      "learning_rate": 1.1244497799119648e-05,
      "loss": 0.134,
      "step": 4719
    },
    {
      "epoch": 9.44,
      "grad_norm": 0.7448661923408508,
      "learning_rate": 1.1204481792717087e-05,
      "loss": 0.1467,
      "step": 4720
    },
    {
      "epoch": 9.442,
      "grad_norm": 0.49750348925590515,
      "learning_rate": 1.1164465786314526e-05,
      "loss": 0.112,
      "step": 4721
    },
    {
      "epoch": 9.444,
      "grad_norm": 0.4569507837295532,
      "learning_rate": 1.1124449779911965e-05,
      "loss": 0.1058,
      "step": 4722
    },
    {
      "epoch": 9.446,
      "grad_norm": 0.41446614265441895,
      "learning_rate": 1.1084433773509405e-05,
      "loss": 0.0815,
      "step": 4723
    },
    {
      "epoch": 9.448,
      "grad_norm": 0.4850035607814789,
      "learning_rate": 1.1044417767106842e-05,
      "loss": 0.1345,
      "step": 4724
    },
    {
      "epoch": 9.45,
      "grad_norm": 0.4015824496746063,
      "learning_rate": 1.1004401760704281e-05,
      "loss": 0.0945,
      "step": 4725
    },
    {
      "epoch": 9.452,
      "grad_norm": 0.5739214420318604,
      "learning_rate": 1.0964385754301722e-05,
      "loss": 0.1409,
      "step": 4726
    },
    {
      "epoch": 9.454,
      "grad_norm": 0.46780067682266235,
      "learning_rate": 1.092436974789916e-05,
      "loss": 0.1336,
      "step": 4727
    },
    {
      "epoch": 9.456,
      "grad_norm": 0.4200630187988281,
      "learning_rate": 1.08843537414966e-05,
      "loss": 0.1037,
      "step": 4728
    },
    {
      "epoch": 9.458,
      "grad_norm": 0.5293201804161072,
      "learning_rate": 1.0844337735094038e-05,
      "loss": 0.113,
      "step": 4729
    },
    {
      "epoch": 9.46,
      "grad_norm": 0.5627740025520325,
      "learning_rate": 1.0804321728691477e-05,
      "loss": 0.1173,
      "step": 4730
    },
    {
      "epoch": 9.462,
      "grad_norm": 0.4287862181663513,
      "learning_rate": 1.0764305722288916e-05,
      "loss": 0.0954,
      "step": 4731
    },
    {
      "epoch": 9.464,
      "grad_norm": 0.4228639304637909,
      "learning_rate": 1.0724289715886355e-05,
      "loss": 0.0829,
      "step": 4732
    },
    {
      "epoch": 9.466,
      "grad_norm": 0.4105352461338043,
      "learning_rate": 1.0684273709483794e-05,
      "loss": 0.092,
      "step": 4733
    },
    {
      "epoch": 9.468,
      "grad_norm": 0.5463685393333435,
      "learning_rate": 1.0644257703081232e-05,
      "loss": 0.1247,
      "step": 4734
    },
    {
      "epoch": 9.47,
      "grad_norm": 0.48853641748428345,
      "learning_rate": 1.0604241696678673e-05,
      "loss": 0.1029,
      "step": 4735
    },
    {
      "epoch": 9.472,
      "grad_norm": 0.5301245450973511,
      "learning_rate": 1.0564225690276112e-05,
      "loss": 0.1202,
      "step": 4736
    },
    {
      "epoch": 9.474,
      "grad_norm": 0.3511303961277008,
      "learning_rate": 1.0524209683873549e-05,
      "loss": 0.079,
      "step": 4737
    },
    {
      "epoch": 9.475999999999999,
      "grad_norm": 0.5374801754951477,
      "learning_rate": 1.048419367747099e-05,
      "loss": 0.1261,
      "step": 4738
    },
    {
      "epoch": 9.478,
      "grad_norm": 0.383758008480072,
      "learning_rate": 1.0444177671068428e-05,
      "loss": 0.0814,
      "step": 4739
    },
    {
      "epoch": 9.48,
      "grad_norm": 0.43246328830718994,
      "learning_rate": 1.0404161664665867e-05,
      "loss": 0.1063,
      "step": 4740
    },
    {
      "epoch": 9.482,
      "grad_norm": 0.4887218773365021,
      "learning_rate": 1.0364145658263306e-05,
      "loss": 0.1598,
      "step": 4741
    },
    {
      "epoch": 9.484,
      "grad_norm": 0.40084701776504517,
      "learning_rate": 1.0324129651860745e-05,
      "loss": 0.084,
      "step": 4742
    },
    {
      "epoch": 9.486,
      "grad_norm": 0.49497780203819275,
      "learning_rate": 1.0284113645458184e-05,
      "loss": 0.1006,
      "step": 4743
    },
    {
      "epoch": 9.488,
      "grad_norm": 0.49596408009529114,
      "learning_rate": 1.0244097639055624e-05,
      "loss": 0.0931,
      "step": 4744
    },
    {
      "epoch": 9.49,
      "grad_norm": 0.6479867696762085,
      "learning_rate": 1.0204081632653061e-05,
      "loss": 0.1158,
      "step": 4745
    },
    {
      "epoch": 9.492,
      "grad_norm": 0.40857237577438354,
      "learning_rate": 1.01640656262505e-05,
      "loss": 0.0989,
      "step": 4746
    },
    {
      "epoch": 9.494,
      "grad_norm": 0.5297290682792664,
      "learning_rate": 1.0124049619847939e-05,
      "loss": 0.111,
      "step": 4747
    },
    {
      "epoch": 9.496,
      "grad_norm": 0.5648529529571533,
      "learning_rate": 1.008403361344538e-05,
      "loss": 0.1418,
      "step": 4748
    },
    {
      "epoch": 9.498,
      "grad_norm": 0.508293628692627,
      "learning_rate": 1.0044017607042817e-05,
      "loss": 0.0909,
      "step": 4749
    },
    {
      "epoch": 9.5,
      "grad_norm": 0.37446799874305725,
      "learning_rate": 1.0004001600640255e-05,
      "loss": 0.102,
      "step": 4750
    },
    {
      "epoch": 9.502,
      "grad_norm": 0.5034705400466919,
      "learning_rate": 9.963985594237696e-06,
      "loss": 0.0945,
      "step": 4751
    },
    {
      "epoch": 9.504,
      "grad_norm": 0.4190710186958313,
      "learning_rate": 9.923969587835135e-06,
      "loss": 0.0785,
      "step": 4752
    },
    {
      "epoch": 9.506,
      "grad_norm": 0.526512086391449,
      "learning_rate": 9.883953581432574e-06,
      "loss": 0.1105,
      "step": 4753
    },
    {
      "epoch": 9.508,
      "grad_norm": 0.321216344833374,
      "learning_rate": 9.843937575030012e-06,
      "loss": 0.0906,
      "step": 4754
    },
    {
      "epoch": 9.51,
      "grad_norm": 0.47563791275024414,
      "learning_rate": 9.803921568627451e-06,
      "loss": 0.1002,
      "step": 4755
    },
    {
      "epoch": 9.512,
      "grad_norm": 0.5521723031997681,
      "learning_rate": 9.76390556222489e-06,
      "loss": 0.129,
      "step": 4756
    },
    {
      "epoch": 9.514,
      "grad_norm": 0.45510223507881165,
      "learning_rate": 9.723889555822329e-06,
      "loss": 0.123,
      "step": 4757
    },
    {
      "epoch": 9.516,
      "grad_norm": 0.5543003082275391,
      "learning_rate": 9.683873549419768e-06,
      "loss": 0.1328,
      "step": 4758
    },
    {
      "epoch": 9.518,
      "grad_norm": 0.3382398188114166,
      "learning_rate": 9.643857543017207e-06,
      "loss": 0.0902,
      "step": 4759
    },
    {
      "epoch": 9.52,
      "grad_norm": 0.4969840943813324,
      "learning_rate": 9.603841536614647e-06,
      "loss": 0.1118,
      "step": 4760
    },
    {
      "epoch": 9.522,
      "grad_norm": 0.6674742698669434,
      "learning_rate": 9.563825530212086e-06,
      "loss": 0.1152,
      "step": 4761
    },
    {
      "epoch": 9.524000000000001,
      "grad_norm": 0.4519960582256317,
      "learning_rate": 9.523809523809523e-06,
      "loss": 0.0915,
      "step": 4762
    },
    {
      "epoch": 9.526,
      "grad_norm": 0.39827731251716614,
      "learning_rate": 9.483793517406964e-06,
      "loss": 0.12,
      "step": 4763
    },
    {
      "epoch": 9.528,
      "grad_norm": 0.4301055669784546,
      "learning_rate": 9.443777511004402e-06,
      "loss": 0.0989,
      "step": 4764
    },
    {
      "epoch": 9.53,
      "grad_norm": 0.6430556774139404,
      "learning_rate": 9.403761504601841e-06,
      "loss": 0.1177,
      "step": 4765
    },
    {
      "epoch": 9.532,
      "grad_norm": 0.47842755913734436,
      "learning_rate": 9.36374549819928e-06,
      "loss": 0.1291,
      "step": 4766
    },
    {
      "epoch": 9.534,
      "grad_norm": 0.45503851771354675,
      "learning_rate": 9.323729491796719e-06,
      "loss": 0.0981,
      "step": 4767
    },
    {
      "epoch": 9.536,
      "grad_norm": 0.4386720657348633,
      "learning_rate": 9.283713485394158e-06,
      "loss": 0.1219,
      "step": 4768
    },
    {
      "epoch": 9.538,
      "grad_norm": 0.37722867727279663,
      "learning_rate": 9.243697478991598e-06,
      "loss": 0.1044,
      "step": 4769
    },
    {
      "epoch": 9.54,
      "grad_norm": 0.336684912443161,
      "learning_rate": 9.203681472589035e-06,
      "loss": 0.0814,
      "step": 4770
    },
    {
      "epoch": 9.542,
      "grad_norm": 0.3137950897216797,
      "learning_rate": 9.163665466186474e-06,
      "loss": 0.0707,
      "step": 4771
    },
    {
      "epoch": 9.544,
      "grad_norm": 0.46142885088920593,
      "learning_rate": 9.123649459783915e-06,
      "loss": 0.1001,
      "step": 4772
    },
    {
      "epoch": 9.546,
      "grad_norm": 0.5230897068977356,
      "learning_rate": 9.083633453381354e-06,
      "loss": 0.1138,
      "step": 4773
    },
    {
      "epoch": 9.548,
      "grad_norm": 0.36919084191322327,
      "learning_rate": 9.04361744697879e-06,
      "loss": 0.086,
      "step": 4774
    },
    {
      "epoch": 9.55,
      "grad_norm": 0.43692871928215027,
      "learning_rate": 9.003601440576231e-06,
      "loss": 0.1139,
      "step": 4775
    },
    {
      "epoch": 9.552,
      "grad_norm": 0.393485426902771,
      "learning_rate": 8.96358543417367e-06,
      "loss": 0.0999,
      "step": 4776
    },
    {
      "epoch": 9.554,
      "grad_norm": 0.35708877444267273,
      "learning_rate": 8.923569427771109e-06,
      "loss": 0.077,
      "step": 4777
    },
    {
      "epoch": 9.556000000000001,
      "grad_norm": 0.4854084551334381,
      "learning_rate": 8.883553421368548e-06,
      "loss": 0.1152,
      "step": 4778
    },
    {
      "epoch": 9.558,
      "grad_norm": 0.4776460528373718,
      "learning_rate": 8.843537414965987e-06,
      "loss": 0.098,
      "step": 4779
    },
    {
      "epoch": 9.56,
      "grad_norm": 0.6822763085365295,
      "learning_rate": 8.803521408563425e-06,
      "loss": 0.1418,
      "step": 4780
    },
    {
      "epoch": 9.562,
      "grad_norm": 0.2951467037200928,
      "learning_rate": 8.763505402160866e-06,
      "loss": 0.0739,
      "step": 4781
    },
    {
      "epoch": 9.564,
      "grad_norm": 0.3477843105792999,
      "learning_rate": 8.723489395758303e-06,
      "loss": 0.081,
      "step": 4782
    },
    {
      "epoch": 9.566,
      "grad_norm": 0.4650390148162842,
      "learning_rate": 8.683473389355742e-06,
      "loss": 0.1185,
      "step": 4783
    },
    {
      "epoch": 9.568,
      "grad_norm": 0.5357627868652344,
      "learning_rate": 8.643457382953182e-06,
      "loss": 0.1014,
      "step": 4784
    },
    {
      "epoch": 9.57,
      "grad_norm": 0.5084903240203857,
      "learning_rate": 8.603441376550621e-06,
      "loss": 0.1376,
      "step": 4785
    },
    {
      "epoch": 9.572,
      "grad_norm": 0.42510730028152466,
      "learning_rate": 8.56342537014806e-06,
      "loss": 0.1125,
      "step": 4786
    },
    {
      "epoch": 9.574,
      "grad_norm": 0.48947495222091675,
      "learning_rate": 8.523409363745499e-06,
      "loss": 0.1079,
      "step": 4787
    },
    {
      "epoch": 9.576,
      "grad_norm": 0.5456661581993103,
      "learning_rate": 8.483393357342938e-06,
      "loss": 0.144,
      "step": 4788
    },
    {
      "epoch": 9.578,
      "grad_norm": 0.4336373805999756,
      "learning_rate": 8.443377350940377e-06,
      "loss": 0.0953,
      "step": 4789
    },
    {
      "epoch": 9.58,
      "grad_norm": 0.40643978118896484,
      "learning_rate": 8.403361344537817e-06,
      "loss": 0.0755,
      "step": 4790
    },
    {
      "epoch": 9.582,
      "grad_norm": 0.4459352195262909,
      "learning_rate": 8.363345338135254e-06,
      "loss": 0.1047,
      "step": 4791
    },
    {
      "epoch": 9.584,
      "grad_norm": 0.4752810299396515,
      "learning_rate": 8.323329331732693e-06,
      "loss": 0.1198,
      "step": 4792
    },
    {
      "epoch": 9.586,
      "grad_norm": 0.4284796714782715,
      "learning_rate": 8.283313325330132e-06,
      "loss": 0.118,
      "step": 4793
    },
    {
      "epoch": 9.588,
      "grad_norm": 0.559160053730011,
      "learning_rate": 8.243297318927572e-06,
      "loss": 0.1659,
      "step": 4794
    },
    {
      "epoch": 9.59,
      "grad_norm": 0.4130381643772125,
      "learning_rate": 8.20328131252501e-06,
      "loss": 0.1172,
      "step": 4795
    },
    {
      "epoch": 9.592,
      "grad_norm": 0.37096738815307617,
      "learning_rate": 8.163265306122448e-06,
      "loss": 0.0932,
      "step": 4796
    },
    {
      "epoch": 9.594,
      "grad_norm": 0.4530934691429138,
      "learning_rate": 8.123249299719889e-06,
      "loss": 0.0867,
      "step": 4797
    },
    {
      "epoch": 9.596,
      "grad_norm": 0.394594669342041,
      "learning_rate": 8.083233293317328e-06,
      "loss": 0.0971,
      "step": 4798
    },
    {
      "epoch": 9.598,
      "grad_norm": 0.46476686000823975,
      "learning_rate": 8.043217286914767e-06,
      "loss": 0.0783,
      "step": 4799
    },
    {
      "epoch": 9.6,
      "grad_norm": 0.464080274105072,
      "learning_rate": 8.003201280512205e-06,
      "loss": 0.0984,
      "step": 4800
    },
    {
      "epoch": 9.602,
      "grad_norm": 0.5047593116760254,
      "learning_rate": 7.963185274109644e-06,
      "loss": 0.1098,
      "step": 4801
    },
    {
      "epoch": 9.604,
      "grad_norm": 0.46764248609542847,
      "learning_rate": 7.923169267707083e-06,
      "loss": 0.0992,
      "step": 4802
    },
    {
      "epoch": 9.606,
      "grad_norm": 0.31825169920921326,
      "learning_rate": 7.883153261304522e-06,
      "loss": 0.0858,
      "step": 4803
    },
    {
      "epoch": 9.608,
      "grad_norm": 0.5502042174339294,
      "learning_rate": 7.84313725490196e-06,
      "loss": 0.1157,
      "step": 4804
    },
    {
      "epoch": 9.61,
      "grad_norm": 0.5632069110870361,
      "learning_rate": 7.8031212484994e-06,
      "loss": 0.1248,
      "step": 4805
    },
    {
      "epoch": 9.612,
      "grad_norm": 0.37427178025245667,
      "learning_rate": 7.76310524209684e-06,
      "loss": 0.1041,
      "step": 4806
    },
    {
      "epoch": 9.614,
      "grad_norm": 0.4049568176269531,
      "learning_rate": 7.723089235694279e-06,
      "loss": 0.0786,
      "step": 4807
    },
    {
      "epoch": 9.616,
      "grad_norm": 0.4323158860206604,
      "learning_rate": 7.683073229291716e-06,
      "loss": 0.1249,
      "step": 4808
    },
    {
      "epoch": 9.618,
      "grad_norm": 0.529553234577179,
      "learning_rate": 7.643057222889157e-06,
      "loss": 0.1554,
      "step": 4809
    },
    {
      "epoch": 9.62,
      "grad_norm": 0.4127788841724396,
      "learning_rate": 7.603041216486595e-06,
      "loss": 0.0975,
      "step": 4810
    },
    {
      "epoch": 9.622,
      "grad_norm": 0.46412938833236694,
      "learning_rate": 7.563025210084033e-06,
      "loss": 0.1014,
      "step": 4811
    },
    {
      "epoch": 9.624,
      "grad_norm": 0.44245463609695435,
      "learning_rate": 7.523009203681473e-06,
      "loss": 0.1064,
      "step": 4812
    },
    {
      "epoch": 9.626,
      "grad_norm": 0.3246373236179352,
      "learning_rate": 7.482993197278912e-06,
      "loss": 0.0805,
      "step": 4813
    },
    {
      "epoch": 9.628,
      "grad_norm": 0.37942206859588623,
      "learning_rate": 7.442977190876351e-06,
      "loss": 0.0864,
      "step": 4814
    },
    {
      "epoch": 9.63,
      "grad_norm": 0.44312915205955505,
      "learning_rate": 7.40296118447379e-06,
      "loss": 0.099,
      "step": 4815
    },
    {
      "epoch": 9.632,
      "grad_norm": 0.53370201587677,
      "learning_rate": 7.362945178071229e-06,
      "loss": 0.1272,
      "step": 4816
    },
    {
      "epoch": 9.634,
      "grad_norm": 0.6594711542129517,
      "learning_rate": 7.322929171668667e-06,
      "loss": 0.1307,
      "step": 4817
    },
    {
      "epoch": 9.636,
      "grad_norm": 0.5616547465324402,
      "learning_rate": 7.282913165266108e-06,
      "loss": 0.1167,
      "step": 4818
    },
    {
      "epoch": 9.638,
      "grad_norm": 0.3470892012119293,
      "learning_rate": 7.242897158863546e-06,
      "loss": 0.0762,
      "step": 4819
    },
    {
      "epoch": 9.64,
      "grad_norm": 0.44685113430023193,
      "learning_rate": 7.2028811524609845e-06,
      "loss": 0.1187,
      "step": 4820
    },
    {
      "epoch": 9.642,
      "grad_norm": 0.38665327429771423,
      "learning_rate": 7.162865146058424e-06,
      "loss": 0.1087,
      "step": 4821
    },
    {
      "epoch": 9.644,
      "grad_norm": 0.4201265275478363,
      "learning_rate": 7.122849139655863e-06,
      "loss": 0.0995,
      "step": 4822
    },
    {
      "epoch": 9.646,
      "grad_norm": 0.462374746799469,
      "learning_rate": 7.082833133253301e-06,
      "loss": 0.1054,
      "step": 4823
    },
    {
      "epoch": 9.648,
      "grad_norm": 0.4522840678691864,
      "learning_rate": 7.0428171268507415e-06,
      "loss": 0.1074,
      "step": 4824
    },
    {
      "epoch": 9.65,
      "grad_norm": 0.5040470361709595,
      "learning_rate": 7.0028011204481795e-06,
      "loss": 0.127,
      "step": 4825
    },
    {
      "epoch": 9.652,
      "grad_norm": 0.38781508803367615,
      "learning_rate": 6.962785114045618e-06,
      "loss": 0.078,
      "step": 4826
    },
    {
      "epoch": 9.654,
      "grad_norm": 0.39032992720603943,
      "learning_rate": 6.922769107643058e-06,
      "loss": 0.0925,
      "step": 4827
    },
    {
      "epoch": 9.656,
      "grad_norm": 0.376517653465271,
      "learning_rate": 6.882753101240497e-06,
      "loss": 0.0848,
      "step": 4828
    },
    {
      "epoch": 9.658,
      "grad_norm": 0.4881475269794464,
      "learning_rate": 6.842737094837935e-06,
      "loss": 0.1155,
      "step": 4829
    },
    {
      "epoch": 9.66,
      "grad_norm": 0.3725850284099579,
      "learning_rate": 6.802721088435375e-06,
      "loss": 0.0986,
      "step": 4830
    },
    {
      "epoch": 9.662,
      "grad_norm": 0.4617556929588318,
      "learning_rate": 6.762705082032813e-06,
      "loss": 0.1038,
      "step": 4831
    },
    {
      "epoch": 9.664,
      "grad_norm": 0.42670345306396484,
      "learning_rate": 6.722689075630252e-06,
      "loss": 0.0894,
      "step": 4832
    },
    {
      "epoch": 9.666,
      "grad_norm": 0.4363112449645996,
      "learning_rate": 6.682673069227692e-06,
      "loss": 0.1004,
      "step": 4833
    },
    {
      "epoch": 9.668,
      "grad_norm": 0.3635559380054474,
      "learning_rate": 6.642657062825131e-06,
      "loss": 0.0757,
      "step": 4834
    },
    {
      "epoch": 9.67,
      "grad_norm": 0.42926695942878723,
      "learning_rate": 6.6026410564225695e-06,
      "loss": 0.1022,
      "step": 4835
    },
    {
      "epoch": 9.672,
      "grad_norm": 0.44815531373023987,
      "learning_rate": 6.562625050020009e-06,
      "loss": 0.0914,
      "step": 4836
    },
    {
      "epoch": 9.674,
      "grad_norm": 0.32480862736701965,
      "learning_rate": 6.522609043617447e-06,
      "loss": 0.0873,
      "step": 4837
    },
    {
      "epoch": 9.676,
      "grad_norm": 0.5130557417869568,
      "learning_rate": 6.482593037214886e-06,
      "loss": 0.0843,
      "step": 4838
    },
    {
      "epoch": 9.678,
      "grad_norm": 0.5900299549102783,
      "learning_rate": 6.442577030812325e-06,
      "loss": 0.1131,
      "step": 4839
    },
    {
      "epoch": 9.68,
      "grad_norm": 0.6623225212097168,
      "learning_rate": 6.4025610244097644e-06,
      "loss": 0.1261,
      "step": 4840
    },
    {
      "epoch": 9.682,
      "grad_norm": 0.30070826411247253,
      "learning_rate": 6.362545018007203e-06,
      "loss": 0.0708,
      "step": 4841
    },
    {
      "epoch": 9.684,
      "grad_norm": 0.45251700282096863,
      "learning_rate": 6.322529011604641e-06,
      "loss": 0.0996,
      "step": 4842
    },
    {
      "epoch": 9.686,
      "grad_norm": 0.5678248405456543,
      "learning_rate": 6.282513005202082e-06,
      "loss": 0.1455,
      "step": 4843
    },
    {
      "epoch": 9.688,
      "grad_norm": 0.5341371893882751,
      "learning_rate": 6.24249699879952e-06,
      "loss": 0.1159,
      "step": 4844
    },
    {
      "epoch": 9.69,
      "grad_norm": 0.42129266262054443,
      "learning_rate": 6.2024809923969594e-06,
      "loss": 0.1121,
      "step": 4845
    },
    {
      "epoch": 9.692,
      "grad_norm": 0.5840120315551758,
      "learning_rate": 6.162464985994398e-06,
      "loss": 0.0968,
      "step": 4846
    },
    {
      "epoch": 9.693999999999999,
      "grad_norm": 0.4834258556365967,
      "learning_rate": 6.122448979591837e-06,
      "loss": 0.101,
      "step": 4847
    },
    {
      "epoch": 9.696,
      "grad_norm": 0.2709643542766571,
      "learning_rate": 6.082432973189276e-06,
      "loss": 0.0743,
      "step": 4848
    },
    {
      "epoch": 9.698,
      "grad_norm": 0.4891614019870758,
      "learning_rate": 6.042416966786716e-06,
      "loss": 0.1143,
      "step": 4849
    },
    {
      "epoch": 9.7,
      "grad_norm": 0.5041660666465759,
      "learning_rate": 6.002400960384154e-06,
      "loss": 0.1439,
      "step": 4850
    },
    {
      "epoch": 9.702,
      "grad_norm": 0.4049464464187622,
      "learning_rate": 5.962384953981593e-06,
      "loss": 0.0857,
      "step": 4851
    },
    {
      "epoch": 9.704,
      "grad_norm": 0.331788569688797,
      "learning_rate": 5.922368947579031e-06,
      "loss": 0.0688,
      "step": 4852
    },
    {
      "epoch": 9.706,
      "grad_norm": 0.5246061682701111,
      "learning_rate": 5.882352941176471e-06,
      "loss": 0.127,
      "step": 4853
    },
    {
      "epoch": 9.708,
      "grad_norm": 0.4786061644554138,
      "learning_rate": 5.84233693477391e-06,
      "loss": 0.1041,
      "step": 4854
    },
    {
      "epoch": 9.71,
      "grad_norm": 0.48269981145858765,
      "learning_rate": 5.802320928371349e-06,
      "loss": 0.1303,
      "step": 4855
    },
    {
      "epoch": 9.712,
      "grad_norm": 0.5652378797531128,
      "learning_rate": 5.762304921968787e-06,
      "loss": 0.1128,
      "step": 4856
    },
    {
      "epoch": 9.714,
      "grad_norm": 0.43591752648353577,
      "learning_rate": 5.722288915566227e-06,
      "loss": 0.1143,
      "step": 4857
    },
    {
      "epoch": 9.716,
      "grad_norm": 0.3123434782028198,
      "learning_rate": 5.682272909163665e-06,
      "loss": 0.0748,
      "step": 4858
    },
    {
      "epoch": 9.718,
      "grad_norm": 0.2786995470523834,
      "learning_rate": 5.642256902761105e-06,
      "loss": 0.0826,
      "step": 4859
    },
    {
      "epoch": 9.72,
      "grad_norm": 0.392215758562088,
      "learning_rate": 5.6022408963585436e-06,
      "loss": 0.0846,
      "step": 4860
    },
    {
      "epoch": 9.722,
      "grad_norm": 0.4466821551322937,
      "learning_rate": 5.562224889955982e-06,
      "loss": 0.1005,
      "step": 4861
    },
    {
      "epoch": 9.724,
      "grad_norm": 0.4277290999889374,
      "learning_rate": 5.522208883553421e-06,
      "loss": 0.0984,
      "step": 4862
    },
    {
      "epoch": 9.725999999999999,
      "grad_norm": 0.581997811794281,
      "learning_rate": 5.482192877150861e-06,
      "loss": 0.1265,
      "step": 4863
    },
    {
      "epoch": 9.728,
      "grad_norm": 0.41122251749038696,
      "learning_rate": 5.4421768707483e-06,
      "loss": 0.0863,
      "step": 4864
    },
    {
      "epoch": 9.73,
      "grad_norm": 0.4193112254142761,
      "learning_rate": 5.4021608643457386e-06,
      "loss": 0.097,
      "step": 4865
    },
    {
      "epoch": 9.732,
      "grad_norm": 0.46730393171310425,
      "learning_rate": 5.362144857943177e-06,
      "loss": 0.105,
      "step": 4866
    },
    {
      "epoch": 9.734,
      "grad_norm": 0.43468666076660156,
      "learning_rate": 5.322128851540616e-06,
      "loss": 0.0946,
      "step": 4867
    },
    {
      "epoch": 9.736,
      "grad_norm": 0.4628412425518036,
      "learning_rate": 5.282112845138056e-06,
      "loss": 0.0965,
      "step": 4868
    },
    {
      "epoch": 9.738,
      "grad_norm": 0.47965511679649353,
      "learning_rate": 5.242096838735495e-06,
      "loss": 0.1068,
      "step": 4869
    },
    {
      "epoch": 9.74,
      "grad_norm": 0.4465172588825226,
      "learning_rate": 5.2020808323329336e-06,
      "loss": 0.1184,
      "step": 4870
    },
    {
      "epoch": 9.742,
      "grad_norm": 0.4834218919277191,
      "learning_rate": 5.162064825930372e-06,
      "loss": 0.0962,
      "step": 4871
    },
    {
      "epoch": 9.744,
      "grad_norm": 0.6416636109352112,
      "learning_rate": 5.122048819527812e-06,
      "loss": 0.1298,
      "step": 4872
    },
    {
      "epoch": 9.746,
      "grad_norm": 0.5516524314880371,
      "learning_rate": 5.08203281312525e-06,
      "loss": 0.0907,
      "step": 4873
    },
    {
      "epoch": 9.748,
      "grad_norm": 0.44879820942878723,
      "learning_rate": 5.04201680672269e-06,
      "loss": 0.0765,
      "step": 4874
    },
    {
      "epoch": 9.75,
      "grad_norm": 0.3128811717033386,
      "learning_rate": 5.002000800320128e-06,
      "loss": 0.0794,
      "step": 4875
    },
    {
      "epoch": 9.752,
      "grad_norm": 0.5547990202903748,
      "learning_rate": 4.961984793917567e-06,
      "loss": 0.1146,
      "step": 4876
    },
    {
      "epoch": 9.754,
      "grad_norm": 0.4217378497123718,
      "learning_rate": 4.921968787515006e-06,
      "loss": 0.0988,
      "step": 4877
    },
    {
      "epoch": 9.756,
      "grad_norm": 0.4908919036388397,
      "learning_rate": 4.881952781112445e-06,
      "loss": 0.09,
      "step": 4878
    },
    {
      "epoch": 9.758,
      "grad_norm": 0.4726605713367462,
      "learning_rate": 4.841936774709884e-06,
      "loss": 0.1223,
      "step": 4879
    },
    {
      "epoch": 9.76,
      "grad_norm": 0.4503423273563385,
      "learning_rate": 4.8019207683073235e-06,
      "loss": 0.0956,
      "step": 4880
    },
    {
      "epoch": 9.762,
      "grad_norm": 0.4465889632701874,
      "learning_rate": 4.7619047619047615e-06,
      "loss": 0.0859,
      "step": 4881
    },
    {
      "epoch": 9.764,
      "grad_norm": 0.42410168051719666,
      "learning_rate": 4.721888755502201e-06,
      "loss": 0.1149,
      "step": 4882
    },
    {
      "epoch": 9.766,
      "grad_norm": 0.4208122491836548,
      "learning_rate": 4.68187274909964e-06,
      "loss": 0.0827,
      "step": 4883
    },
    {
      "epoch": 9.768,
      "grad_norm": 0.4534691870212555,
      "learning_rate": 4.641856742697079e-06,
      "loss": 0.1021,
      "step": 4884
    },
    {
      "epoch": 9.77,
      "grad_norm": 0.3632364273071289,
      "learning_rate": 4.601840736294518e-06,
      "loss": 0.0863,
      "step": 4885
    },
    {
      "epoch": 9.772,
      "grad_norm": 0.5570332407951355,
      "learning_rate": 4.561824729891957e-06,
      "loss": 0.0995,
      "step": 4886
    },
    {
      "epoch": 9.774000000000001,
      "grad_norm": 0.3053395450115204,
      "learning_rate": 4.521808723489395e-06,
      "loss": 0.095,
      "step": 4887
    },
    {
      "epoch": 9.776,
      "grad_norm": 0.391521692276001,
      "learning_rate": 4.481792717086835e-06,
      "loss": 0.0825,
      "step": 4888
    },
    {
      "epoch": 9.778,
      "grad_norm": 0.32404500246047974,
      "learning_rate": 4.441776710684274e-06,
      "loss": 0.0714,
      "step": 4889
    },
    {
      "epoch": 9.78,
      "grad_norm": 0.4735064208507538,
      "learning_rate": 4.401760704281713e-06,
      "loss": 0.0892,
      "step": 4890
    },
    {
      "epoch": 9.782,
      "grad_norm": 0.42047151923179626,
      "learning_rate": 4.3617446978791515e-06,
      "loss": 0.1007,
      "step": 4891
    },
    {
      "epoch": 9.784,
      "grad_norm": 0.683854877948761,
      "learning_rate": 4.321728691476591e-06,
      "loss": 0.1396,
      "step": 4892
    },
    {
      "epoch": 9.786,
      "grad_norm": 0.5286815166473389,
      "learning_rate": 4.28171268507403e-06,
      "loss": 0.1319,
      "step": 4893
    },
    {
      "epoch": 9.788,
      "grad_norm": 0.5550742745399475,
      "learning_rate": 4.241696678671469e-06,
      "loss": 0.1266,
      "step": 4894
    },
    {
      "epoch": 9.79,
      "grad_norm": 0.402244508266449,
      "learning_rate": 4.2016806722689085e-06,
      "loss": 0.1118,
      "step": 4895
    },
    {
      "epoch": 9.792,
      "grad_norm": 0.6325705647468567,
      "learning_rate": 4.1616646658663465e-06,
      "loss": 0.1276,
      "step": 4896
    },
    {
      "epoch": 9.794,
      "grad_norm": 0.758557915687561,
      "learning_rate": 4.121648659463786e-06,
      "loss": 0.1129,
      "step": 4897
    },
    {
      "epoch": 9.796,
      "grad_norm": 0.45314064621925354,
      "learning_rate": 4.081632653061224e-06,
      "loss": 0.0976,
      "step": 4898
    },
    {
      "epoch": 9.798,
      "grad_norm": 0.41492128372192383,
      "learning_rate": 4.041616646658664e-06,
      "loss": 0.0952,
      "step": 4899
    },
    {
      "epoch": 9.8,
      "grad_norm": 0.4346621334552765,
      "learning_rate": 4.001600640256103e-06,
      "loss": 0.1006,
      "step": 4900
    },
    {
      "epoch": 9.802,
      "grad_norm": 0.5533825755119324,
      "learning_rate": 3.9615846338535415e-06,
      "loss": 0.1271,
      "step": 4901
    },
    {
      "epoch": 9.804,
      "grad_norm": 0.36916083097457886,
      "learning_rate": 3.92156862745098e-06,
      "loss": 0.0899,
      "step": 4902
    },
    {
      "epoch": 9.806000000000001,
      "grad_norm": 0.5651475787162781,
      "learning_rate": 3.88155262104842e-06,
      "loss": 0.1093,
      "step": 4903
    },
    {
      "epoch": 9.808,
      "grad_norm": 0.30722135305404663,
      "learning_rate": 3.841536614645858e-06,
      "loss": 0.0767,
      "step": 4904
    },
    {
      "epoch": 9.81,
      "grad_norm": 0.4910699129104614,
      "learning_rate": 3.8015206082432977e-06,
      "loss": 0.1137,
      "step": 4905
    },
    {
      "epoch": 9.812,
      "grad_norm": 0.4523046016693115,
      "learning_rate": 3.7615046018407365e-06,
      "loss": 0.1125,
      "step": 4906
    },
    {
      "epoch": 9.814,
      "grad_norm": 0.3815518021583557,
      "learning_rate": 3.7214885954381753e-06,
      "loss": 0.1184,
      "step": 4907
    },
    {
      "epoch": 9.816,
      "grad_norm": 0.5211619734764099,
      "learning_rate": 3.6814725890356146e-06,
      "loss": 0.0764,
      "step": 4908
    },
    {
      "epoch": 9.818,
      "grad_norm": 0.5182317495346069,
      "learning_rate": 3.641456582633054e-06,
      "loss": 0.1138,
      "step": 4909
    },
    {
      "epoch": 9.82,
      "grad_norm": 0.6506872773170471,
      "learning_rate": 3.6014405762304922e-06,
      "loss": 0.1872,
      "step": 4910
    },
    {
      "epoch": 9.822,
      "grad_norm": 0.33719414472579956,
      "learning_rate": 3.5614245698279315e-06,
      "loss": 0.0911,
      "step": 4911
    },
    {
      "epoch": 9.824,
      "grad_norm": 0.3704532980918884,
      "learning_rate": 3.5214085634253707e-06,
      "loss": 0.0924,
      "step": 4912
    },
    {
      "epoch": 9.826,
      "grad_norm": 0.5559196472167969,
      "learning_rate": 3.481392557022809e-06,
      "loss": 0.1261,
      "step": 4913
    },
    {
      "epoch": 9.828,
      "grad_norm": 0.36088085174560547,
      "learning_rate": 3.4413765506202484e-06,
      "loss": 0.1046,
      "step": 4914
    },
    {
      "epoch": 9.83,
      "grad_norm": 0.6067166328430176,
      "learning_rate": 3.4013605442176877e-06,
      "loss": 0.1268,
      "step": 4915
    },
    {
      "epoch": 9.832,
      "grad_norm": 0.3941763937473297,
      "learning_rate": 3.361344537815126e-06,
      "loss": 0.1084,
      "step": 4916
    },
    {
      "epoch": 9.834,
      "grad_norm": 0.41507506370544434,
      "learning_rate": 3.3213285314125653e-06,
      "loss": 0.0874,
      "step": 4917
    },
    {
      "epoch": 9.836,
      "grad_norm": 0.45299774408340454,
      "learning_rate": 3.2813125250100046e-06,
      "loss": 0.0891,
      "step": 4918
    },
    {
      "epoch": 9.838,
      "grad_norm": 0.47482940554618835,
      "learning_rate": 3.241296518607443e-06,
      "loss": 0.118,
      "step": 4919
    },
    {
      "epoch": 9.84,
      "grad_norm": 0.45036715269088745,
      "learning_rate": 3.2012805122048822e-06,
      "loss": 0.1166,
      "step": 4920
    },
    {
      "epoch": 9.842,
      "grad_norm": 0.5532904267311096,
      "learning_rate": 3.1612645058023206e-06,
      "loss": 0.113,
      "step": 4921
    },
    {
      "epoch": 9.844,
      "grad_norm": 0.5433518886566162,
      "learning_rate": 3.12124849939976e-06,
      "loss": 0.1256,
      "step": 4922
    },
    {
      "epoch": 9.846,
      "grad_norm": 0.490557461977005,
      "learning_rate": 3.081232492997199e-06,
      "loss": 0.1162,
      "step": 4923
    },
    {
      "epoch": 9.848,
      "grad_norm": 0.47666141390800476,
      "learning_rate": 3.041216486594638e-06,
      "loss": 0.1108,
      "step": 4924
    },
    {
      "epoch": 9.85,
      "grad_norm": 0.6371438503265381,
      "learning_rate": 3.001200480192077e-06,
      "loss": 0.1155,
      "step": 4925
    },
    {
      "epoch": 9.852,
      "grad_norm": 0.4221828281879425,
      "learning_rate": 2.9611844737895156e-06,
      "loss": 0.0954,
      "step": 4926
    },
    {
      "epoch": 9.854,
      "grad_norm": 0.5158068537712097,
      "learning_rate": 2.921168467386955e-06,
      "loss": 0.1221,
      "step": 4927
    },
    {
      "epoch": 9.856,
      "grad_norm": 0.4337862730026245,
      "learning_rate": 2.8811524609843937e-06,
      "loss": 0.1164,
      "step": 4928
    },
    {
      "epoch": 9.858,
      "grad_norm": 0.6922174096107483,
      "learning_rate": 2.8411364545818325e-06,
      "loss": 0.1178,
      "step": 4929
    },
    {
      "epoch": 9.86,
      "grad_norm": 0.5397732853889465,
      "learning_rate": 2.8011204481792718e-06,
      "loss": 0.1346,
      "step": 4930
    },
    {
      "epoch": 9.862,
      "grad_norm": 0.584632158279419,
      "learning_rate": 2.7611044417767106e-06,
      "loss": 0.1379,
      "step": 4931
    },
    {
      "epoch": 9.864,
      "grad_norm": 0.3773265480995178,
      "learning_rate": 2.72108843537415e-06,
      "loss": 0.0907,
      "step": 4932
    },
    {
      "epoch": 9.866,
      "grad_norm": 0.4029622972011566,
      "learning_rate": 2.6810724289715887e-06,
      "loss": 0.0894,
      "step": 4933
    },
    {
      "epoch": 9.868,
      "grad_norm": 0.3702922761440277,
      "learning_rate": 2.641056422569028e-06,
      "loss": 0.0824,
      "step": 4934
    },
    {
      "epoch": 9.87,
      "grad_norm": 0.42178937792778015,
      "learning_rate": 2.6010404161664668e-06,
      "loss": 0.0909,
      "step": 4935
    },
    {
      "epoch": 9.872,
      "grad_norm": 0.43505579233169556,
      "learning_rate": 2.561024409763906e-06,
      "loss": 0.0931,
      "step": 4936
    },
    {
      "epoch": 9.874,
      "grad_norm": 0.554649829864502,
      "learning_rate": 2.521008403361345e-06,
      "loss": 0.1251,
      "step": 4937
    },
    {
      "epoch": 9.876,
      "grad_norm": 0.2869059443473816,
      "learning_rate": 2.4809923969587837e-06,
      "loss": 0.0785,
      "step": 4938
    },
    {
      "epoch": 9.878,
      "grad_norm": 0.38648611307144165,
      "learning_rate": 2.4409763905562225e-06,
      "loss": 0.0942,
      "step": 4939
    },
    {
      "epoch": 9.88,
      "grad_norm": 0.5935617685317993,
      "learning_rate": 2.4009603841536618e-06,
      "loss": 0.1124,
      "step": 4940
    },
    {
      "epoch": 9.882,
      "grad_norm": 0.4725913107395172,
      "learning_rate": 2.3609443777511006e-06,
      "loss": 0.0901,
      "step": 4941
    },
    {
      "epoch": 9.884,
      "grad_norm": 0.6162242889404297,
      "learning_rate": 2.3209283713485394e-06,
      "loss": 0.0925,
      "step": 4942
    },
    {
      "epoch": 9.886,
      "grad_norm": 0.5751839280128479,
      "learning_rate": 2.2809123649459787e-06,
      "loss": 0.1072,
      "step": 4943
    },
    {
      "epoch": 9.888,
      "grad_norm": 0.5511135458946228,
      "learning_rate": 2.2408963585434175e-06,
      "loss": 0.0803,
      "step": 4944
    },
    {
      "epoch": 9.89,
      "grad_norm": 0.38552334904670715,
      "learning_rate": 2.2008803521408563e-06,
      "loss": 0.1105,
      "step": 4945
    },
    {
      "epoch": 9.892,
      "grad_norm": 0.5499106645584106,
      "learning_rate": 2.1608643457382956e-06,
      "loss": 0.108,
      "step": 4946
    },
    {
      "epoch": 9.894,
      "grad_norm": 0.41584479808807373,
      "learning_rate": 2.1208483393357344e-06,
      "loss": 0.0777,
      "step": 4947
    },
    {
      "epoch": 9.896,
      "grad_norm": 0.4318093955516815,
      "learning_rate": 2.0808323329331733e-06,
      "loss": 0.0979,
      "step": 4948
    },
    {
      "epoch": 9.898,
      "grad_norm": 0.6004402041435242,
      "learning_rate": 2.040816326530612e-06,
      "loss": 0.1255,
      "step": 4949
    },
    {
      "epoch": 9.9,
      "grad_norm": 0.4796943962574005,
      "learning_rate": 2.0008003201280513e-06,
      "loss": 0.1032,
      "step": 4950
    },
    {
      "epoch": 9.902,
      "grad_norm": 0.4702901840209961,
      "learning_rate": 1.96078431372549e-06,
      "loss": 0.1067,
      "step": 4951
    },
    {
      "epoch": 9.904,
      "grad_norm": 0.4572288393974304,
      "learning_rate": 1.920768307322929e-06,
      "loss": 0.0979,
      "step": 4952
    },
    {
      "epoch": 9.906,
      "grad_norm": 0.4985215365886688,
      "learning_rate": 1.8807523009203682e-06,
      "loss": 0.1212,
      "step": 4953
    },
    {
      "epoch": 9.908,
      "grad_norm": 0.4545636475086212,
      "learning_rate": 1.8407362945178073e-06,
      "loss": 0.1129,
      "step": 4954
    },
    {
      "epoch": 9.91,
      "grad_norm": 0.391416072845459,
      "learning_rate": 1.8007202881152461e-06,
      "loss": 0.1117,
      "step": 4955
    },
    {
      "epoch": 9.912,
      "grad_norm": 0.4442654252052307,
      "learning_rate": 1.7607042817126854e-06,
      "loss": 0.1138,
      "step": 4956
    },
    {
      "epoch": 9.914,
      "grad_norm": 0.4678890109062195,
      "learning_rate": 1.7206882753101242e-06,
      "loss": 0.0854,
      "step": 4957
    },
    {
      "epoch": 9.916,
      "grad_norm": 0.5737485885620117,
      "learning_rate": 1.680672268907563e-06,
      "loss": 0.1022,
      "step": 4958
    },
    {
      "epoch": 9.918,
      "grad_norm": 0.35345280170440674,
      "learning_rate": 1.6406562625050023e-06,
      "loss": 0.097,
      "step": 4959
    },
    {
      "epoch": 9.92,
      "grad_norm": 0.5734924077987671,
      "learning_rate": 1.6006402561024411e-06,
      "loss": 0.1362,
      "step": 4960
    },
    {
      "epoch": 9.922,
      "grad_norm": 0.6411468982696533,
      "learning_rate": 1.56062424969988e-06,
      "loss": 0.1488,
      "step": 4961
    },
    {
      "epoch": 9.924,
      "grad_norm": 0.6422485113143921,
      "learning_rate": 1.520608243297319e-06,
      "loss": 0.1345,
      "step": 4962
    },
    {
      "epoch": 9.926,
      "grad_norm": 0.7229016423225403,
      "learning_rate": 1.4805922368947578e-06,
      "loss": 0.1466,
      "step": 4963
    },
    {
      "epoch": 9.928,
      "grad_norm": 0.34765246510505676,
      "learning_rate": 1.4405762304921969e-06,
      "loss": 0.0814,
      "step": 4964
    },
    {
      "epoch": 9.93,
      "grad_norm": 0.42676517367362976,
      "learning_rate": 1.4005602240896359e-06,
      "loss": 0.0895,
      "step": 4965
    },
    {
      "epoch": 9.932,
      "grad_norm": 0.6496121287345886,
      "learning_rate": 1.360544217687075e-06,
      "loss": 0.1246,
      "step": 4966
    },
    {
      "epoch": 9.934,
      "grad_norm": 0.6458474397659302,
      "learning_rate": 1.320528211284514e-06,
      "loss": 0.1319,
      "step": 4967
    },
    {
      "epoch": 9.936,
      "grad_norm": 0.5057796835899353,
      "learning_rate": 1.280512204881953e-06,
      "loss": 0.126,
      "step": 4968
    },
    {
      "epoch": 9.938,
      "grad_norm": 0.47422537207603455,
      "learning_rate": 1.2404961984793918e-06,
      "loss": 0.1205,
      "step": 4969
    },
    {
      "epoch": 9.94,
      "grad_norm": 0.38417789340019226,
      "learning_rate": 1.2004801920768309e-06,
      "loss": 0.106,
      "step": 4970
    },
    {
      "epoch": 9.942,
      "grad_norm": 0.4783719778060913,
      "learning_rate": 1.1604641856742697e-06,
      "loss": 0.1179,
      "step": 4971
    },
    {
      "epoch": 9.943999999999999,
      "grad_norm": 0.6701499223709106,
      "learning_rate": 1.1204481792717088e-06,
      "loss": 0.1228,
      "step": 4972
    },
    {
      "epoch": 9.946,
      "grad_norm": 0.5032358169555664,
      "learning_rate": 1.0804321728691478e-06,
      "loss": 0.1063,
      "step": 4973
    },
    {
      "epoch": 9.948,
      "grad_norm": 0.5176131129264832,
      "learning_rate": 1.0404161664665866e-06,
      "loss": 0.1044,
      "step": 4974
    },
    {
      "epoch": 9.95,
      "grad_norm": 0.4721498191356659,
      "learning_rate": 1.0004001600640257e-06,
      "loss": 0.0852,
      "step": 4975
    },
    {
      "epoch": 9.952,
      "grad_norm": 0.42099177837371826,
      "learning_rate": 9.603841536614645e-07,
      "loss": 0.1024,
      "step": 4976
    },
    {
      "epoch": 9.954,
      "grad_norm": 0.5389631986618042,
      "learning_rate": 9.203681472589036e-07,
      "loss": 0.1116,
      "step": 4977
    },
    {
      "epoch": 9.956,
      "grad_norm": 0.3813855051994324,
      "learning_rate": 8.803521408563427e-07,
      "loss": 0.0927,
      "step": 4978
    },
    {
      "epoch": 9.958,
      "grad_norm": 0.45190882682800293,
      "learning_rate": 8.403361344537815e-07,
      "loss": 0.1034,
      "step": 4979
    },
    {
      "epoch": 9.96,
      "grad_norm": 0.5510184168815613,
      "learning_rate": 8.003201280512206e-07,
      "loss": 0.109,
      "step": 4980
    },
    {
      "epoch": 9.962,
      "grad_norm": 0.4144138991832733,
      "learning_rate": 7.603041216486595e-07,
      "loss": 0.0925,
      "step": 4981
    },
    {
      "epoch": 9.964,
      "grad_norm": 0.47464004158973694,
      "learning_rate": 7.202881152460984e-07,
      "loss": 0.0987,
      "step": 4982
    },
    {
      "epoch": 9.966,
      "grad_norm": 0.4445215165615082,
      "learning_rate": 6.802721088435375e-07,
      "loss": 0.0882,
      "step": 4983
    },
    {
      "epoch": 9.968,
      "grad_norm": 0.5168370008468628,
      "learning_rate": 6.402561024409765e-07,
      "loss": 0.1183,
      "step": 4984
    },
    {
      "epoch": 9.97,
      "grad_norm": 0.4982364773750305,
      "learning_rate": 6.002400960384154e-07,
      "loss": 0.1252,
      "step": 4985
    },
    {
      "epoch": 9.972,
      "grad_norm": 0.44736653566360474,
      "learning_rate": 5.602240896358544e-07,
      "loss": 0.1163,
      "step": 4986
    },
    {
      "epoch": 9.974,
      "grad_norm": 0.45531266927719116,
      "learning_rate": 5.202080832332933e-07,
      "loss": 0.1092,
      "step": 4987
    },
    {
      "epoch": 9.975999999999999,
      "grad_norm": 0.4671938419342041,
      "learning_rate": 4.801920768307322e-07,
      "loss": 0.122,
      "step": 4988
    },
    {
      "epoch": 9.978,
      "grad_norm": 0.4346978962421417,
      "learning_rate": 4.4017607042817134e-07,
      "loss": 0.1094,
      "step": 4989
    },
    {
      "epoch": 9.98,
      "grad_norm": 0.46955278515815735,
      "learning_rate": 4.001600640256103e-07,
      "loss": 0.1225,
      "step": 4990
    },
    {
      "epoch": 9.982,
      "grad_norm": 0.5054312348365784,
      "learning_rate": 3.601440576230492e-07,
      "loss": 0.1012,
      "step": 4991
    },
    {
      "epoch": 9.984,
      "grad_norm": 0.6122709512710571,
      "learning_rate": 3.2012805122048825e-07,
      "loss": 0.138,
      "step": 4992
    },
    {
      "epoch": 9.986,
      "grad_norm": 0.41928020119667053,
      "learning_rate": 2.801120448179272e-07,
      "loss": 0.1167,
      "step": 4993
    },
    {
      "epoch": 9.988,
      "grad_norm": 0.6032360792160034,
      "learning_rate": 2.400960384153661e-07,
      "loss": 0.1344,
      "step": 4994
    },
    {
      "epoch": 9.99,
      "grad_norm": 0.438132107257843,
      "learning_rate": 2.0008003201280514e-07,
      "loss": 0.0925,
      "step": 4995
    },
    {
      "epoch": 9.992,
      "grad_norm": 0.45033299922943115,
      "learning_rate": 1.6006402561024413e-07,
      "loss": 0.093,
      "step": 4996
    },
    {
      "epoch": 9.994,
      "grad_norm": 0.5855768918991089,
      "learning_rate": 1.2004801920768306e-07,
      "loss": 0.1162,
      "step": 4997
    },
    {
      "epoch": 9.996,
      "grad_norm": 0.2864484190940857,
      "learning_rate": 8.003201280512206e-08,
      "loss": 0.0943,
      "step": 4998
    },
    {
      "epoch": 9.998,
      "grad_norm": 0.4247126579284668,
      "learning_rate": 4.001600640256103e-08,
      "loss": 0.0829,
      "step": 4999
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.46783819794654846,
      "learning_rate": 0.0,
      "loss": 0.1191,
      "step": 5000
    }
  ],
  "logging_steps": 1,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.801839249161216e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
